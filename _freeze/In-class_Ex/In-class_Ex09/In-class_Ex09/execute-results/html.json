{
  "hash": "89209117c6eff9bd1415ae6295305a5f",
  "result": {
    "markdown": "---\ntitle: \"Geographically Weighted Predictive Models\"\nexecute: \n  eval: true\n  echo: true\n  warning: false\n  freeze: true\ndate: \"2024-03-18\"\n---\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(sf, spdep, GWmodel, SpatialML, \n               tmap, rsample, tidymodels, Metrics, tidyverse, gtsummary, rpart, rpart.plot, ggstatsplot, performance)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrs_sf <- read_rds(\"data/rds/HDB_resale.rds\")\n```\n:::\n\n\nSplit 50/50 for the training data and the testing data\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nresale_split <- initial_split(rs_sf,\n                             prop = 5/10,)\ntrain_sf <- training(resale_split)\ntest_sf <- testing(resale_split)\n```\n:::\n\n\nresale_split will contain all the data, we do not need to save it as an object if we want to save space because after we split to train and test, it will be useless. Instead, we can use pipelining %>%.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_df <- train_sf %>%\n  st_drop_geometry() %>%\n  as.data.frame()\n\ntest_df <- test_sf %>%\n  st_drop_geometry() %>%\n  as.data.frame()\n```\n:::\n\nWe have to read the documentation to know whether we need it in a normal dataframe or tibble dataframe. We use as.data.frame() to create a dataframe, which has 17 columns instead of 18 because we have dropped the geometry column.\n\n## Computing Correlation Matirx \n\n\n::: {.cell}\n\n```{.r .cell-code}\nrs_sf1 <- rs_sf %>% \n  st_drop_geometry()\ncorrplot::corrplot(cor(rs_sf1[, 2:17]), \n                   diag = FALSE, \n                   order = \"AOE\",\n                   tl.pos = \"td\", \n                   tl.cex = 0.5, \n                   method = \"number\", \n                   type = \"upper\")\n```\n\n::: {.cell-output-display}\n![](In-class_Ex09_files/figure-html/unnamed-chunk-5-1.png){width=1152}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprice_mlr <- lm(formula = RESALE_PRICE ~ FLOOR_AREA_SQM +\n                  STOREY_ORDER + REMAINING_LEASE_MTHS +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + PROX_CHAS + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                data=train_df)\nsummary(price_mlr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = RESALE_PRICE ~ FLOOR_AREA_SQM + STOREY_ORDER + REMAINING_LEASE_MTHS + \n    PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK + \n    PROX_MALL + PROX_SUPERMARKET + PROX_CHAS + WITHIN_350M_KINDERGARTEN + \n    WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH, \n    data = train_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-179676  -39020   -1719   36755  327324 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(>|t|)    \n(Intercept)              109622.960  11993.611   9.140  < 2e-16 ***\nFLOOR_AREA_SQM             2733.136    103.116  26.505  < 2e-16 ***\nSTOREY_ORDER              14198.168    384.182  36.957  < 2e-16 ***\nREMAINING_LEASE_MTHS        346.624      5.208  66.557  < 2e-16 ***\nPROX_CBD                 -16943.794    227.064 -74.621  < 2e-16 ***\nPROX_ELDERLYCARE         -13891.413   1124.964 -12.348  < 2e-16 ***\nPROX_HAWKER              -17758.037   1461.269 -12.152  < 2e-16 ***\nPROX_MRT                 -32357.534   1965.095 -16.466  < 2e-16 ***\nPROX_PARK                 -6714.626   1672.160  -4.016 5.99e-05 ***\nPROX_MALL                -14080.474   2268.191  -6.208 5.64e-10 ***\nPROX_SUPERMARKET         -24077.152   5068.317  -4.751 2.06e-06 ***\nPROX_CHAS                 -5819.260   7208.182  -0.807 0.419510    \nWITHIN_350M_KINDERGARTEN   8730.822    721.593  12.099  < 2e-16 ***\nWITHIN_350M_CHILDCARE     -4629.126    399.231 -11.595  < 2e-16 ***\nWITHIN_350M_BUS             979.339    252.851   3.873 0.000108 ***\nWITHIN_1KM_PRISCH         -8434.367    553.862 -15.228  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 61050 on 7934 degrees of freedom\nMultiple R-squared:  0.7405,\tAdjusted R-squared:  0.7401 \nF-statistic:  1510 on 15 and 7934 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n## Revising mlr model \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_df <- train_df %>%\n  select(-c(PROX_CHAS))\ntrain_sf <- train_sf %>%\n  select(-c(PROX_CHAS))\ntest_df <- test_df %>%\n  select(-c(PROX_CHAS))\ntest_sf <- test_sf %>%\n  select(-c(PROX_CHAS))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprice_mlr <- lm(formula = RESALE_PRICE ~ FLOOR_AREA_SQM +\n                  STOREY_ORDER + REMAINING_LEASE_MTHS +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                data=train_df)\nsummary(price_mlr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = RESALE_PRICE ~ FLOOR_AREA_SQM + STOREY_ORDER + REMAINING_LEASE_MTHS + \n    PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK + \n    PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + \n    WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH, \n    data = train_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-179178  -39031   -1868   36751  327631 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(>|t|)    \n(Intercept)              109413.550  11990.543   9.125  < 2e-16 ***\nFLOOR_AREA_SQM             2725.663    102.698  26.541  < 2e-16 ***\nSTOREY_ORDER              14192.913    384.118  36.949  < 2e-16 ***\nREMAINING_LEASE_MTHS        346.996      5.187  66.893  < 2e-16 ***\nPROX_CBD                 -16943.081    227.058 -74.620  < 2e-16 ***\nPROX_ELDERLYCARE         -13972.191   1120.481 -12.470  < 2e-16 ***\nPROX_HAWKER              -17968.486   1437.798 -12.497  < 2e-16 ***\nPROX_MRT                 -32448.233   1961.837 -16.540  < 2e-16 ***\nPROX_PARK                 -6753.096   1671.444  -4.040 5.39e-05 ***\nPROX_MALL                -14003.731   2266.148  -6.180 6.75e-10 ***\nPROX_SUPERMARKET         -25566.285   4720.643  -5.416 6.28e-08 ***\nWITHIN_350M_KINDERGARTEN   8740.242    721.483  12.114  < 2e-16 ***\nWITHIN_350M_CHILDCARE     -4614.476    398.810 -11.571  < 2e-16 ***\nWITHIN_350M_BUS             990.698    252.454   3.924 8.77e-05 ***\nWITHIN_1KM_PRISCH         -8438.093    553.831 -15.236  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 61040 on 7935 degrees of freedom\nMultiple R-squared:  0.7405,\tAdjusted R-squared:  0.7401 \nF-statistic:  1618 on 14 and 7935 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nExtract te x y coordinates of the full, training and test data sets\n\n::: {.cell}\n\n```{.r .cell-code}\ncoords <- st_coordinates(rs_sf)\ncoords_train <- st_coordinates(train_sf)\ncoords_test <- st_coordinates(test_sf)\n```\n:::\n\nThe algorithm will require 2 dataset, one is the dataframe that consist of independent and dependent variables. Another separate data table needs to have all the X and Y coordinates (named X and Y columns).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nrs_rp <- rpart(formula = RESALE_PRICE ~ FLOOR_AREA_SQM +\n                  STOREY_ORDER + REMAINING_LEASE_MTHS +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                data=train_df)\nrs_rp\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nn= 7950 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 7950 1.139546e+14 433705.6  \n   2) PROX_CBD>=7.974483 6665 4.472144e+13 403736.0  \n     4) REMAINING_LEASE_MTHS< 1020.5 4228 1.573100e+13 370187.4  \n       8) PROX_CBD>=14.48068 1820 2.748388e+12 337963.6 *\n       9) PROX_CBD< 14.48068 2408 9.664405e+12 394542.6 *\n     5) REMAINING_LEASE_MTHS>=1020.5 2437 1.597594e+13 461940.1  \n      10) PROX_CBD>=10.40657 2331 9.762718e+12 451754.4  \n        20) PROX_CBD>=14.20377 1088 3.345588e+12 426109.1 *\n        21) PROX_CBD< 14.20377 1243 5.075243e+12 474201.8 *\n      11) PROX_CBD< 10.40657 106 6.532500e+11 685929.1 *\n   3) PROX_CBD< 7.974483 1285 3.219685e+13 589151.4  \n     6) REMAINING_LEASE_MTHS< 930.5 745 6.613365e+12 486637.6  \n      12) FLOOR_AREA_SQM< 98.5 451 2.446537e+12 442460.5 *\n      13) FLOOR_AREA_SQM>=98.5 294 1.936449e+12 554405.7 *\n     7) REMAINING_LEASE_MTHS>=930.5 540 6.952722e+12 730582.5  \n      14) REMAINING_LEASE_MTHS< 1071.5 314 2.461969e+12 676641.3 *\n      15) REMAINING_LEASE_MTHS>=1071.5 226 2.307737e+12 805527.4 *\n```\n:::\n:::\n\nrs_rp is a collection of pbject that gives us the split result and importance of variables so on. We can then plot the tree to see how it is split.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrpart.plot(rs_rp)\n```\n\n::: {.cell-output-display}\n![](In-class_Ex09_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nWhen we use the condition PROX_CBD >= 8, we will split to 2 major groups. The next level is the REMAINING_LEASE_MONTHS. Once we know the conditions, we can use SQL to extract the target response variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nrs_rf <- ranger(formula = RESALE_PRICE ~ FLOOR_AREA_SQM +\n                  STOREY_ORDER + REMAINING_LEASE_MTHS +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                data=train_df,\n                importance = \"impurity\")\nrs_rf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRanger result\n\nCall:\n ranger(formula = RESALE_PRICE ~ FLOOR_AREA_SQM + STOREY_ORDER +      REMAINING_LEASE_MTHS + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +      PROX_MRT + PROX_PARK + PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +      WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH,      data = train_df, importance = \"impurity\") \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      7950 \nNumber of independent variables:  14 \nMtry:                             3 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       774008152 \nR squared (OOB):                  0.9460084 \n```\n:::\n:::\n\nUsing the ranger package is relatively faster, so most of the time we will use this instead of random forest. By default, we use 500 trees. Initially, the importance will be set to none, so we cannot retrieve the importnce. So we need to specify to determine which variable are important in our calculation. Impurity is commonly used in decision trees and regression trees. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nvi <- as.data.frame(rs_rf$variable.importance)\nvi$variables <- rownames(vi)\nvi <- vi %>%\n  rename(vi = \"rs_rf$variable.importance\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = vi,\n       aes(x=vi,\n           y=reorder(variables, vi))) #use reorder to sort the bars to view the importance, if not it will be displayed alphabetically +\n```\n\n::: {.cell-output-display}\n![](In-class_Ex09_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n```{.r .cell-code}\n  geom_bar(stat=\"identity\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ngeom_bar: just = 0.5, width = NULL, na.rm = FALSE, orientation = NA\nstat_identity: na.rm = FALSE\nposition_stack \n```\n:::\n:::\n\n\nIf we have a model that looks like that it is okay, because there are different importance for each variable. But if the model shows one super long bar for one variable, that means there might be issues with the data that creates a complete separation situation, then we might need to exclude the extreme predictor to be able to analyse the other variables.\n\nWe need to calculate the bandwidth and fit it into the model. This will take very long and the data is very big, so we should confine our observations to a certain area, so that we can build an application and it wont hang.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrs_grf <- read_rds(\"data/models/rs_grf.rds\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_df <- cbind(test_sf, coords_test) %>%\n  st_drop_geometry()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngrf_pred <- predict.grf(rs_grf,\n                        test_df,\n                        x.var.name = \"X\",\n                        y.var.name = \"Y\",\n                        local.w = 1,\n                        global.w = 0)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngrf_pred <- read_rds(\"data/models/grf_pred.rds\")\ngrf_pred_df <- as.data.frame(grf_pred)\n```\n:::\n\n\nAppend the predicted values onto the test_df\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_pred <- test_df %>% \n  select(RESALE_PRICE) %>%\n  cbind(grf_pred_df)\n```\n:::\n\ntest_pred will have 2 columns, first one is the resale price and second is the predicted value using the geographical random forest to compare the predicted and actual values. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_pred <- predict(rs_rf, test_df)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_pred_df <- as.data.frame(rf_pred$predictions) %>%\n  rename(rf_pred = \"rf_pred$predictions\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_pred <- cbind(test_pred,\n                   rf_pred_df)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmlr_pred <- predict(price_mlr, test_df)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmlr_pred_df <- as.data.frame(mlr_pred) %>%\n  rename(mlr_pred = \"mlr_pred\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_pred <- cbind(test_pred,\n                   mlr_pred_df)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nyardstick::rmse(test_pred,\n                RESALE_PRICE,\n                mlr_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      61821.\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmc <- test_pred %>%\n  pivot_longer(cols = c(2:4),\n               names_to = \"models\",\n               values_to = \"predicted\")\n```\n:::\n\nThis gives us a summary of the statistics. Choose the model that gives us the best prediction.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = test_pred,\n       aes(x = grf_pred,\n           y = RESALE_PRICE)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](In-class_Ex09_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::",
    "supporting": [
      "In-class_Ex09_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}