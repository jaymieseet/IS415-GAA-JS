[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IS415-GAA-JS",
    "section": "",
    "text": "this is the course website of IS415. you will find my coursework published on this website"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html",
    "title": "In-class_Ex03",
    "section": "",
    "text": "Show the code\ninstall.packages(\"maptools\", repos = \"https://packagemanager.posit.co/cran/2023-10-13\")\n\n\n\n\nShow the code\npacman::p_load(maptools, sf, raster, spatstat, tmap, tidyverse)\n\n\nImporting the data we currently have: Take not that the Child Care Service data is in WGS84 projection, so we need to use st_transform(crs=3414) to change it to the correct projection system All projection should be standardised!\n\n\nShow the code\nchildcare_sf <- st_read(\"data/geospatial/ChildCareServices.geojson\") %>%\n  st_transform(crs = 3414)\n\n\nReading layer `ChildCareServices' from data source \n  `C:\\jaymieseet\\IS415-GAA-JS\\In-class_Ex\\In-class_Ex03\\data\\geospatial\\ChildCareServices.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1925 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nShow the code\nmpsz_sf <- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\jaymieseet\\IS415-GAA-JS\\In-class_Ex\\In-class_Ex03\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\nShow the code\nmpsz_sf\n\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\nWe can derive the CostalOutline from the Master Plan 2014 Subzone data\n\n\nShow the code\nplot(mpsz_sf)\n\n\n\n\n\nBy plotting mpsz_sf, we can see the map that we want to derive the CostalOutline from. In order to get the outline, we have to dissolve away the boundaries of the subzones (Combine the polygons together) *\n\nst_combine() does not dissolve the boundary, but only combines the polygons (the output is one whole giant polygon)\nst_union() dissolves the boundary, and we can choose the specific attributes that we want to dissolve (eg. by subzones or by planning area). The polygons are still individual as they are not combined.\n\n\n\nShow the code\nsg_sf <- mpsz_sf %>%\n  st_union()\n\n\nNow the polygons should be combined!\n\n\nShow the code\nplot(sg_sf)\n\n\n\n\n\n“Spatstat” is an R package designed for analyzing spatial point patterns, commonly encountered in fields like ecology, epidemiology, and geospatial statistics. In the context of spatstat, “ppp” stands for “point pattern,” and it is a specific format within spatstat used to represent point patterns.\nWhen working with spatstat, your spatial data needs to be converted into the “ppp” format, which is the expected input format for many spatstat functions. This format includes not just the coordinates of points but also additional information about the pattern, such as marks assigned to each point.\nConvert the sf dataframe to ppp that conforms to the spastat standard Check the duplication in a ppp object! When dealing with georeferenced locations using postal code, grouping of multiple locations under a single postal code is possible.\n\n\nShow the code\nchildcare_ppp <- as.ppp(childcare_sf)\nany(duplicated(childcare_ppp))\n\n\n[1] FALSE\n\n\nCreating owin object: sf method\n\n\nShow the code\nsg_owin <- as.owin(sg_sf)\n\n\nNote that it has to be in sf format. Anything that uses the ‘as’ function needs to be in sf format.\nThe code chunk below will be used to extract the target planning areas use the sf layer to use diplyr filter\n\n\nShow the code\npg <- mpsz_sf %>%\n  filter(PLN_AREA_N == \"PUNGGOL\")\ntm <- mpsz_sf %>%\n  filter(PLN_AREA_N == \"TAMPINES\")\nck <- mpsz_sf %>%\n  filter(PLN_AREA_N == \"CHOA CHU KANG\")\njw <- mpsz_sf %>%\n  filter(PLN_AREA_N == \"JURONG WEST\")\n\n\nPlotting out the target planning areas\n\n\nShow the code\npar(mfrow=c(2,2))\nplot(pg, main = \"Punggol\")\n\n\n\n\n\nShow the code\nplot(tm, main = \"Tampines\")\n\n\n\n\n\nShow the code\nplot(ck, main = \"Choa Chu Kang\")\n\n\n\n\n\nShow the code\nplot(jw, main = \"Jurong West\")\n\n\n\n\n\n‘main’ is used to label the planning areas\n\n\nShow the code\nclass(mpsz_sf)\n\n\n[1] \"sf\"         \"data.frame\"\n\n\n\n\nShow the code\npacman::p_load(sp, sf, rgdal, spNetwork, tmap)\nnetwork <- st_read(dsn=\"data/geospatial\", \n                   layer=\"Punggol_St\")\n\n\nReading layer `Punggol_St' from data source \n  `C:\\jaymieseet\\IS415-GAA-JS\\In-class_Ex\\In-class_Ex03\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2642 features and 2 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 34038.56 ymin: 40941.11 xmax: 38882.85 ymax: 44801.27\nProjected CRS: SVY21 / Singapore TM\n\n\nShow the code\nchildcare <- st_read(dsn=\"data/geospatial\",\n                     layer=\"Punggol_CC\")\n\n\nReading layer `Punggol_CC' from data source \n  `C:\\jaymieseet\\IS415-GAA-JS\\In-class_Ex\\In-class_Ex03\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 61 features and 1 field\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 34423.98 ymin: 41503.6 xmax: 37619.47 ymax: 44685.77\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\n\n\nAlways plot the dots (points) then the lines (polygons), if not the polygons will block the points\n\n\nShow the code\ntmap_mode('view')\ntm_shape(childcare) + \n  tm_dots() + \n  tm_shape(network) +\n  tm_lines()\n\n\n\n\n\n\n\n\n\nShow the code\nlixels <- lixelize_lines(network, \n                         700, \n                         mindist = 350)\n\n\nLength of a lixel, lx_length is set to 750cm Minimum length of a lixel, mindist is set to 375cm (it should be half of the length)\ndensities <- nkde(network, events = childcare, w = rep(1,nrow(childcare)), samples = samples, kernel_name = “quartic”, bw = 300, div= “bw”, method = “simple”, digits = 1, tol = 1, grid_shape = c(1,1), max_depth = 8, agg = 5, #we aggregate events within a 5m radius (faster calculation) sparse = TRUE, verbose = FALSE)\nshows us which road segment explicitly have more childcare centres instead of the general density of the area. It is useful for decision making, for example, deciding where to place traffic crossings to minimise road congestion."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "title": "Hands-on Exercise 4",
    "section": "",
    "text": "hunan <- left_join(hunan,hunan2012)%>%\n  select(1:4, 7, 15)\n\nwhen we left_join, it append everything from the hunan2012 to hunan. the select allow us to explicitly select the columns that we want. simeplfeatures have a geometric column, so if we inspect the hunan data now, we have 7 variables.\n\nbasemap <- tm_shape(hunan) +\n  tm_polygons() +\n  tm_text(\"NAME_3\", size=0.5)\n\ngdppc <- qtm(hunan, \"GDPPC\")\ntmap_arrange(basemap, gdppc, asp=1, ncol=2)\n\n\n\n\nqtm is a quick way to view our map data (plotting hunan and GDPPC), do not need to specify any classification methods. But a better way to plot a thematic map is to use all the tmap properties to define the style.\nThe plotting does not start until tm_polygons(). tm_text() is to add labelling on your map. tmap_arrange() allows us to put all the maps together in one whole view.\n\nwm_q <- poly2nb(hunan, queen=TRUE)\n\nwm_q is a list and this list indicates the neighbours of each polygon.\n\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThis shows the summary of the lists. even though the average number of links have a decimal place, the number of actual neighbours should be an integer, so we can round up or round down.\nThe top row of numbers is the number of neighbours and the bottom row is the number of geographical area. eg. there is only 1 geographical area with 11 neighbours.\nArea 30 and 65 have the least number of links. Area 85 have the most number of links.\n\nwm_r <- poly2nb(hunan, queen=FALSE)\nsummary(wm_r)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\n\n#coords <- coordinates(hunan)\nk1 <- knn2nb(knearneigh(coords))\nk1dists <- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  13.86   28.12   34.99   36.19   44.54   67.50 \n\n\nreturns the neighbour list, based on their distance. Input is k1 coordinates.\nuse the largest distance to determine the cut off, always round up and not round down to not miss out any points.In this case, our cut-off distance will be 62.\n\nwm_d62 <- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 49 \nNumber of nonzero links: 226 \nPercentage nonzero weights: 9.412745 \nAverage number of links: 4.612245 \n2 regions with no links:\n1 21\n4 disjoint connected subgraphs\n\n\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_d62, coords, add=TRUE)\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08)\n\n\n\n\n\nb_weights <- lapply(wm_q, function(x) 0*x + 1)\nb_weights2 <- nb2listw(wm_q, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\n\nIn-class exercise\n\npacman::p_load(sf, spdep, tmap, tidyverse, knitr,GWmodel)\n\n\nhunan <- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\jaymieseet\\IS415-GAA-JS\\In-class_Ex\\In-class_Ex04\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\nhunan2012 <- read_csv(\"data/aspatial/Hunan_2012.csv\")\nhunan <- left_join(hunan,hunan2012)%>%\n  select(1:4, 7, 15)\n\n\nwn_d62 <- dnearneigh(coords,0,62, longlat = TRUE)\nhunan_sp <- hunan %>%\n  as_Spatial()\n\nit is a complex data model and there is no geometric data\n\ngwstat <- gwss(data = hunan_sp,\n               vars = \"GDPPC\",\n               bw=6,\n               kernel = \"bisquare\",\n               adaptive = TRUE,\n               longlat = T)\n\nbandwidth and adaptive have to match. if adaptive = TRUE, bw = 6 (neighbours), if it is false, bw will equal to 62 (kilometres)\nplot the map and try with fixed distance"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "title": "In-Class Exercise 5",
    "section": "",
    "text": "Exploring an alternative R package to spdep."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#importing-geospatial-data",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#importing-geospatial-data",
    "title": "In-Class Exercise 5",
    "section": "Importing Geospatial Data",
    "text": "Importing Geospatial Data\n\nhunan <- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\jaymieseet\\IS415-GAA-JS\\In-class_Ex\\In-class_Ex05\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\nWe should always check the data in our R environment. The geometry of this layer should be POLYGON."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#import-attribute-data",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#import-attribute-data",
    "title": "In-Class Exercise 5",
    "section": "Import Attribute Data",
    "text": "Import Attribute Data\n\nhunan2012 <- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\nWe must check through both data to see whether there are the same fields that will allow us to do the relational join."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#performing-relational-join",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#performing-relational-join",
    "title": "In-Class Exercise 5",
    "section": "Performing Relational Join",
    "text": "Performing Relational Join\n\nhunan_GDPPC <- left_join(hunan,hunan2012) %>%\n  select(1:4, 7, 15)\n\nWe only want to keep the columns that we need for the Hunan GDPPC."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#choropleth-map-for-the-distribution-of-gdp-per-capita-hunan-province",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#choropleth-map-for-the-distribution-of-gdp-per-capita-hunan-province",
    "title": "In-Class Exercise 5",
    "section": "Choropleth Map for the Distribution of GDP Per Capita, Hunan Province",
    "text": "Choropleth Map for the Distribution of GDP Per Capita, Hunan Province\ntm_shape already takes in the spatial data, since our data is already in sf, we do not need to specify tm_shape(hunan$geometry) as the geometry layer will already be read.\n\ntmap_mode(\"plot\")\ntm_shape(hunan_GDPPC) +\n  tm_fill(\"GDPPC\",\n          style = \"quantile\",\n          palette = 'Blues',\n          title='GDPPC') +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#computing-global-moranl",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#computing-global-moranl",
    "title": "In-Class Exercise 5",
    "section": "Computing Global Moran’l",
    "text": "Computing Global Moran’l\n\nmoranI <- global_moran(wm_q$GDPPC, \n                       wm_q$nb, \n                       wm_q$wt) \nglimpse(moranI)\n\nList of 2\n $ I: num 0.301\n $ K: num 7.64"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#performing-global-morani-test",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#performing-global-morani-test",
    "title": "In-Class Exercise 5",
    "section": "Performing Global Moran’I test",
    "text": "Performing Global Moran’I test\n\nglobal_moran_test(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#performing-global-morani-permutation-test",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#performing-global-morani-permutation-test",
    "title": "In-Class Exercise 5",
    "section": "Performing Global Moran’I permutation test",
    "text": "Performing Global Moran’I permutation test\n\nglobal_moran_perm(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value < 2.2e-16\nalternative hypothesis: two.sided"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html",
    "title": "In-Class Exercise 7",
    "section": "",
    "text": "pacman::p_load(sp, spdep, tmap, sf, ClustGeo, \n               ggpubr, cluster, factoextra, NbClust,\n               heatmaply, corrplot, psych, tidyverse, GGally)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#data-preparation",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#data-preparation",
    "title": "In-Class Exercise 7",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nict_derived <- ict %>%\n  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) %>%\n  mutate(`TV_PR` = `Television`/`Total households`*1000) %>%\n  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) %>%\n  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) %>%\n  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) %>%\n  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) %>%\n  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,\n         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,\n         `TT_HOUSEHOLDS`=`Total households`,\n         `RADIO`=`Radio`, `TV`=`Television`, \n         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,\n         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`) \n\nWe have to standardize the values (divide by total household and scale it up) to derive the ownership. Then, we rename it to make the field name more concise and easier to recognise.\nWe can create all the plots to plot the distribution of the newly derived variables. The code chunk below allows you to plot out all the variables in one view.\n\nradio <- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\ntv <- ggplot(data=ict_derived, \n             aes(x= `TV_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\nllphone <- ggplot(data=ict_derived, \n             aes(x= `LLPHONE_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\nmphone <- ggplot(data=ict_derived, \n             aes(x= `MPHONE_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\ncomputer <- ggplot(data=ict_derived, \n             aes(x= `COMPUTER_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\ninternet <- ggplot(data=ict_derived, \n             aes(x= `INTERNET_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\nLeft join the 2 dataframes with the unique identifier used to join both data objects is TS_PCODE. When we append all the ICT variables over, the first data file should be the geometry sf file, the second one will be the attribute data file. ‘by’ parameter defines the unique identifier, which is not necessary here since the unique identifier field has the same name, but it is good practice to define it.\nIf both files are a spatial file, we should use st_join() instead. Left join allows the use the sf for only one file.\n\nshan_sf <- left_join(shan_sf, \n                     ict_derived, by=c(\"TS_PCODE\"=\"TS_PCODE\"))\n  \nwrite_rds(shan_sf, \"data/rds/shan_sf.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#hierarchy-cluster-analysis",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#hierarchy-cluster-analysis",
    "title": "In-Class Exercise 7",
    "section": "Hierarchy Cluster Analysis",
    "text": "Hierarchy Cluster Analysis\n\ncluster_vars <- shan_sf %>%\n  st_set_geometry(NULL) %>%\n  select(\"TS.x\", \"RADIO_PR\", \"TV_PR\", \"LLPHONE_PR\", \"MPHONE_PR\", \"COMPUTER_PR\")\nhead(cluster_vars,10)\n\n        TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\n1    Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\n2    Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\n3    Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\n4   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\n5     Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\n6      Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\n7      Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\n8   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\n9  Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\n10   Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\n\nExtracting clustering variables\n\nshan_ict <- select(cluster_vars, c(2:6))\nhead(shan_ict, 10)\n\n   RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\n1  286.1852 554.1313   35.30618  260.6944    12.15939\n2  417.4647 505.1300   19.83584  162.3917    12.88190\n3  484.5215 260.5734   11.93591  120.2856     4.41465\n4  231.6499 541.7189   28.54454  249.4903    13.76255\n5  449.4903 708.6423   72.75255  392.6089    16.45042\n6  280.7624 611.6204   42.06478  408.7951    29.63160\n7  318.6118 535.8494   39.83270  214.8476    18.97032\n8  387.1017 630.0035   31.51366  320.5686    21.76677\n9  349.3359 547.9456   38.44960  323.0201    15.76465\n10 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\n\n\nData Standardisation\n\nshan_ict.std <- normalize(shan_ict)\nsummary(shan_ict.std)\n\n    RADIO_PR          TV_PR          LLPHONE_PR       MPHONE_PR     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.2544   1st Qu.:0.4600   1st Qu.:0.1123   1st Qu.:0.2199  \n Median :0.4097   Median :0.5523   Median :0.1948   Median :0.3846  \n Mean   :0.4199   Mean   :0.5416   Mean   :0.2703   Mean   :0.3972  \n 3rd Qu.:0.5330   3rd Qu.:0.6750   3rd Qu.:0.3746   3rd Qu.:0.5608  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n  COMPUTER_PR     \n Min.   :0.00000  \n 1st Qu.:0.09598  \n Median :0.17607  \n Mean   :0.23692  \n 3rd Qu.:0.29868  \n Max.   :1.00000  \n\n\n\nshan_ict.z <- scale(shan_ict)\ndescribe(shan_ict.z)\n\n            vars  n mean sd median trimmed  mad   min  max range  skew kurtosis\nRADIO_PR       1 55    0  1  -0.04   -0.06 0.94 -1.85 2.55  4.40  0.48    -0.27\nTV_PR          2 55    0  1   0.05    0.04 0.78 -2.47 2.09  4.56 -0.38    -0.23\nLLPHONE_PR     3 55    0  1  -0.33   -0.15 0.68 -1.19 3.20  4.39  1.37     1.49\nMPHONE_PR      4 55    0  1  -0.05   -0.06 1.01 -1.58 2.40  3.98  0.48    -0.34\nCOMPUTER_PR    5 55    0  1  -0.26   -0.18 0.64 -1.03 3.31  4.34  1.80     2.96\n              se\nRADIO_PR    0.13\nTV_PR       0.13\nLLPHONE_PR  0.13\nMPHONE_PR   0.13\nCOMPUTER_PR 0.13\n\n\nThe Z-score always equal to 1.\n\n\nVisualising the standardised clustering variables\nPlot the graph to see the how the use of the different standardization affects.\n\nr <- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  ggtitle(\"Raw values without standardisation\")\n\nshan_ict_s_df <- as.data.frame(shan_ict.std)\ns <- ggplot(data=shan_ict_s_df, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  ggtitle(\"Min-Max Standardisation\")\n\nshan_ict_z_df <- as.data.frame(shan_ict.z)\nz <- ggplot(data=shan_ict_z_df, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  ggtitle(\"Z-score Standardisation\")\n\nggarrange(r, s, z,\n          ncol = 3,\n          nrow = 1)\n\n\n\n\n\nr <- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"light blue\") +\n  ggtitle(\"Raw values without standardisation\")\n\nshan_ict_s_df <- as.data.frame(shan_ict.std)\ns <- ggplot(data=shan_ict_s_df, \n       aes(x=`RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"light blue\") +\n  ggtitle(\"Min-Max Standardisation\")\n\nshan_ict_z_df <- as.data.frame(shan_ict.z)\nz <- ggplot(data=shan_ict_z_df, \n       aes(x=`RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"light blue\") +\n  ggtitle(\"Z-score Standardisation\")\n\nggarrange(r, s, z,\n          ncol = 3,\n          nrow = 1)\n\n\n\n\n\nproxmat <- dist(shan_ict, method = 'euclidean')\n\nUsing the distance metrics.\n\nhclust_ward <- hclust(proxmat, method = 'ward.D')\n\nWe can extract the clustering results and plot it into a dendrogram.\n\nplot(hclust_ward, cex = 0.6)\n\n\n\n\nFor example, if we put the cut-off at 200, we will have 2 clusters. To determine the cut-off, we should calculate the definite value.\nWe can do multiple calculations to see which one gives us the best results.\n\nm <- c( \"average\", \"single\", \"complete\", \"ward\")\nnames(m) <- c( \"average\", \"single\", \"complete\", \"ward\")\n\nac <- function(x) {\n  agnes(shan_ict, method = x)$ac\n}\n\nmap_dbl(m, ac)\n\n  average    single  complete      ward \n0.8131144 0.6628705 0.8950702 0.9427730 \n\n\n\nset.seed(12345)\ngap_stat <- clusGap(shan_ict, \n                    FUN = hcut, \n                    nstart = 25, \n                    K.max = 10, \n                    B = 50)\n# Print the result\nprint(gap_stat, method = \"firstmax\")\n\nClustering Gap statistic [\"clusGap\"] from call:\nclusGap(x = shan_ict, FUNcluster = hcut, K.max = 10, B = 50, nstart = 25)\nB=50 simulated reference sets, k = 1..10; spaceH0=\"scaledPCA\"\n --> Number of clusters (method 'firstmax'): 1\n          logW   E.logW       gap     SE.sim\n [1,] 8.407129 8.680794 0.2736651 0.04460994\n [2,] 8.130029 8.350712 0.2206824 0.03880130\n [3,] 7.992265 8.202550 0.2102844 0.03362652\n [4,] 7.862224 8.080655 0.2184311 0.03784781\n [5,] 7.756461 7.978022 0.2215615 0.03897071\n [6,] 7.665594 7.887777 0.2221833 0.03973087\n [7,] 7.590919 7.806333 0.2154145 0.04054939\n [8,] 7.526680 7.731619 0.2049390 0.04198644\n [9,] 7.458024 7.660795 0.2027705 0.04421874\n[10,] 7.377412 7.593858 0.2164465 0.04540947\n\n\n\nfviz_gap_stat(gap_stat)\n\n\n\n\nFrom her we can see that the closest optimisation difference is 5 and 6. 6 is slightly higher so we should choose 6.\nUse rect.hclust() to clearly show the clusters so we can also see which cluster has the most amount of members.\n\nplot(hclust_ward, cex = 0.6)\nrect.hclust(hclust_ward, \n            k = 6, \n            border = 2:5)\n\n\n\n\nUse as.factor() to convert it into a factor data type and arranged in a ordinal scale.\n\ngroups <- as.factor(cutree(hclust_ward, k=6))\n\nUse cbind() to append the 2 dataframes instead of left_join() because there are no unique identifers. However, if we perform any form of sorting, the sequence change and we cannot use cbind().\n\nshan_sf_cluster <- cbind(shan_sf, as.matrix(groups)) %>%\n  rename(`CLUSTER`=`as.matrix.groups.`)\n\n\nqtm(shan_sf_cluster, \"CLUSTER\")\n\n\n\n\nSince it is in the class factor, the numbers are all treated as individual classes. If it was in the numeric class, it would become a range.\nPreviously, poly2nb() only worked with sp objects, but the latest version does not require us to convert it to sp and we can just use the sf class.\n\nshan.nb <- poly2nb(shan_sf)\nsummary(shan.nb)\n\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\n\nSince it is an sf object, we can use the plot() function to show the map.\n\nplot(st_geometry(shan_sf), border=grey(.5))\npts <- st_coordinates(st_centroid(shan_sf))\nplot(shan.nb,\n     pts,\n     col=\"blue\",\n     add=TRUE)\n\n\n\n\n\n\n\nIt is not enough to just identify the clusters, we should plot out the clusters to evaluate them."
  },
  {
    "objectID": "In-class_Ex02/In-class_Ex02.html",
    "href": "In-class_Ex02/In-class_Ex02.html",
    "title": "In-class Exercise 2",
    "section": "",
    "text": "The packages used for this exercise are:\n\nlubridate\narrow\ntidyverse\ntmap\nsf\n\n\n\nShow the code\npacman::p_load(sf, tmap, tidyverse, lubridate, arrow)\n\n\nImporting GrabPosisi Dataset with arrow package\n\n\nShow the code\ndf <- read_parquet(\"data/GrabPosisi/part-00000-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\n\n\nConverting the data type of pingtimestamp from character to date-time\nThis code overwrites the field characteristics in the df. Before, the data type in int, but after this code chuck, it should change to POSIXct\n\n\nShow the code\ndf$pingtimestamp <- as_datetime(df$pingtimestamp)\n\n\nExtracting the trip origin location\n\nGroup them according to the trajectory.\nArrange them according to the pingtimestamp, sorted in ascending order\nEvery trip that we generate, the first one should be the origin, so filter row_number()==1\nMutate to derive new variables, weekday, start_hr and day.\n\nwday, hour and mdaylabel = true is to label the day of the week, if not it will just be labelled as 1, 2, 3, 4\nfactor to put it into ordinal scale (hour, minute, second)\n\n*More information on the parameters found on the tidyverse reference page\n\n\n\nShow the code\norigin_df <- df%>%\n  group_by(trj_id) %>%\n  arrange(pingtimestamp) %>%\n  filter(row_number()==1) %>%\n  mutate(weekday = wday(pingtimestamp, \n                        label=TRUE, \n                        abbr=TRUE), \n         start_hr = factor(hour(pingtimestamp)), \n         day= factor(mday(pingtimestamp)))\n\n\nExtracting trip destination locations\nSimilar to the code before, but arrange it in descending order\n\n\nShow the code\ndestination_df <- df%>%\n  group_by(trj_id) %>%\n  arrange(desc(pingtimestamp)) %>%\n  filter(row_number()==1) %>%\n  mutate(weekday = wday(pingtimestamp, \n                        label=TRUE, \n                        abbr=TRUE), \n         end_hr = factor(hour(pingtimestamp)), \n         day= factor(mday(pingtimestamp)))\n\n\nSave it in r datafile format because it will save all the object class eg. the weekday, start_hr fields that we created, so that we can reuse it in the future.\n\n\n\nWhen we want to restart on this file, import back the data\n\n\nShow the code\norigin_df <- read_rds(\"data/rds/origin_df.rds\")\ndestination_df <- read_rds(\"data/rds/destination_df.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html",
    "title": "Hands-on Exercise 7",
    "section": "",
    "text": "Spatial Point Pattern Analysis is the evaluation of the pattern or distribution, of a set of points on a surface. The point can be location of:\n\nevents such as crime, traffic accident and disease onset, or\nbusiness services (coffee and fastfood outlets) or facilities such as childcare and eldercare.\n\nUsing appropriate functions of spatstat, this hands-on exercise aims to discover the spatial point processes of childecare centres in Singapore.\nThe specific questions we would like to answer are as follows:\n\nare the childcare centres in Singapore randomly distributed throughout the country?\nif the answer is not, then the next logical question is where are the locations with higher concentration of childcare centres?\n\n\n\n\nTo provide answers to the questions above, three data sets will be used. They are:\n\nCHILDCARE, a point feature data providing both location and attribute information of childcare centres. It was downloaded from Data.gov.sg and is in geojson format.\nMP14_SUBZONE_WEB_PL, a polygon feature data providing information of URA 2014 Master Plan Planning Subzone boundary data. It is in ESRI shapefile format. This data set was also downloaded from Data.gov.sg.\nCostalOutline, a polygon feature data showing the national boundary of Singapore. It is provided by SLA and is in ESRI shapefile format.\n\n\n\n\nIn this hands-on exercise, five R packages will be used, they are:\n\nsf, a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspatstat, which has a wide range of useful functions for point pattern analysis. In this hands-on exercise, it will be used to perform 1st- and 2nd-order spatial point patterns analysis and derive kernel density estimation (KDE) layer.\nraster which reads, writes, manipulates, analyses and model of gridded spatial data (i.e. raster). In this hands-on exercise, it will be used to convert image output generate by spatstat into raster format.\nmaptools which provides a set of tools for manipulating geographic data. In this hands-on exercise, we mainly use it to convert Spatial objects into ppp format of spatstat.\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\nUse the code chunk below to install and launch the five R packages.\n\npacman::p_load(maptools, sf, raster, spatstat, tmap)\n\n\n\n\n\n\nIn this section, st_read() of sf package will be used to import these three geospatial data sets into R.\n\nchildcare_sf <- st_read(\"data/geospatial/ChildCareServices.geojson\") %>%\n  st_transform(crs = 3414)\n\nReading layer `ChildCareServices' from data source \n  `C:\\jaymieseet\\IS415-GAA-JS\\Hands-on_Ex\\Hands-on_Ex07\\data\\geospatial\\ChildCareServices.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1925 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\nsg_sf <- st_read(dsn = \"data/geospatial\", layer=\"CostalOutline\")\n\nReading layer `CostalOutline' from data source \n  `C:\\jaymieseet\\IS415-GAA-JS\\Hands-on_Ex\\Hands-on_Ex07\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21\n\nmpsz_sf <- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\jaymieseet\\IS415-GAA-JS\\Hands-on_Ex\\Hands-on_Ex07\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nBefore we can use these data for analysis, it is important for us to ensure that they are projected in same projection system.\n\nst_crs(childcare_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\nst_crs(sg_sf)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\nst_crs(mpsz_sf)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nNotice that except childcare_sf, both mpsz_sf and sg_sf do not have proper crs information.\nAlthough mpsz data frame is projected in svy21 but when we read until the end of the print, it indicates that the EPSG is 9001. This is a wrong EPSG code because the correct EPSG code for svy21 should be 3414.\nIn order to assign the correct EPSG code to mpsz data frame, st_set_crs() of sf package is used as shown in the code chunk below.\n\nmpsz_sf <- st_set_crs(mpsz_sf, 3414)\nsg_sf <- st_set_crs(sg_sf, 3414)\n\nNow, let us check the CSR again by using the code chunk below.\n\nst_crs(sg_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\nst_crs(mpsz_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nNotice that the EPSG code is 3414 now.\n\n\n\nAfter checking the referencing system of each geospatial data data frame, it is also useful for us to plot a map to show their spatial patterns.\n\ntmap_mode('plot')\ntm_shape(mpsz_sf)+\n  tm_polygons()+\n  tm_shape(childcare_sf)+\n  tm_dots()\n\n\n\n\nNotice that all the geospatial layers are within the same map extend. This shows that their referencing system and coordinate values are referred to similar spatial context. This is very important in any geospatial analysis.\nAlternatively, we can also prepare a pin map by using the code chunk below.\n\ntmap_mode('view')\n\ntmap mode set to interactive viewing\n\ntm_shape(childcare_sf)+   tm_dots()\n\n\n\n\n\n\nNotice that at the interactive mode, tmap is using leaflet for R API. The advantage of this interactive pin map is it allows us to navigate and zoom around the map freely. We can also query the information of each simple feature (i.e. the point) by clicking of them. Last but not least, you can also change the background of the internet map layer. Currently, three internet map layers are provided. They are: ESRI.WorldGrayCanvas, OpenStreetMap, and ESRI.WorldTopoMap. The default is ESRI.WorldGrayCanvas.\n\nReminder: Always remember to switch back to plot mode after the interactive map. This is because, each interactive mode will consume a connection. You should also avoid displaying ecessive numbers of interactive maps (i.e. not more than 10) in one RMarkdown document when publish on Netlify.\n\n\n\n\n\nAlthough simple feature data frame is gaining popularity again sp’s Spatial* classes, there are, however, many geospatial analysis packages require the input geospatial data in sp’s Spatial* classes. In this section, you will learn how to convert simple feature data frame to sp’s Spatial* class.\n\n\nThe code chunk below uses as_Spatial() of sf package to convert the three geospatial data from simple feature data frame to sp’s Spatial* class.\n\nchildcare <- as_Spatial(childcare_sf)\nmpsz <- as_Spatial(mpsz_sf)\nsg <- as_Spatial(sg_sf)\n\nDisplay the information of these three Spatial* classes as shown below\n\nchildcare\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1925 \nextent      : 11810.03, 45404.24, 25596.33, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 2\nnames       :    Name,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Description \nmin values  :   kml_1, <center><table><tr><th colspan='2' align='center'><em>Attributes</em></th></tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSBLOCKHOUSENUMBER</th> <td></td> </tr><tr bgcolor=\"\"> <th>ADDRESSBUILDINGNAME</th> <td></td> </tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSPOSTALCODE</th> <td>100044</td> </tr><tr bgcolor=\"\"> <th>ADDRESSSTREETNAME</th> <td>44, TELOK BLANGAH DRIVE, #01 - 19/51, SINGAPORE 100044</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSTYPE</th> <td></td> </tr><tr bgcolor=\"\"> <th>DESCRIPTION</th> <td>Child Care Services</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>HYPERLINK</th> <td></td> </tr><tr bgcolor=\"\"> <th>LANDXADDRESSPOINT</th> <td></td> </tr><tr bgcolor=\"#E3E3F3\"> <th>LANDYADDRESSPOINT</th> <td></td> </tr><tr bgcolor=\"\"> <th>NAME</th> <td>PCF SPARKLETOTS PRESCHOOL @ TELOK BLANGAH BLK 44 (CC)</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>PHOTOURL</th> <td></td> </tr><tr bgcolor=\"\"> <th>ADDRESSFLOORNUMBER</th> <td></td> </tr><tr bgcolor=\"#E3E3F3\"> <th>INC_CRC</th> <td>349C54F201805938</td> </tr><tr bgcolor=\"\"> <th>FMEL_UPD_D</th> <td>20211201093837</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSUNITNUMBER</th> <td></td> </tr></table></center> \nmax values  : kml_999,                                            <center><table><tr><th colspan='2' align='center'><em>Attributes</em></th></tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSBLOCKHOUSENUMBER</th> <td></td> </tr><tr bgcolor=\"\"> <th>ADDRESSBUILDINGNAME</th> <td></td> </tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSPOSTALCODE</th> <td>99982</td> </tr><tr bgcolor=\"\"> <th>ADDRESSSTREETNAME</th> <td>35, ALLANBROOKE ROAD, SINGAPORE 099982</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSTYPE</th> <td></td> </tr><tr bgcolor=\"\"> <th>DESCRIPTION</th> <td>Child Care Services</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>HYPERLINK</th> <td></td> </tr><tr bgcolor=\"\"> <th>LANDXADDRESSPOINT</th> <td></td> </tr><tr bgcolor=\"#E3E3F3\"> <th>LANDYADDRESSPOINT</th> <td></td> </tr><tr bgcolor=\"\"> <th>NAME</th> <td>ISLANDER PRE-SCHOOL PTE LTD</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>PHOTOURL</th> <td></td> </tr><tr bgcolor=\"\"> <th>ADDRESSFLOORNUMBER</th> <td></td> </tr><tr bgcolor=\"#E3E3F3\"> <th>INC_CRC</th> <td>4F63ACF93EFABE7F</td> </tr><tr bgcolor=\"\"> <th>FMEL_UPD_D</th> <td>20211201093837</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSUNITNUMBER</th> <td></td> </tr></table></center> \n\nmpsz\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 323 \nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 15\nnames       : OBJECTID, SUBZONE_NO, SUBZONE_N, SUBZONE_C, CA_IND, PLN_AREA_N, PLN_AREA_C,       REGION_N, REGION_C,          INC_CRC, FMEL_UPD_D,     X_ADDR,     Y_ADDR,    SHAPE_Leng,    SHAPE_Area \nmin values  :        1,          1, ADMIRALTY,    AMSZ01,      N, ANG MO KIO,         AM, CENTRAL REGION,       CR, 00F5E30B5C9B7AD8,      16409,  5092.8949,  19579.069, 871.554887798, 39437.9352703 \nmax values  :      323,         17,    YUNNAN,    YSSZ09,      Y,     YISHUN,         YS,    WEST REGION,       WR, FFCCF172717C2EAF,      16409, 50424.7923, 49552.7904, 68083.9364708,  69748298.792 \n\nsg\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 60 \nextent      : 2663.926, 56047.79, 16357.98, 50244.03  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 4\nnames       : GDO_GID, MSLINK, MAPID,              COSTAL_NAM \nmin values  :       1,      1,     0,             ISLAND LINK \nmax values  :      60,     67,     0, SINGAPORE - MAIN ISLAND \n\n\nNotice that the geospatial data have been converted into their respective sp’s Spatial* classes now.\n\n\n\nspatstat requires the analytical data in ppp object form. There is no direct way to convert a Spatial* classes into ppp object. We need to convert the Spatial classes* into Spatial object first.\nThe codes chunk below converts the Spatial* classes into generic sp objects.\n\nchildcare_sp <- as(childcare, \"SpatialPoints\")\nsg_sp <- as(sg, \"SpatialPolygons\")\n\nNext, you should display the sp objects properties as shown below.\n\nchildcare_sp \n\nclass       : SpatialPoints \nfeatures    : 1925 \nextent      : 11810.03, 45404.24, 25596.33, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n\nsg_sp\n\nclass       : SpatialPolygons \nfeatures    : 60 \nextent      : 2663.926, 56047.79, 16357.98, 50244.03  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n\n\n\n\n\nNow, we will use as.ppp() function of spatstat to convert the spatial data into spatstat’s ppp object format.\n\nchildcare_ppp <- as(childcare_sp, \"ppp\")\nchildcare_ppp\n\nPlanar point pattern: 1925 points\nwindow: rectangle = [11810.03, 45404.24] x [25596.33, 49300.88] units\n\n\nNow, let us plot childcare_ppp and examine the difference.\n\nplot(childcare_ppp)\n\n\n\n\nYou can take a quick look at the summary statistics of the newly created ppp object by using the code chunk below.\n\nsummary(childcare_ppp)\n\nPlanar point pattern:  1925 points\nAverage intensity 2.417323e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: rectangle = [11810.03, 45404.24] x [25596.33, 49300.88] units\n                    (33590 x 23700 units)\nWindow area = 796335000 square units\n\n\nNotice the warning message about duplicates. In spatial point patterns analysis an issue of significant is the presence of duplicates. The statistical methodology used for spatial point patterns processes is based largely on the assumption that process are simple, that is, that the points cannot be coincident.\n\n\n\nWe can check the duplication in a ppp object by using the code chunk below.\n\nany(duplicated(childcare_ppp))\n\n[1] TRUE\n\n\nTo count the number of co-indicence point, we will use the multiplicity() function as shown in the code chunk below.\n\nmultiplicity(childcare_ppp)\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n   1    2    1    1    1    1    2    1    1    1    1    1    1    3    1    1 \n  17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32 \n   1    3    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n  33   34   35   36   37   38   39   40   41   42   43   44   45   46   47   48 \n   1    1    1    1    4    1    1    1    1    1    1    1    1    1    1    2 \n  49   50   51   52   53   54   55   56   57   58   59   60   61   62   63   64 \n   1    1    1    2    1    1    1    1    1    1    1    1    1    2    1    1 \n  65   66   67   68   69   70   71   72   73   74   75   76   77   78   79   80 \n   1    3    1    1    1    2    1   10    1    1    1    1    1    1    1    1 \n  81   82   83   84   85   86   87   88   89   90   91   92   93   94   95   96 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n  97   98   99  100  101  102  103  104  105  106  107  108  109  110  111  112 \n   1    1    1    1    1    1    1    2    1    1    3    1    1    1    2    1 \n 113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128 \n   1    2    2    2    1    1    1    1    1    1    1    1    2    1    1    1 \n 129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144 \n   1    1    1    1    1    3    1    1    1    1    1    1    1    1    1    1 \n 145  146  147  148  149  150  151  152  153  154  155  156  157  158  159  160 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 161  162  163  164  165  166  167  168  169  170  171  172  173  174  175  176 \n   1    1    2    2    2    1    1    1    1    1    2    1    4    1    1    2 \n 177  178  179  180  181  182  183  184  185  186  187  188  189  190  191  192 \n   1    1    1    1    1    1    1    1    2    1    1    1    1    1    1    1 \n 193  194  195  196  197  198  199  200  201  202  203  204  205  206  207  208 \n   3    1    1    1    1    1    3    1    1    1    1    1    1    1    1    1 \n 209  210  211  212  213  214  215  216  217  218  219  220  221  222  223  224 \n   1    1    1    1    1   10    1    1    3    1    1    1    1    1    1    1 \n 225  226  227  228  229  230  231  232  233  234  235  236  237  238  239  240 \n   1    1    1    2    1    1    1    1    1    1    1    1    1    1    1    1 \n 241  242  243  244  245  246  247  248  249  250  251  252  253  254  255  256 \n   1    1    2    6    1    2    1    1    2    1    1    1    1    1    1    1 \n 257  258  259  260  261  262  263  264  265  266  267  268  269  270  271  272 \n   3    2    3    2    1    2    1    1    2    4    1    6    6    1    1    1 \n 273  274  275  276  277  278  279  280  281  282  283  284  285  286  287  288 \n   2    1    1    1    1    2    1    1    1    1    1    1    3    1    1    1 \n 289  290  291  292  293  294  295  296  297  298  299  300  301  302  303  304 \n   1    1    4    1    2    1    1    1    1    1    1    1    1    1    1    1 \n 305  306  307  308  309  310  311  312  313  314  315  316  317  318  319  320 \n   1    1    1    1    1    1    1    1    1    1    1    2    1    1    1    1 \n 321  322  323  324  325  326  327  328  329  330  331  332  333  334  335  336 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 337  338  339  340  341  342  343  344  345  346  347  348  349  350  351  352 \n   1    1    2    1    1    1    2    1    1    1    2    1    1    1    1    1 \n 353  354  355  356  357  358  359  360  361  362  363  364  365  366  367  368 \n   1    1    1    1    2    1    2    2    1    1    1    1    2    1    1    1 \n 369  370  371  372  373  374  375  376  377  378  379  380  381  382  383  384 \n   4    1    1    1    1    2    1    1    1    1    1    1    2    1    1    1 \n 385  386  387  388  389  390  391  392  393  394  395  396  397  398  399  400 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    2 \n 401  402  403  404  405  406  407  408  409  410  411  412  413  414  415  416 \n   2    1    1    1    1    1    1    1    1    1    1    1    1    1    1    4 \n 417  418  419  420  421  422  423  424  425  426  427  428  429  430  431  432 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 433  434  435  436  437  438  439  440  441  442  443  444  445  446  447  448 \n   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    1 \n 449  450  451  452  453  454  455  456  457  458  459  460  461  462  463  464 \n   1    1    2    1    1    1    1    1    1    1    1    1    2    1    1    1 \n 465  466  467  468  469  470  471  472  473  474  475  476  477  478  479  480 \n   1    1    2    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 481  482  483  484  485  486  487  488  489  490  491  492  493  494  495  496 \n   2    2    1    1    1    1    1   10    1    2    1    1    1    2    1    3 \n 497  498  499  500  501  502  503  504  505  506  507  508  509  510  511  512 \n   1    1    1    1   10   10   10    1    1    1    1    1    1    1    1    1 \n 513  514  515  516  517  518  519  520  521  522  523  524  525  526  527  528 \n   1    1    1    2    1    2    1    1    1    1    3    1    2    1    1    1 \n 529  530  531  532  533  534  535  536  537  538  539  540  541  542  543  544 \n   1    1    1    1    1    1    3    1    1    1    1    1    2    1    1    2 \n 545  546  547  548  549  550  551  552  553  554  555  556  557  558  559  560 \n   1    1    3    1    1    1    1    1    1    1    1    2    2    2    1    1 \n 561  562  563  564  565  566  567  568  569  570  571  572  573  574  575  576 \n   2    3    1    1    1    2    1    1    1    2    2    1    1    1    1    1 \n 577  578  579  580  581  582  583  584  585  586  587  588  589  590  591  592 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    4    1    1 \n 593  594  595  596  597  598  599  600  601  602  603  604  605  606  607  608 \n   1    1    1    1    1    3    1    1    1    1    1    1    1    1    1    1 \n 609  610  611  612  613  614  615  616  617  618  619  620  621  622  623  624 \n   1    1    1    1    1    4    1    1    1    1    1    1    4    1    1    1 \n 625  626  627  628  629  630  631  632  633  634  635  636  637  638  639  640 \n   1    1    1    1    1    2    1    1    1    1    1    1    1    1    1    1 \n 641  642  643  644  645  646  647  648  649  650  651  652  653  654  655  656 \n   1    1    1    1    2    1    1    1    1    1    1    1    1    2    1    1 \n 657  658  659  660  661  662  663  664  665  666  667  668  669  670  671  672 \n   1    1    1    1    1    1    1    1    1    1    2    1    1    3    1    1 \n 673  674  675  676  677  678  679  680  681  682  683  684  685  686  687  688 \n   1    1    1    1    1    1    1    1    1   10    1    1    1    1    1    2 \n 689  690  691  692  693  694  695  696  697  698  699  700  701  702  703  704 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 705  706  707  708  709  710  711  712  713  714  715  716  717  718  719  720 \n   1    1    1    2    1    2    1   10    1    4    1    2    1    1    1    1 \n 721  722  723  724  725  726  727  728  729  730  731  732  733  734  735  736 \n   3    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 737  738  739  740  741  742  743  744  745  746  747  748  749  750  751  752 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 753  754  755  756  757  758  759  760  761  762  763  764  765  766  767  768 \n   1    3    1    1    3    1    1    1    1    2    1    1    1    1    1    1 \n 769  770  771  772  773  774  775  776  777  778  779  780  781  782  783  784 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 785  786  787  788  789  790  791  792  793  794  795  796  797  798  799  800 \n   1    1    1    1    1    1    1    1    1    1    2    1    1    1    1    1 \n 801  802  803  804  805  806  807  808  809  810  811  812  813  814  815  816 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 817  818  819  820  821  822  823  824  825  826  827  828  829  830  831  832 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 833  834  835  836  837  838  839  840  841  842  843  844  845  846  847  848 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 849  850  851  852  853  854  855  856  857  858  859  860  861  862  863  864 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 865  866  867  868  869  870  871  872  873  874  875  876  877  878  879  880 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 881  882  883  884  885  886  887  888  889  890  891  892  893  894  895  896 \n   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    2 \n 897  898  899  900  901  902  903  904  905  906  907  908  909  910  911  912 \n   1    1    1    2    1    1    1    1    1    1    1    1    1    1    1    1 \n 913  914  915  916  917  918  919  920  921  922  923  924  925  926  927  928 \n   1    1    2    1    1    1    1    1    2    2    1    1    1    1    2    1 \n 929  930  931  932  933  934  935  936  937  938  939  940  941  942  943  944 \n   1    1    2    1    2    1    1    1    2    1    1    1    2    1    1    1 \n 945  946  947  948  949  950  951  952  953  954  955  956  957  958  959  960 \n   1    1    2    1    1    2    1    1    1    1    1    1    1    1    2    1 \n 961  962  963  964  965  966  967  968  969  970  971  972  973  974  975  976 \n   1    2    2    1    1    1    1    2    1    1    1    1    2    1    1    2 \n 977  978  979  980  981  982  983  984  985  986  987  988  989  990  991  992 \n   1    1    1    1    2    1    1    1    1    1    1    1    1    1    1    1 \n 993  994  995  996  997  998  999 1000 1001 1002 1003 1004 1005 1006 1007 1008 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 \n   1    1    1    2    4    1    1    1    1    1    1    2    1    2    2    2 \n1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 \n   2    1    1    1    1    2    1    1    2    2    2    2    1    1    1    1 \n1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 \n   2    1    1    1    2    1    2    1    1    1    1    1    1    1    1    1 \n1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 \n   1    2    2    2    1    1    1    1    1    2    1    1    2    2    2    1 \n1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 \n   1    1    1    1    2    1    1    2    1    1    1    1    1    1    1    1 \n1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 \n   1    3    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 \n   2    1    2    1    2    1    1    1    1    1    1    2    2    1    1    2 \n1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 \n   1    2    1    2    1    2    1    1    1    1    1    2    1    1    1    1 \n1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 \n   1    2    1    2    2    2    2    2    1    1    1    1    1    2    1    1 \n1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 \n   1    1    1    1    1    2    1    1    2    1    1    1    1    2    1    1 \n1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 \n   1    2    1    1    1    1    2    1    1    1    1    1    1    1    1    1 \n1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 \n   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    1 \n1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 \n   1    1    1    2    1    1    1    3    1    1    1    1    1    1    1   10 \n1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 \n   2    1    3    2    1    2    1    1    2    3    2    1    1    1    1    1 \n1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 \n   1    1    1    1    1    2    1    2    1    1    1    1    1    1    1    1 \n1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 \n   1    1    1    1    1    1    1    1    1    1    4    1    1    1    1    1 \n1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 \n   2    1    1    1    2    1    2    1    1    1    1    1    1    1    1    1 \n1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 \n  10    1    2    4    1    1    1    4    1    4    1    1    1    1    1    1 \n1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 \n   1    1    1    1    1    1    1    1    1    4    2    3    2    1    1    1 \n1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 \n   2    2    1    1    1    1    1    2    2    3    1    1    1    1    1    2 \n1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 \n   2    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 \n   1    1    1    2    1    1    1    1    1    1    1    1    1    1    1    1 \n1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 \n   2    2    2    1    1    1    6    1    1    1    1    1    1    1    1    1 \n1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 \n   1    1    1    4    1    1    1    1    1    1    1    1    1    1    1    1 \n1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 \n   1    1    1    1    2    2    1    1    1    1    1    1    1    1    1    1 \n1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 \n   1    1    1    1    2    1    1    1    1    2    1    1    1    1    2    1 \n1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 \n   2    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 \n   2    1    1    1    1    1    1    3    1    1    1    1    1    1    1    1 \n1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 \n   1    1    1    1    1    1    1    1    1    6    1    1    1    1    1    1 \n1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 \n   1    1    1    1    1    1    1    3    1    1    4    1    1    2    1    1 \n1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 \n   2    1    1    1    2    1    4    1    2    1    1    1    1    1    1    1 \n1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 \n   1    1    1    1    1    1    1    1    2    1    1    2    1    1    1    1 \n1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 \n   1    1    1    1    2    1    1    3    1    1    1    2    1    1    1    1 \n1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 \n   2    1    1    1    1    1    1    2    1    1    2    1    1    1    1    1 \n1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 \n   3    1    1    2    1    1    1    1    1    1    1    1    1    2    1    1 \n1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 \n   1    1    1    1    1    1    1    2    1    1    1    1    1    1    1    1 \n1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 \n   1    1    1    4    1    1    1    6    1    1    1    1    1    1    1    1 \n1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 \n   1    1    1    2    1    1    1    2    1    1    1    1    1    2    1    1 \n1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 \n   1    2    1    1    1    1    1    1    1    1    2    2    2    1    1    1 \n1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 \n   2    1    2    1    2    1    2    1    1    2    1    2    2    2    2    1 \n1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 \n   1    1    1    1    1    2    1    1    1    2    1    1    1    1    2    1 \n1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 \n   1    4    1    4    1    4    1    1    2    1    1    1    1    1    3    1 \n1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 \n   1    1    1    2    2    2    2    2    2    2    2    1    1    2    2    2 \n1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 \n   1    2    1    1    1    1    1    2    2    2    1    2    2    2    2    1 \n1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 \n   2    1    1    1    1    1    1    1    2    2    1    2    1    1    1    1 \n1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 \n   1    1    1    1    2    1    2    2    2    2    2    2    1    1    2    1 \n1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 \n   1    1    1    2    2    2    2    2    1    1    1    2    1    1    2    2 \n1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 \n   1    2    1    1    2    1    1    2    2    2    1    2    1    2    1    1 \n1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 \n   1    1    1    1    1    1    2    1    1    1    1    4    1    1    1    1 \n1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 \n   3    1    1    2    1    1    1    2    1    1    1    1    1    2    2    1 \n1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 \n   1    1    2    1    2    2    1    1    1    1    1    2    1    1    2    1 \n1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 \n   1    3    2    2    2    1    2    1    3    1    1    1    1    1    1    1 \n1921 1922 1923 1924 1925 \n   1    1    1    1    3 \n\n\nIf we want to know how many locations have more than one point event, we can use the code chunk below.\n\nsum(multiplicity(childcare_ppp) > 1)\n\n[1] 338\n\n\nThe output shows that there are 128 duplicated point events.\nTo view the locations of these duplicate point events, we will plot childcare data by using the code chunk below.\n\ntmap_mode('view')\ntm_shape(childcare) +\n  tm_dots(alpha=0.4, \n          size=0.05)\n\n\n\n\n\n\n\n# tmap mode set to plotting\ntmap_mode('plot')\n\nThere are three ways to overcome this problem. The easiest way is to delete the duplicates. But, that will also mean that some useful point events will be lost.\nThe second solution is use jittering, which will add a small perturbation to the duplicate points so that they do not occupy the exact same space.\nThe third solution is to make each point “unique” and then attach the duplicates of the points to the patterns as marks, as attributes of the points. Then you would need analytical techniques that take into account these marks.\nThe code chunk below implements the jittering approach.\n\nchildcare_ppp_jit <- rjitter(childcare_ppp, \n                             retry=TRUE, \n                             nsim=1, \n                             drop=TRUE)\nany(duplicated(childcare_ppp_jit))\n\n[1] FALSE\n\n\nNow we can see that there are no longer any duplicates.\n\n\n\nWhen analysing spatial point patterns, it is a good practice to confine the analysis with a geographical area like Singapore boundary. In spatstat, an object called owin is specially designed to represent this polygonal region.\nThe code chunk below is used to covert sg SpatialPolygon object into owin object of spatstat.\n\nsg_owin <- as(sg_sp, \"owin\")\n\nThe output object can be displayed by using plot() function.\n\nplot(sg_owin)\n\n\n\nsummary(sg_owin)\n\nWindow: polygonal boundary\n60 separate polygons (no holes)\n            vertices        area relative.area\npolygon 1         38 1.56140e+04      2.09e-05\npolygon 2        735 4.69093e+06      6.27e-03\npolygon 3         49 1.66986e+04      2.23e-05\npolygon 4         76 3.12332e+05      4.17e-04\npolygon 5       5141 6.36179e+08      8.50e-01\npolygon 6         42 5.58317e+04      7.46e-05\npolygon 7         67 1.31354e+06      1.75e-03\npolygon 8         15 4.46420e+03      5.96e-06\npolygon 9         14 5.46674e+03      7.30e-06\npolygon 10        37 5.26194e+03      7.03e-06\npolygon 11        53 3.44003e+04      4.59e-05\npolygon 12        74 5.82234e+04      7.78e-05\npolygon 13        69 5.63134e+04      7.52e-05\npolygon 14       143 1.45139e+05      1.94e-04\npolygon 15       165 3.38736e+05      4.52e-04\npolygon 16       130 9.40465e+04      1.26e-04\npolygon 17        19 1.80977e+03      2.42e-06\npolygon 18        16 2.01046e+03      2.69e-06\npolygon 19        93 4.30642e+05      5.75e-04\npolygon 20        90 4.15092e+05      5.54e-04\npolygon 21       721 1.92795e+06      2.57e-03\npolygon 22       330 1.11896e+06      1.49e-03\npolygon 23       115 9.28394e+05      1.24e-03\npolygon 24        37 1.01705e+04      1.36e-05\npolygon 25        25 1.66227e+04      2.22e-05\npolygon 26        10 2.14507e+03      2.86e-06\npolygon 27       190 2.02489e+05      2.70e-04\npolygon 28       175 9.25904e+05      1.24e-03\npolygon 29      1993 9.99217e+06      1.33e-02\npolygon 30        38 2.42492e+04      3.24e-05\npolygon 31        24 6.35239e+03      8.48e-06\npolygon 32        53 6.35791e+05      8.49e-04\npolygon 33        41 1.60161e+04      2.14e-05\npolygon 34        22 2.54368e+03      3.40e-06\npolygon 35        30 1.08382e+04      1.45e-05\npolygon 36       327 2.16921e+06      2.90e-03\npolygon 37       111 6.62927e+05      8.85e-04\npolygon 38        90 1.15991e+05      1.55e-04\npolygon 39        98 6.26829e+04      8.37e-05\npolygon 40       415 3.25384e+06      4.35e-03\npolygon 41       222 1.51142e+06      2.02e-03\npolygon 42       107 6.33039e+05      8.45e-04\npolygon 43         7 2.48299e+03      3.32e-06\npolygon 44        17 3.28303e+04      4.38e-05\npolygon 45        26 8.34758e+03      1.11e-05\npolygon 46       177 4.67446e+05      6.24e-04\npolygon 47        16 3.19460e+03      4.27e-06\npolygon 48        15 4.87296e+03      6.51e-06\npolygon 49        66 1.61841e+04      2.16e-05\npolygon 50       149 5.63430e+06      7.53e-03\npolygon 51       609 2.62570e+07      3.51e-02\npolygon 52         8 7.82256e+03      1.04e-05\npolygon 53       976 2.33447e+07      3.12e-02\npolygon 54        55 8.25379e+04      1.10e-04\npolygon 55       976 2.33447e+07      3.12e-02\npolygon 56        61 3.33449e+05      4.45e-04\npolygon 57         6 1.68410e+04      2.25e-05\npolygon 58         4 9.45963e+03      1.26e-05\npolygon 59        46 6.99702e+05      9.35e-04\npolygon 60        13 7.00873e+04      9.36e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 748741000 square units\nFraction of frame area: 0.414\n\n\n\n\n\nIn this last step of geospatial data wrangling, we will extract childcare events that are located within Singapore by using the code chunk below.\n\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\nThe output object combined both the point and polygon feature in one ppp object class as shown below.\n\nsummary(childcareSG_ppp)\n\nPlanar point pattern:  1925 points\nAverage intensity 2.570982e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: polygonal boundary\n60 separate polygons (no holes)\n            vertices        area relative.area\npolygon 1         38 1.56140e+04      2.09e-05\npolygon 2        735 4.69093e+06      6.27e-03\npolygon 3         49 1.66986e+04      2.23e-05\npolygon 4         76 3.12332e+05      4.17e-04\npolygon 5       5141 6.36179e+08      8.50e-01\npolygon 6         42 5.58317e+04      7.46e-05\npolygon 7         67 1.31354e+06      1.75e-03\npolygon 8         15 4.46420e+03      5.96e-06\npolygon 9         14 5.46674e+03      7.30e-06\npolygon 10        37 5.26194e+03      7.03e-06\npolygon 11        53 3.44003e+04      4.59e-05\npolygon 12        74 5.82234e+04      7.78e-05\npolygon 13        69 5.63134e+04      7.52e-05\npolygon 14       143 1.45139e+05      1.94e-04\npolygon 15       165 3.38736e+05      4.52e-04\npolygon 16       130 9.40465e+04      1.26e-04\npolygon 17        19 1.80977e+03      2.42e-06\npolygon 18        16 2.01046e+03      2.69e-06\npolygon 19        93 4.30642e+05      5.75e-04\npolygon 20        90 4.15092e+05      5.54e-04\npolygon 21       721 1.92795e+06      2.57e-03\npolygon 22       330 1.11896e+06      1.49e-03\npolygon 23       115 9.28394e+05      1.24e-03\npolygon 24        37 1.01705e+04      1.36e-05\npolygon 25        25 1.66227e+04      2.22e-05\npolygon 26        10 2.14507e+03      2.86e-06\npolygon 27       190 2.02489e+05      2.70e-04\npolygon 28       175 9.25904e+05      1.24e-03\npolygon 29      1993 9.99217e+06      1.33e-02\npolygon 30        38 2.42492e+04      3.24e-05\npolygon 31        24 6.35239e+03      8.48e-06\npolygon 32        53 6.35791e+05      8.49e-04\npolygon 33        41 1.60161e+04      2.14e-05\npolygon 34        22 2.54368e+03      3.40e-06\npolygon 35        30 1.08382e+04      1.45e-05\npolygon 36       327 2.16921e+06      2.90e-03\npolygon 37       111 6.62927e+05      8.85e-04\npolygon 38        90 1.15991e+05      1.55e-04\npolygon 39        98 6.26829e+04      8.37e-05\npolygon 40       415 3.25384e+06      4.35e-03\npolygon 41       222 1.51142e+06      2.02e-03\npolygon 42       107 6.33039e+05      8.45e-04\npolygon 43         7 2.48299e+03      3.32e-06\npolygon 44        17 3.28303e+04      4.38e-05\npolygon 45        26 8.34758e+03      1.11e-05\npolygon 46       177 4.67446e+05      6.24e-04\npolygon 47        16 3.19460e+03      4.27e-06\npolygon 48        15 4.87296e+03      6.51e-06\npolygon 49        66 1.61841e+04      2.16e-05\npolygon 50       149 5.63430e+06      7.53e-03\npolygon 51       609 2.62570e+07      3.51e-02\npolygon 52         8 7.82256e+03      1.04e-05\npolygon 53       976 2.33447e+07      3.12e-02\npolygon 54        55 8.25379e+04      1.10e-04\npolygon 55       976 2.33447e+07      3.12e-02\npolygon 56        61 3.33449e+05      4.45e-04\npolygon 57         6 1.68410e+04      2.25e-05\npolygon 58         4 9.45963e+03      1.26e-05\npolygon 59        46 6.99702e+05      9.35e-04\npolygon 60        13 7.00873e+04      9.36e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 748741000 square units\nFraction of frame area: 0.414\n\nplot(childcareSG_ppp)\n\n\n\n\n\n\nThe code chunk below will be used to extract the target planning areas.\n\npg = mpsz[mpsz@data$PLN_AREA_N == \"PUNGGOL\",]\ntm = mpsz[mpsz@data$PLN_AREA_N == \"TAMPINES\",]\nck = mpsz[mpsz@data$PLN_AREA_N == \"CHOA CHU KANG\",]\njw = mpsz[mpsz@data$PLN_AREA_N == \"JURONG WEST\",]\n\nPlotting target planning areas\n\npar(mfrow=c(2,2))\nplot(pg, main = \"Ponggol\")\nplot(tm, main = \"Tampines\")\nplot(ck, main = \"Choa Chu Kang\")\nplot(jw, main = \"Jurong West\")\n\n\n\n\n\n\n\nNext, we will convert these SpatialPolygonsDataFrame layers into generic spatialpolygons layers.\n\npg_sp = as(pg, \"SpatialPolygons\")\ntm_sp = as(tm, \"SpatialPolygons\")\nck_sp = as(ck, \"SpatialPolygons\")\njw_sp = as(jw, \"SpatialPolygons\")\n\n\n\n\nNow, we will convert these SpatialPolygons objects into owin objects that is required by spatstat.\n\npg_owin = as(pg_sp, \"owin\")\ntm_owin = as(tm_sp, \"owin\")\nck_owin = as(ck_sp, \"owin\")\njw_owin = as(jw_sp, \"owin\")\n\n\n\n\nBy using the code chunk below, we are able to extract childcare that is within the specific region to do our analysis later on.\n\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin]\nchildcare_ck_ppp = childcare_ppp_jit[ck_owin]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\nNext, rescale() function is used to trasnform the unit of measurement from metre to kilometre.\n\nchildcare_pg_ppp.km = rescale(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale(childcare_tm_ppp, 1000, \"km\")\nchildcare_ck_ppp.km = rescale(childcare_ck_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale(childcare_jw_ppp, 1000, \"km\")\n\nThe code chunk below is used to plot these four study areas and the locations of the childcare centres.\n\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main=\"Punggol\")\nplot(childcare_tm_ppp.km, main=\"Tampines\")\nplot(childcare_ck_ppp.km, main=\"Choa Chu Kang\")\nplot(childcare_jw_ppp.km, main=\"Jurong West\")\n\n\n\n\n\n\n\n\n\nThe G function measures the distribution of the distances from an arbitrary event to its nearest event. In this section, you will learn how to compute G-function estimation by using Gest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\n\n\n\nThe code chunk below is used to compute G-function using Gest() of spatat package.\n\nG_CK = Gest(childcare_ck_ppp, correction = \"border\")\nplot(G_CK, xlim=c(0,500))\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nMonte Carlo test with G-fucntion\n\nG_CK.csr <- envelope(childcare_ck_ppp, Gest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\nplot(G_CK.csr)\n\n\n\n\n\n\n\n\n\n\n\nG_tm = Gest(childcare_tm_ppp, correction = \"best\")\nplot(G_tm)\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected is p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\nG_tm.csr <- envelope(childcare_tm_ppp, Gest, correction = \"all\", nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\nplot(G_tm.csr)\n\n\n\n\n\n\n\n\n\nThe F function estimates the empty space function F(r) or its hazard rate h(r) from a point pattern in a window of arbitrary shape. In this section, you will learn how to compute F-function estimation by using Fest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\n\n\n\nThe code chunk below is used to compute F-function using Fest() of spatat package.\n\nF_CK = Fest(childcare_ck_ppp)\nplot(F_CK)\n\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nMonte Carlo test with F-fucntion\n\nF_CK.csr <- envelope(childcare_ck_ppp, Fest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\nplot(F_CK.csr)\n\n\n\n\n\n\n\n\n\nMonte Carlo test with F-fucntion\n\nF_tm = Fest(childcare_tm_ppp, correction = \"best\")\nplot(F_tm)\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected is p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\nF_tm.csr <- envelope(childcare_tm_ppp, Fest, correction = \"all\", nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\nplot(F_tm.csr)\n\n\n\n\n\n\n\n\n\nK-function measures the number of events found up to a given distance of any particular event. In this section, you will learn how to compute K-function estimates by using Kest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\n\n\n\n\nK_ck = Kest(childcare_ck_ppp, correction = \"Ripley\")\nplot(K_ck, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\nK_ck.csr <- envelope(childcare_ck_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\nplot(K_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"K(d)-r\")\n\n\n\n\n\n\n\n\n\n\n\nK_tm = Kest(childcare_tm_ppp, correction = \"Ripley\")\nplot(K_tm, . -r ~ r, \n     ylab= \"K(d)-r\", xlab = \"d(m)\", \n     xlim=c(0,1000))\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\nK_tm.csr <- envelope(childcare_tm_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\nplot(K_tm.csr, . - r ~ r, \n     xlab=\"d\", ylab=\"K(d)-r\", xlim=c(0,500))\n\n\n\n\n\n\n\n\n\nIn this section, you will learn how to compute L-function estimation by using Lest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\n\n\n\n\nL_ck = Lest(childcare_ck_ppp, correction = \"Ripley\")\nplot(L_ck, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value if smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\nL_ck.csr <- envelope(childcare_ck_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\nplot(L_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")\n\n\n\n\n\n\n\n\n\n\n\nL_tm = Lest(childcare_tm_ppp, correction = \"Ripley\")\nplot(L_tm, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\", \n     xlim=c(0,1000))\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nThe code chunk below will be used to perform the hypothesis testing.\n\nL_tm.csr <- envelope(childcare_tm_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\nplot(L_tm.csr, . - r ~ r, \n     xlab=\"d\", ylab=\"L(d)-r\", xlim=c(0,500))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "",
    "text": "Welcome to my first hands-on exercise of this module, IS415 Geospatial and Applications!\nIn this exercise, our primary aim is to delve into geospatial data science tasks using the capabilities of the sf package in R.\n\nInstalling and loading sf and tidyverse packages into R environment\nImporting geospatial and aspatial data by using appropriate functions (sf and readr package)\nExploring the content of simple feature dataframe\nAssigning and transforming coordinate systems\nPerforming geoprocessing, data wrangling and Exploratory Data Analysis (EDA) tasks"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#installing-and-loading-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#installing-and-loading-r-packages",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Installing and loading R packages",
    "text": "Installing and loading R packages\nIn this section, I will install and load tidyverse and sf packages.\n\npacman::p_load(tidyverse, sf)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-polygon-feature-data-in-shapefile-format",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-polygon-feature-data-in-shapefile-format",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Importing polygon feature data in shapefile format",
    "text": "Importing polygon feature data in shapefile format\n\nmpsz <- st_read(dsn = \"data/geospatial\", \n                  layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\jaymieseet\\IS415-GAA-JS\\Hands-on_Ex\\Hands-on_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-polyline-feature-data-in-shapefile-form",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-polyline-feature-data-in-shapefile-form",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Importing polyline feature data in shapefile form",
    "text": "Importing polyline feature data in shapefile form\n\ncyclingpath <- st_read(dsn = \"data/geospatial\", \n                  layer = \"CyclingPathGazette\")\n\nReading layer `CyclingPathGazette' from data source \n  `C:\\jaymieseet\\IS415-GAA-JS\\Hands-on_Ex\\Hands-on_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2558 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-gis-data-in-kml-format",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-gis-data-in-kml-format",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Importing GIS data in kml format",
    "text": "Importing GIS data in kml format\n\npreschool = st_read(\"data/geospatial/PreSchoolsLocation.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `C:\\jaymieseet\\IS415-GAA-JS\\Hands-on_Ex\\Hands-on_Ex01\\data\\geospatial\\PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#plotting-the-geometry",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#plotting-the-geometry",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Plotting the geometry",
    "text": "Plotting the geometry"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#plotting-a-specific-attribute-eg.-pln_area_n",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#plotting-a-specific-attribute-eg.-pln_area_n",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Plotting a specific attribute eg. “PLN_AREA_N”",
    "text": "Plotting a specific attribute eg. “PLN_AREA_N”"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#assigning-the-correct-epsg-code-3414-to-mpsz-dataframe",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#assigning-the-correct-epsg-code-3414-to-mpsz-dataframe",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Assigning the correct EPSG code (3414) to mpsz dataframe",
    "text": "Assigning the correct EPSG code (3414) to mpsz dataframe\n\nmpsz3414 <- st_set_crs(mpsz, 3414)\nst_crs(mpsz3414)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#transforming-the-projection-of-preschool-from-wgs84-to-svy21",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#transforming-the-projection-of-preschool-from-wgs84-to-svy21",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Transforming the projection of preschool from WGS84 to SVY21",
    "text": "Transforming the projection of preschool from WGS84 to SVY21\n\npreschool3414 <- st_transform(preschool, \n                              crs = 3414)\nst_crs(preschool3414)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#creating-a-simple-feature-dataframe-from-an-aspatial-dataframe",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#creating-a-simple-feature-dataframe-from-an-aspatial-dataframe",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Creating a simple feature dataframe from an aspatial dataframe",
    "text": "Creating a simple feature dataframe from an aspatial dataframe\n\nlistings_sf <- st_as_sf(listings, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %>%\n  st_transform(crs = 3414)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#examining-the-contents-of-the-newly-created-simple-data-frame",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#examining-the-contents-of-the-newly-created-simple-data-frame",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Examining the contents of the newly created simple data frame",
    "text": "Examining the contents of the newly created simple data frame\n\n\nRows: 3,457\nColumns: 74\n$ id                                           <dbl> 71609, 71896, 71903, 2753…\n$ listing_url                                  <chr> \"https://www.airbnb.com/r…\n$ scrape_id                                    <dbl> 2.023123e+13, 2.023123e+1…\n$ last_scraped                                 <date> 2023-12-27, 2023-12-26, …\n$ source                                       <chr> \"previous scrape\", \"city …\n$ name                                         <chr> \"Villa in Singapore · ★4.…\n$ description                                  <lgl> NA, NA, NA, NA, NA, NA, N…\n$ neighborhood_overview                        <chr> NA, NA, \"Quiet and view o…\n$ picture_url                                  <chr> \"https://a0.muscache.com/…\n$ host_id                                      <dbl> 367042, 367042, 367042, 1…\n$ host_url                                     <chr> \"https://www.airbnb.com/u…\n$ host_name                                    <chr> \"Belinda\", \"Belinda\", \"Be…\n$ host_since                                   <date> 2011-01-29, 2011-01-29, …\n$ host_location                                <chr> \"Singapore\", \"Singapore\",…\n$ host_about                                   <chr> \"Hi My name is Belinda -H…\n$ host_response_time                           <chr> \"N/A\", \"N/A\", \"N/A\", \"wit…\n$ host_response_rate                           <chr> \"N/A\", \"N/A\", \"N/A\", \"100…\n$ host_acceptance_rate                         <chr> \"100%\", \"100%\", \"100%\", \"…\n$ host_is_superhost                            <lgl> FALSE, FALSE, FALSE, FALS…\n$ host_thumbnail_url                           <chr> \"https://a0.muscache.com/…\n$ host_picture_url                             <chr> \"https://a0.muscache.com/…\n$ host_neighbourhood                           <chr> \"Tampines\", \"Tampines\", \"…\n$ host_listings_count                          <dbl> 5, 5, 5, 51, 51, 5, 7, 51…\n$ host_total_listings_count                    <dbl> 15, 15, 15, 68, 68, 15, 8…\n$ host_verifications                           <chr> \"['email', 'phone']\", \"['…\n$ host_has_profile_pic                         <lgl> TRUE, TRUE, TRUE, TRUE, T…\n$ host_identity_verified                       <lgl> TRUE, TRUE, TRUE, TRUE, T…\n$ neighbourhood                                <chr> NA, NA, \"Singapore, Singa…\n$ neighbourhood_cleansed                       <chr> \"Tampines\", \"Tampines\", \"…\n$ neighbourhood_group_cleansed                 <chr> \"East Region\", \"East Regi…\n$ property_type                                <chr> \"Private room in villa\", …\n$ room_type                                    <chr> \"Private room\", \"Private …\n$ accommodates                                 <dbl> 3, 1, 2, 1, 1, 4, 2, 1, 1…\n$ bathrooms                                    <lgl> NA, NA, NA, NA, NA, NA, N…\n$ bathrooms_text                               <chr> \"1 private bath\", \"Shared…\n$ bedrooms                                     <lgl> NA, NA, NA, NA, NA, NA, N…\n$ beds                                         <dbl> 3, 1, 2, 1, 1, 5, 1, 1, 1…\n$ amenities                                    <chr> \"[]\", \"[]\", \"[]\", \"[]\", \"…\n$ price                                        <chr> \"$150.00\", \"$80.00\", \"$80…\n$ minimum_nights                               <dbl> 92, 92, 92, 60, 60, 92, 9…\n$ maximum_nights                               <dbl> 365, 365, 365, 999, 999, …\n$ minimum_minimum_nights                       <dbl> 92, 92, 92, 60, 60, 92, 9…\n$ maximum_minimum_nights                       <dbl> 92, 92, 92, 60, 60, 92, 9…\n$ minimum_maximum_nights                       <dbl> 1125, 1125, 1125, 1125, 1…\n$ maximum_maximum_nights                       <dbl> 1125, 1125, 1125, 1125, 1…\n$ minimum_nights_avg_ntm                       <dbl> 92, 92, 92, 60, 60, 92, 9…\n$ maximum_nights_avg_ntm                       <dbl> 1125, 1125, 1125, 1125, 1…\n$ calendar_updated                             <lgl> NA, NA, NA, NA, NA, NA, N…\n$ has_availability                             <lgl> TRUE, TRUE, TRUE, TRUE, T…\n$ availability_30                              <dbl> 30, 30, 30, 6, 6, 29, 30,…\n$ availability_60                              <dbl> 34, 60, 60, 6, 6, 33, 60,…\n$ availability_90                              <dbl> 55, 90, 90, 6, 6, 54, 90,…\n$ availability_365                             <dbl> 55, 91, 91, 183, 183, 54,…\n$ calendar_last_scraped                        <date> 2023-12-27, 2023-12-26, …\n$ number_of_reviews                            <dbl> 19, 24, 46, 20, 16, 12, 1…\n$ number_of_reviews_ltm                        <dbl> 0, 0, 0, 0, 3, 0, 0, 1, 2…\n$ number_of_reviews_l30d                       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ first_review                                 <date> 2011-12-19, 2011-07-30, …\n$ last_review                                  <date> 2020-01-17, 2019-10-13, …\n$ review_scores_rating                         <dbl> 4.44, 4.16, 4.41, 4.40, 4…\n$ review_scores_accuracy                       <dbl> 4.37, 4.22, 4.39, 4.16, 4…\n$ review_scores_cleanliness                    <dbl> 4.00, 4.09, 4.52, 4.26, 4…\n$ review_scores_checkin                        <dbl> 4.63, 4.43, 4.63, 4.47, 4…\n$ review_scores_communication                  <dbl> 4.78, 4.43, 4.64, 4.42, 4…\n$ review_scores_location                       <dbl> 4.26, 4.17, 4.50, 4.53, 4…\n$ review_scores_value                          <dbl> 4.32, 4.04, 4.36, 4.63, 4…\n$ license                                      <chr> NA, NA, NA, \"S0399\", \"S03…\n$ instant_bookable                             <lgl> FALSE, FALSE, FALSE, TRUE…\n$ calculated_host_listings_count               <dbl> 5, 5, 5, 51, 51, 5, 7, 51…\n$ calculated_host_listings_count_entire_homes  <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0…\n$ calculated_host_listings_count_private_rooms <dbl> 5, 5, 5, 51, 51, 5, 6, 51…\n$ calculated_host_listings_count_shared_rooms  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ reviews_per_month                            <dbl> 0.13, 0.16, 0.30, 0.15, 0…\n$ geometry                                     <POINT [m]> POINT (41972.5 3639…"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#buffering",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#buffering",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Buffering",
    "text": "Buffering\nBuffer of 5 metres of reserved land on the both sides of the current cycling path\n\nbuffer_cycling <- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 30)\nbuffer_cycling$AREA <- st_area(buffer_cycling)\nsum(buffer_cycling$AREA)\n\n1774367 [m^2]"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#point-in-polygon-count",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#point-in-polygon-count",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Point-in-Polygon Count",
    "text": "Point-in-Polygon Count\n\nmpsz3414$`PreSch Count`<- lengths(st_intersects(mpsz3414, preschool3414))\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    4.00    7.09   10.00   72.00 \n\ntop_n(mpsz3414, 1, `PreSch Count`)\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 39655.33 ymin: 35966 xmax: 42940.57 ymax: 38622.37\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO     SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      189          2 TAMPINES EAST    TMSZ02      N   TAMPINES         TM\n     REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR SHAPE_Leng\n1 EAST REGION       ER 21658EAAF84F4D8D 2014-12-05 41122.55 37392.39   10180.62\n  SHAPE_Area                       geometry PreSch Count\n1    4339824 MULTIPOLYGON (((42196.76 38...           72"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#density-of-preschools-by-planning-subzone",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#density-of-preschools-by-planning-subzone",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Density of Preschools by planning subzone",
    "text": "Density of Preschools by planning subzone\n\nmpsz3414$Area <- mpsz3414 %>%\n  st_area()\nmpsz3414 <- mpsz3414 %>%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "title": "Hands-on Exercise 2: Thematic Mapping and Geovisualisation with R",
    "section": "",
    "text": "pacman::p_load(sf, tmap, tidyverse)\n\n\n\n\n\n\n\nmpsz <- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\jaymieseet\\IS415-GAA-JS\\Hands-on_Ex\\Hands-on_Ex02\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\n\n\nmpsz \n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\n\n\npopdata <- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#joining-the-attribute-data-and-geospatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#joining-the-attribute-data-and-geospatial-data",
    "title": "Hands-on Exercise 2: Thematic Mapping and Geovisualisation with R",
    "section": "Joining the attribute data and geospatial data",
    "text": "Joining the attribute data and geospatial data\n\npopdata2020 <- popdata2020 %>%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = list(toupper)) %>%\n  filter(`ECONOMY ACTIVE` > 0)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#left-join-to-join-the-geographical-data-and-attribute-table-using-planning-subzone-name",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#left-join-to-join-the-geographical-data-and-attribute-table-using-planning-subzone-name",
    "title": "Hands-on Exercise 2: Thematic Mapping and Geovisualisation with R",
    "section": "Left-join to join the geographical data and attribute table using planning subzone name",
    "text": "Left-join to join the geographical data and attribute table using planning subzone name\n\nmpsz_pop2020 <- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#creating-a-choropleth-map-by-using-tmaps-elements",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#creating-a-choropleth-map-by-using-tmaps-elements",
    "title": "Hands-on Exercise 2: Thematic Mapping and Geovisualisation with R",
    "section": "Creating a choropleth map by using tmap’s elements",
    "text": "Creating a choropleth map by using tmap’s elements\n\n\n\n\n\n\nDrawing a base map\n\n\n\n\n\n\n\nDrawing a choropleth map using tm_polygons()\n\n\n\n\n\n\n\nDrawing a choropleth map using tm_fill() and tm_border()\n\n\n\n\n\nTo add the boundary of the planning subzones"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#data-classfication-methods-of-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#data-classfication-methods-of-tmap",
    "title": "Hands-on Exercise 2: Thematic Mapping and Geovisualisation with R",
    "section": "Data classfication methods of tmap",
    "text": "Data classfication methods of tmap\n\n\n\n\n\nEqual data classification method is used"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#plotting-choropleth-map-with-custome-break",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#plotting-choropleth-map-with-custome-break",
    "title": "Hands-on Exercise 2: Thematic Mapping and Geovisualisation with R",
    "section": "Plotting choropleth map with custome break",
    "text": "Plotting choropleth map with custome break\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nSetting break point at 0.60, 0.70, 0.80, and 0.90. We also include a minimum and maximum, which is set at 0 and 10\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#colour-scheme",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#colour-scheme",
    "title": "Hands-on Exercise 2: Thematic Mapping and Geovisualisation with R",
    "section": "Colour Scheme",
    "text": "Colour Scheme\n\nUsing ColourBrewer palette\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nReverse the colour shading with a ‘-’ prefix\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#map-layouts",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#map-layouts",
    "title": "Hands-on Exercise 2: Thematic Mapping and Geovisualisation with R",
    "section": "Map Layouts",
    "text": "Map Layouts\n\nMap Legend\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\nMap Style\n“classic” style is used\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n“Cartographic Furniture” used to draw other map furniture such as compass, scale bar and grid lines\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#drawing-small-multiple-choropleth-maps",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#drawing-small-multiple-choropleth-maps",
    "title": "Hands-on Exercise 2: Thematic Mapping and Geovisualisation with R",
    "section": "Drawing Small Multiple Choropleth Maps",
    "text": "Drawing Small Multiple Choropleth Maps\n\nBy assigning multiple values to at least one of the aesthetic arguments\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\nSmall multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#creating-multiple-small-choropleth-maps-by-defining-a-group-by-variable-in-tm_facets",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#creating-multiple-small-choropleth-maps-by-defining-a-group-by-variable-in-tm_facets",
    "title": "Hands-on Exercise 2: Thematic Mapping and Geovisualisation with R",
    "section": "Creating multiple small choropleth maps by defining a group-by variable in tm_facets()",
    "text": "Creating multiple small choropleth maps by defining a group-by variable in tm_facets()\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#by-creating-multiple-stand-alone-maps-with-tmap_arrange",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#by-creating-multiple-stand-alone-maps-with-tmap_arrange",
    "title": "Hands-on Exercise 2: Thematic Mapping and Geovisualisation with R",
    "section": "By creating multiple stand-alone maps with tmap_arrange()",
    "text": "By creating multiple stand-alone maps with tmap_arrange()\n\nyoungmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#mapping-spatial-object-meeting-a-selection-criterion",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#mapping-spatial-object-meeting-a-selection-criterion",
    "title": "Hands-on Exercise 2: Thematic Mapping and Geovisualisation with R",
    "section": "Mapping Spatial Object Meeting a Selection Criterion",
    "text": "Mapping Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth maps, use selection function to map spatial objects meeting the selection criterion\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html",
    "title": "Hands-on Exercise 3",
    "section": "",
    "text": "Data Set\nDescription\nSource\n\n\n\n\nCHILDCARE\nPoint feature data providing both location and attribute information of childcare centres\ndata.gov.sg (geojson format)\n\n\nMP14_SUBZONE_WEB_PL\nPolygon feature data providing information of URA 2024 Master Plan Planning Subzone boundary data\ndata.gov.sg (ESRI shapefile format)\n\n\nCoastalOutline\nPolygon feature data showing the national boundary of Singapore\nSingapore Land Authority (ESRI shapefile format)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#importing-the-spatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#importing-the-spatial-data",
    "title": "Hands-on Exercise 3",
    "section": "Importing the spatial data",
    "text": "Importing the spatial data\n\n\nShow the code\nchildcare_sf <- st_read(\"data/geospatial/ChildCareServices.geojson\") %>%\n  st_transform(crs = 3414)\n\n\nReading layer `ChildCareServices' from data source \n  `C:\\jaymieseet\\IS415-GAA-JS\\Hands-on_Ex\\Hands-on_Ex03\\data\\geospatial\\ChildCareServices.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1925 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\n\nShow the code\nsg_sf <- st_read(dsn = \"data/geospatial\", layer=\"CostalOutline\")\n\n\nReading layer `CostalOutline' from data source \n  `C:\\jaymieseet\\IS415-GAA-JS\\Hands-on_Ex\\Hands-on_Ex03\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21\n\n\nShow the code\nmpsz <- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\jaymieseet\\IS415-GAA-JS\\Hands-on_Ex\\Hands-on_Ex03\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#transforming-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#transforming-the-data",
    "title": "Hands-on Exercise 3",
    "section": "Transforming the data",
    "text": "Transforming the data\n\n\nShow the code\nst_geometry(mpsz)\n\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nShow the code\nmpsz_sf <- st_set_crs(mpsz,3414)\nmpsz_sf\n\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\nShow the code\nsg_sf\n\n\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21\nFirst 10 features:\n   GDO_GID MSLINK MAPID              COSTAL_NAM                       geometry\n1        1      1     0                 Linkway POLYGON ((14362.86 32307.49...\n2        2      3     0                 SENTOSA POLYGON ((25683.97 26236.91...\n3        3      5     0          PULAU SARIMBUN POLYGON ((11471.97 46273.01...\n4        4      6     0           PULAU SAMULUN POLYGON ((12602.3 32061.35,...\n5        5      7     0 SINGAPORE - MAIN ISLAND POLYGON ((17915.53 46770.73...\n6        6      8     0            PULAU KEPPEL POLYGON ((25606.84 27481.21...\n7        7      9     0             PULAU BRANI POLYGON ((27778.17 27321.28...\n8        8     10     0                   ISLET POLYGON ((25874.11 26121.48...\n9        9     11     0           PULAU PALAWAN POLYGON ((25937.33 25797.66...\n10      10     12     0                   ISLET POLYGON ((27459.33 24854.08...\n\n\n\n\nShow the code\ntmap_mode('view')\ntm_shape(childcare_sf)+\n  tm_dots()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#converting-sf-dataframes-to-sps-spatial-class",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#converting-sf-dataframes-to-sps-spatial-class",
    "title": "Hands-on Exercise 3",
    "section": "Converting sf dataframes to sp’s spatial class",
    "text": "Converting sf dataframes to sp’s spatial class\n\n\nShow the code\nchildcare <- as_Spatial(childcare_sf)\nmpsz <- as_Spatial(mpsz_sf)\nsg <- as_Spatial(sg_sf)\n\n\nInspecting the information\n\n\nShow the code\nchildcare\n\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1925 \nextent      : 11810.03, 45404.24, 25596.33, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 2\nnames       :    Name,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Description \nmin values  :   kml_1, <center><table><tr><th colspan='2' align='center'><em>Attributes</em></th></tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSBLOCKHOUSENUMBER</th> <td></td> </tr><tr bgcolor=\"\"> <th>ADDRESSBUILDINGNAME</th> <td></td> </tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSPOSTALCODE</th> <td>100044</td> </tr><tr bgcolor=\"\"> <th>ADDRESSSTREETNAME</th> <td>44, TELOK BLANGAH DRIVE, #01 - 19/51, SINGAPORE 100044</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSTYPE</th> <td></td> </tr><tr bgcolor=\"\"> <th>DESCRIPTION</th> <td>Child Care Services</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>HYPERLINK</th> <td></td> </tr><tr bgcolor=\"\"> <th>LANDXADDRESSPOINT</th> <td></td> </tr><tr bgcolor=\"#E3E3F3\"> <th>LANDYADDRESSPOINT</th> <td></td> </tr><tr bgcolor=\"\"> <th>NAME</th> <td>PCF SPARKLETOTS PRESCHOOL @ TELOK BLANGAH BLK 44 (CC)</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>PHOTOURL</th> <td></td> </tr><tr bgcolor=\"\"> <th>ADDRESSFLOORNUMBER</th> <td></td> </tr><tr bgcolor=\"#E3E3F3\"> <th>INC_CRC</th> <td>349C54F201805938</td> </tr><tr bgcolor=\"\"> <th>FMEL_UPD_D</th> <td>20211201093837</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSUNITNUMBER</th> <td></td> </tr></table></center> \nmax values  : kml_999,                                            <center><table><tr><th colspan='2' align='center'><em>Attributes</em></th></tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSBLOCKHOUSENUMBER</th> <td></td> </tr><tr bgcolor=\"\"> <th>ADDRESSBUILDINGNAME</th> <td></td> </tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSPOSTALCODE</th> <td>99982</td> </tr><tr bgcolor=\"\"> <th>ADDRESSSTREETNAME</th> <td>35, ALLANBROOKE ROAD, SINGAPORE 099982</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSTYPE</th> <td></td> </tr><tr bgcolor=\"\"> <th>DESCRIPTION</th> <td>Child Care Services</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>HYPERLINK</th> <td></td> </tr><tr bgcolor=\"\"> <th>LANDXADDRESSPOINT</th> <td></td> </tr><tr bgcolor=\"#E3E3F3\"> <th>LANDYADDRESSPOINT</th> <td></td> </tr><tr bgcolor=\"\"> <th>NAME</th> <td>ISLANDER PRE-SCHOOL PTE LTD</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>PHOTOURL</th> <td></td> </tr><tr bgcolor=\"\"> <th>ADDRESSFLOORNUMBER</th> <td></td> </tr><tr bgcolor=\"#E3E3F3\"> <th>INC_CRC</th> <td>4F63ACF93EFABE7F</td> </tr><tr bgcolor=\"\"> <th>FMEL_UPD_D</th> <td>20211201093837</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSUNITNUMBER</th> <td></td> </tr></table></center> \n\n\nShow the code\nmpsz\n\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 323 \nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 15\nnames       : OBJECTID, SUBZONE_NO, SUBZONE_N, SUBZONE_C, CA_IND, PLN_AREA_N, PLN_AREA_C,       REGION_N, REGION_C,          INC_CRC, FMEL_UPD_D,     X_ADDR,     Y_ADDR,    SHAPE_Leng,    SHAPE_Area \nmin values  :        1,          1, ADMIRALTY,    AMSZ01,      N, ANG MO KIO,         AM, CENTRAL REGION,       CR, 00F5E30B5C9B7AD8,      16409,  5092.8949,  19579.069, 871.554887798, 39437.9352703 \nmax values  :      323,         17,    YUNNAN,    YSSZ09,      Y,     YISHUN,         YS,    WEST REGION,       WR, FFCCF172717C2EAF,      16409, 50424.7923, 49552.7904, 68083.9364708,  69748298.792 \n\n\nShow the code\nsg\n\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 60 \nextent      : 2663.926, 56047.79, 16357.98, 50244.03  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +datum=WGS84 +units=m +no_defs \nvariables   : 4\nnames       : GDO_GID, MSLINK, MAPID,              COSTAL_NAM \nmin values  :       1,      1,     0,             ISLAND LINK \nmax values  :      60,     67,     0, SINGAPORE - MAIN ISLAND"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#converting-the-spatial-class-into-generic-sp-format",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#converting-the-spatial-class-into-generic-sp-format",
    "title": "Hands-on Exercise 3",
    "section": "Converting the spatial class into generic sp format",
    "text": "Converting the spatial class into generic sp format\n\n\nShow the code\nchildcare_sp <- as(childcare, \"SpatialPoints\")\nsg_sp <- as(sg, \"SpatialPolygons\")\nchildcare_sp\n\n\nclass       : SpatialPoints \nfeatures    : 1925 \nextent      : 11810.03, 45404.24, 25596.33, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n\n\nShow the code\nsg_sp\n\n\nclass       : SpatialPolygons \nfeatures    : 60 \nextent      : 2663.926, 56047.79, 16357.98, 50244.03  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +datum=WGS84 +units=m +no_defs"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#converting-the-genric-sp-format-into-spatstats-ppp-format",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#converting-the-genric-sp-format-into-spatstats-ppp-format",
    "title": "Hands-on Exercise 3",
    "section": "Converting the genric sp format into spatstat’s ppp format",
    "text": "Converting the genric sp format into spatstat’s ppp format\n\n\nShow the code\ninstall.packages(\"maptools\", repos = \"https://packagemanager.posit.co/cran/2023-10-13\")\nchildcare_ppp <- as(childcare_sp, \"ppp\")\nchildcare_ppp\n\n\nPlanar point pattern: 1925 points\nwindow: rectangle = [11810.03, 45404.24] x [25596.33, 49300.88] units\n\n\nShow the code\n#plot(childcare_ppp)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html",
    "title": "Hands-on Exercise 4",
    "section": "",
    "text": "In this exercise, we will focus on computing spatial weights using R. Some of the main learning objectives are:\n\nimporting geospatial data and csv file\ncompute spatial weights and calculating spatially lagged variables"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#performing-relational-join",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#performing-relational-join",
    "title": "Hands-on Exercise 4",
    "section": "Performing relational join",
    "text": "Performing relational join\nIn order to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe.\nThis is performed using left_join() of dplyr package.\n\n\nShow the code\nhunan <- left_join(hunan,hunan2012)%>%\n  select(1:4, 7, 15)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#computing-queen-contiguity-based-neighbours",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#computing-queen-contiguity-based-neighbours",
    "title": "Hands-on Exercise 4",
    "section": "Computing (QUEEN) contiguity based neighbours",
    "text": "Computing (QUEEN) contiguity based neighbours\n\n\nShow the code\nwm_q <- poly2nb(hunan, queen=TRUE)\nsummary(wm_q)\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nIn this code chunk, “queen” argument is set to true. If you do not specify queen=FALSE, this function will return a list of first order neighbours using the Queen criteria.\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one neighbours.\nWe can use the function wm_q to list all neighbouring polygons of a specific polygon in our polygon object. For example, to see the neighbours for the first polygon in the object:\n\n\nShow the code\nwm_q[[1]]\n\n\n[1]  2  3  4 57 85\n\n\nFrom the output, we know that polygon 1 has 5 neighbours. The numbers prined are the polygon IDs as stored in hunan SpatialPolygonsDataFrame.\nWe can then retrive the county name of Polygon 1:\n\n\nShow the code\nhunan$County[1]\n\n\n[1] \"Anxiang\"\n\n\nThis reveals that polygon 1 is Anxiang county\nTo further understand its neighbours, we can retrieve the county names of the five neighbouring polygons:\n\n\nShow the code\nhunan$NAME_3[c(2,3,4,57,85)]\n\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\nWe can also retrive the GDPPC of the five neighbouring countries:\n\n\nShow the code\nnb1 <- wm_q[[1]]\nnb1 <- hunan$GDPPC[nb1]\nnb1\n\n\n[1] 20981 34592 24473 21311 22879\n\n\nLastly, we can display the complete weight matrix by using str():\n\n\nShow the code\nstr(wm_q)\n\n\nList of 88\n $ : int [1:5] 2 3 4 57 85\n $ : int [1:5] 1 57 58 78 85\n $ : int [1:4] 1 4 5 85\n $ : int [1:4] 1 3 5 6\n $ : int [1:4] 3 4 6 85\n $ : int [1:5] 4 5 69 75 85\n $ : int [1:4] 67 71 74 84\n $ : int [1:7] 9 46 47 56 78 80 86\n $ : int [1:6] 8 66 68 78 84 86\n $ : int [1:8] 16 17 19 20 22 70 72 73\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:3] 11 15 17\n $ : int [1:4] 13 14 17 83\n $ : int [1:5] 10 17 22 72 83\n $ : int [1:7] 10 11 14 15 16 72 83\n $ : int [1:5] 20 22 23 77 83\n $ : int [1:6] 10 20 21 73 74 86\n $ : int [1:7] 10 18 19 21 22 23 82\n $ : int [1:5] 19 20 35 82 86\n $ : int [1:5] 10 16 18 20 83\n $ : int [1:7] 18 20 38 41 77 79 82\n $ : int [1:5] 25 28 31 32 54\n $ : int [1:5] 24 28 31 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:3] 26 29 42\n $ : int [1:5] 24 25 33 49 54\n $ : int [1:3] 27 37 42\n $ : int 33\n $ : int [1:8] 24 25 32 36 39 40 56 81\n $ : int [1:8] 24 31 50 54 55 56 75 85\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 47 80 82 86\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:4] 29 42 43 44\n $ : int [1:4] 23 44 77 79\n $ : int [1:5] 31 40 42 43 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:7] 26 27 29 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:4] 37 38 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:3] 8 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:5] 32 48 52 54 55\n $ : int [1:3] 48 49 52\n $ : int [1:5] 48 49 50 51 54\n $ : int [1:3] 48 55 75\n $ : int [1:6] 24 28 32 49 50 52\n $ : int [1:5] 32 48 50 53 75\n $ : int [1:7] 8 31 32 36 78 80 85\n $ : int [1:6] 1 2 58 64 76 85\n $ : int [1:5] 2 57 68 76 78\n $ : int [1:4] 60 61 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:7] 12 59 60 62 63 77 87\n $ : int [1:3] 61 77 87\n $ : int [1:4] 12 61 77 83\n $ : int [1:2] 57 76\n $ : int 76\n $ : int [1:5] 9 67 68 76 84\n $ : int [1:4] 7 66 76 84\n $ : int [1:5] 9 58 66 76 78\n $ : int [1:3] 6 75 85\n $ : int [1:3] 10 72 73\n $ : int [1:3] 7 73 74\n $ : int [1:5] 10 11 16 17 70\n $ : int [1:5] 10 19 70 71 74\n $ : int [1:6] 7 19 71 73 84 86\n $ : int [1:6] 6 32 53 55 69 85\n $ : int [1:7] 57 58 64 65 66 67 68\n $ : int [1:7] 18 23 38 61 62 63 83\n $ : int [1:7] 2 8 9 56 58 68 85\n $ : int [1:7] 23 38 40 41 43 44 45\n $ : int [1:8] 8 34 35 36 41 45 47 56\n $ : int [1:6] 25 26 31 33 39 42\n $ : int [1:5] 20 21 23 35 41\n $ : int [1:9] 12 13 15 16 17 18 22 63 77\n $ : int [1:6] 7 9 66 67 74 86\n $ : int [1:11] 1 2 3 5 6 32 56 57 69 75 ...\n $ : int [1:9] 8 9 19 21 35 46 47 74 84\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language poly2nb(pl = hunan, queen = TRUE)\n - attr(*, \"type\")= chr \"queen\"\n - attr(*, \"sym\")= logi TRUE"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#creating-rook-contiguity-based-neighbours",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#creating-rook-contiguity-based-neighbours",
    "title": "Hands-on Exercise 4",
    "section": "Creating (ROOK) contiguity based neighbours",
    "text": "Creating (ROOK) contiguity based neighbours\n\n\nShow the code\nwm_r <- poly2nb(hunan, queen=FALSE)\nsummary(wm_r)\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "title": "Hands-on Exercise 5",
    "section": "",
    "text": "In this hands-on exercise, you will learn how to compute Global Measures of Spatial Autocorrelation (GMSA) by using spdep package. By the end to this hands-on exercise, you will be able to:\n\nimport geospatial data using appropriate function(s) of sf package\nimport csv file using appropriate function of readr package\nperform relational join using appropriate join function of dplyr package\ncompute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of spdep package\n\nplot Moran scatterplot\ncompute and plot spatial correlogram using appropriate function of spdep package\n\nprovide statistically correct interpretation of GSA statistics.\n\n\n\nIn spatial policy, one of the main development objective of the local government and planners is to ensure equal distribution of development in the province. Our task in this study, hence, is to apply appropriate spatial statistical methods to discover if development are even distributed geographically. If the answer is No. Then, our next question will be “is there sign of spatial clustering?”. And, if the answer for this question is yes, then our next question will be “where are these clusters?”\nIn this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita) of Hunan Provice, People Republic of China.\n\n\n\nTwo data sets will be used in this hands-on exercise, they are:\n\nHunan province administrative boundary layer at county level. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\n\nBefore we get started, we need to ensure that spdep, sf, tmap and tidyverse packages of R are currently installed in your R.\n\nsf is use for importing and handling geospatial data in R,\ntidyverse is mainly use for wrangling attribute data in R,\nspdep will be used to compute spatial weights, global and local spatial autocorrelation statistics, and\ntmap will be used to prepare cartographic quality chropleth map.\n\nThe code chunk below is used to perform the following tasks:\n\ncreating a package list containing the necessary R packages,\nchecking if the R packages in the package list have been installed in R,\n\nif they have yet to be installed, RStudio will installed the missing packages,\n\nlaunching the packages into R environment.\n\n\npacman::p_load(sf, spdep, tmap, tidyverse)\n\n\n\n\n\nIn this section, you will learn how to bring a geospatial data and its associated attribute table into R environment. The geospatial data is in ESRI shapefile format and the attribute table is in csv fomat.\n\n\nThe code chunk below uses st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf.\n\nhunan <- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\jaymieseet\\IS415-GAA-JS\\Hands-on_Ex\\Hands-on_Ex05\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\nNext, we will import Hunan_2012.csv into R by using read_csv() of readr package. The output is R data frame class.\n\nhunan2012 <- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\nhunan <- left_join(hunan,hunan2012) %>%\n  select(1:4, 7, 15)\n\n\n\n\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\nequal <- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\")\n\nquantile <- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)\n\n\n\n\n\n\n\n\nIn this section, you will learn how to compute global spatial autocorrelation statistics and to perform spatial complete randomness test for global spatial autocorrelation.\n\n\nBefore we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area.\nIn the code chunk below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. If you look at the documentation you will see that you can pass a “queen” argument that takes TRUE or FALSE as options. If you do not specify this argument the default is set to TRUE, that is, if you don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\nMore specifically, the code chunk below is used to compute Queen contiguity weight matrix.\n\nwm_q <- poly2nb(hunan, \n                queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one neighbours.\n\n\n\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\nrswm_q <- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\n\n\n\n\nIn this section, you will learn how to perform Moran’s I statistics testing by using moran.test() of spdep.\n\n\nThe code chunk below performs Moran’s I statistical testing using moran.test() of spdep.\n\nmoran.test(hunan$GDPPC, \n           listw=rswm_q, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\n\n\nThe code chunk below performs permutation test for Moran’s I statistic by using moran.mc() of spdep. A total of 1000 simulation will be performed.\n\nset.seed(1234)\nbperm= moran.mc(hunan$GDPPC, \n                listw=rswm_q, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\n\nIt is always a good practice for us the examine the simulated Moran’s I test statistics in greater detail. This can be achieved by plotting the distribution of the statistical values as a histogram by using the code chunk below.\nIn the code chunk below hist() and ablin of R Graphics are used.\n\nmean(bperm$res[1:999])\n\n[1] -0.01504572\n\nvar(bperm$res[1:999])\n\n[1] 0.004371574\n\nsummary(bperm$res[1:999])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.18339 -0.06168 -0.02125 -0.01505  0.02611  0.27593 \n\n\n\nhist(bperm$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n\n\n\n\n\n\n\n\nIn this section, you will learn how to perform Geary’s C statistics testing by using appropriate functions of spdep package.\n\n\nThe code chunk below performs Geary’s C test for spatial autocorrelation by using geary.test() of spdep.\n\ngeary.test(hunan$GDPPC, listw=rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\n\n\n\nThe code chunk below performs permutation test for Geary’s C statistic by using geary.mc() of spdep.\n\nset.seed(1234)\nbperm=geary.mc(hunan$GDPPC, \n               listw=rswm_q, \n               nsim=999)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\n\nNext, we will plot a histogram to reveal the distribution of the simulated values by using the code chunk below.\n\nmean(bperm$res[1:999])\n\n[1] 1.004402\n\nvar(bperm$res[1:999])\n\n[1] 0.007436493\n\nsummary(bperm$res[1:999])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7142  0.9502  1.0052  1.0044  1.0595  1.2722 \n\n\n\nhist(bperm$res, freq=TRUE, breaks=20, xlab=\"Simulated Geary c\")\nabline(v=1, col=\"red\") \n\n\n\n\n\n\n\n\nSpatial correlograms are great to examine patterns of spatial autocorrelation in your data or model residuals. They show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran’s I or Geary’s c) against distance.Although correlograms are not as fundamental as variograms (a keystone concept of geostatistics), they are very useful as an exploratory and descriptive tool. For this purpose they actually provide richer information than variograms.\n\n\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Moran’s I. The plot() of base Graph is then used to plot the output.\n\nMI_corr <- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr)\n\n\n\n\nBy plotting the output might not allow us to provide complete interpretation. This is because not all autocorrelation values are statistically significant. Hence, it is important for us to examine the full analysis report by printing out the analysis results as in the code chunk below.\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Geary’s C. The plot() of base Graph is then used to plot the output.\n\nGC_corr <- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr)\n\n\n\n\n\nprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#cluster-and-outlier-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#cluster-and-outlier-analysis",
    "title": "Hands-on Exercise 5",
    "section": "Cluster and Outlier Analysis",
    "text": "Cluster and Outlier Analysis\nLocal Indicators of Spatial Association or LISA are statistics that evaluate the existence of clusters in the spatial arrangement of a given variable. For instance if we are studying cancer rates among census tracts in a given city local clusters in the rates mean that there are areas that have higher or lower rates than is to be expected by chance alone; that is, the values occurring are above or below those of a random distribution in space.\nIn this section, you will learn how to apply appropriate Local Indicators for Spatial Association (LISA), especially local Moran’I to detect cluster and/or outlier from GDP per capita 2012 of Hunan Province, PRC.\n\nComputing local Moran’s I\nTo compute local Moran’s I, the localmoran() function of spdep will be used. It computes Ii values, given a set of zi values and a listw object providing neighbour weighting information for the polygon associated with the zi values.\nThe code chunks below are used to compute local Moran’s I of GDPPC2012 at the county level.\n\nfips <- order(hunan$County)\nlocalMI <- localmoran(hunan$GDPPC, rswm_q)\nhead(localMI)\n\n            Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.001468468 -2.815006e-05 4.723841e-04 -0.06626904      0.9471636\n2  0.025878173 -6.061953e-04 1.016664e-02  0.26266425      0.7928094\n3 -0.011987646 -5.366648e-03 1.133362e-01 -0.01966705      0.9843090\n4  0.001022468 -2.404783e-07 5.105969e-06  0.45259801      0.6508382\n5  0.014814881 -6.829362e-05 1.449949e-03  0.39085814      0.6959021\n6 -0.038793829 -3.860263e-04 6.475559e-03 -0.47728835      0.6331568\n\n\nlocalmoran() function returns a matrix of values whose columns are:\n\nIi: the local Moran’s I statistics\nE.Ii: the expectation of local moran statistic under the randomisation hypothesis\nVar.Ii: the variance of local moran statistic under the randomisation hypothesis\nZ.Ii:the standard deviate of local moran statistic\nPr(): the p-value of local moran statistic\n\nThe code chunk below list the content of the local Moran matrix derived by using printCoefmat().\n\nprintCoefmat(data.frame(\n  localMI[fips,], \n  row.names=hunan$County[fips]),\n  check.names=FALSE)\n\n                       Ii        E.Ii      Var.Ii        Z.Ii Pr.z....E.Ii..\nAnhua         -2.2493e-02 -5.0048e-03  5.8235e-02 -7.2467e-02         0.9422\nAnren         -3.9932e-01 -7.0111e-03  7.0348e-02 -1.4791e+00         0.1391\nAnxiang       -1.4685e-03 -2.8150e-05  4.7238e-04 -6.6269e-02         0.9472\nBaojing        3.4737e-01 -5.0089e-03  8.3636e-02  1.2185e+00         0.2230\nChaling        2.0559e-02 -9.6812e-04  2.7711e-02  1.2932e-01         0.8971\nChangning     -2.9868e-05 -9.0010e-09  1.5105e-07 -7.6828e-02         0.9388\nChangsha       4.9022e+00 -2.1348e-01  2.3194e+00  3.3590e+00         0.0008\nChengbu        7.3725e-01 -1.0534e-02  2.2132e-01  1.5895e+00         0.1119\nChenxi         1.4544e-01 -2.8156e-03  4.7116e-02  6.8299e-01         0.4946\nCili           7.3176e-02 -1.6747e-03  4.7902e-02  3.4200e-01         0.7324\nDao            2.1420e-01 -2.0824e-03  4.4123e-02  1.0297e+00         0.3032\nDongan         1.5210e-01 -6.3485e-04  1.3471e-02  1.3159e+00         0.1882\nDongkou        5.2918e-01 -6.4461e-03  1.0748e-01  1.6338e+00         0.1023\nFenghuang      1.8013e-01 -6.2832e-03  1.3257e-01  5.1198e-01         0.6087\nGuidong       -5.9160e-01 -1.3086e-02  3.7003e-01 -9.5104e-01         0.3416\nGuiyang        1.8240e-01 -3.6908e-03  3.2610e-02  1.0305e+00         0.3028\nGuzhang        2.8466e-01 -8.5054e-03  1.4152e-01  7.7931e-01         0.4358\nHanshou        2.5878e-02 -6.0620e-04  1.0167e-02  2.6266e-01         0.7928\nHengdong       9.9964e-03 -4.9063e-04  6.7742e-03  1.2742e-01         0.8986\nHengnan        2.8064e-02 -3.2160e-04  3.7597e-03  4.6294e-01         0.6434\nHengshan      -5.8201e-03 -3.0437e-05  5.1076e-04 -2.5618e-01         0.7978\nHengyang       6.2997e-02 -1.3046e-03  2.1865e-02  4.3486e-01         0.6637\nHongjiang      1.8790e-01 -2.3019e-03  3.1725e-02  1.0678e+00         0.2856\nHuarong       -1.5389e-02 -1.8667e-03  8.1030e-02 -4.7503e-02         0.9621\nHuayuan        8.3772e-02 -8.5569e-04  2.4495e-02  5.4072e-01         0.5887\nHuitong        2.5997e-01 -5.2447e-03  1.1077e-01  7.9685e-01         0.4255\nJiahe         -1.2431e-01 -3.0550e-03  5.1111e-02 -5.3633e-01         0.5917\nJianghua       2.8651e-01 -3.8280e-03  8.0968e-02  1.0204e+00         0.3076\nJiangyong      2.4337e-01 -2.7082e-03  1.1746e-01  7.1800e-01         0.4728\nJingzhou       1.8270e-01 -8.5106e-04  2.4363e-02  1.1759e+00         0.2396\nJinshi        -1.1988e-02 -5.3666e-03  1.1334e-01 -1.9667e-02         0.9843\nJishou        -2.8680e-01 -2.6305e-03  4.4028e-02 -1.3543e+00         0.1756\nLanshan        6.3334e-02 -9.6365e-04  2.0441e-02  4.4972e-01         0.6529\nLeiyang        1.1581e-02 -1.4948e-04  2.5082e-03  2.3422e-01         0.8148\nLengshuijiang -1.7903e+00 -8.2129e-02  2.1598e+00 -1.1623e+00         0.2451\nLi             1.0225e-03 -2.4048e-07  5.1060e-06  4.5260e-01         0.6508\nLianyuan      -1.4672e-01 -1.8983e-03  1.9145e-02 -1.0467e+00         0.2952\nLiling         1.3774e+00 -1.5097e-02  4.2601e-01  2.1335e+00         0.0329\nLinli          1.4815e-02 -6.8294e-05  1.4499e-03  3.9086e-01         0.6959\nLinwu         -2.4621e-03 -9.0703e-06  1.9258e-04 -1.7676e-01         0.8597\nLinxiang       6.5904e-02 -2.9028e-03  2.5470e-01  1.3634e-01         0.8916\nLiuyang        3.3688e+00 -7.7502e-02  1.5180e+00  2.7972e+00         0.0052\nLonghui        8.0801e-01 -1.1377e-02  1.5538e-01  2.0787e+00         0.0376\nLongshan       7.5663e-01 -1.1100e-02  3.1449e-01  1.3690e+00         0.1710\nLuxi           1.8177e-01 -2.4855e-03  3.4249e-02  9.9561e-01         0.3194\nMayang         2.1852e-01 -5.8773e-03  9.8049e-02  7.1663e-01         0.4736\nMiluo          1.8704e+00 -1.6927e-02  2.7925e-01  3.5715e+00         0.0004\nNan           -9.5789e-03 -4.9497e-04  6.8341e-03 -1.0988e-01         0.9125\nNingxiang      1.5607e+00 -7.3878e-02  8.0012e-01  1.8274e+00         0.0676\nNingyuan       2.0910e-01 -7.0884e-03  8.2306e-02  7.5356e-01         0.4511\nPingjiang     -9.8964e-01 -2.6457e-03  5.6027e-02 -4.1698e+00         0.0000\nQidong         1.1806e-01 -2.1207e-03  2.4747e-02  7.6396e-01         0.4449\nQiyang         6.1966e-02 -7.3374e-04  8.5743e-03  6.7712e-01         0.4983\nRucheng       -3.6992e-01 -8.8999e-03  2.5272e-01 -7.1814e-01         0.4727\nSangzhi        2.5053e-01 -4.9470e-03  6.8000e-02  9.7972e-01         0.3272\nShaodong      -3.2659e-02 -3.6592e-05  5.0546e-04 -1.4510e+00         0.1468\nShaoshan       2.1223e+00 -5.0227e-02  1.3668e+00  1.8583e+00         0.0631\nShaoyang       5.9499e-01 -1.1253e-02  1.3012e-01  1.6807e+00         0.0928\nShimen        -3.8794e-02 -3.8603e-04  6.4756e-03 -4.7729e-01         0.6332\nShuangfeng     9.2835e-03 -2.2867e-03  3.1516e-02  6.5174e-02         0.9480\nShuangpai      8.0591e-02 -3.1366e-04  8.9838e-03  8.5358e-01         0.3933\nSuining        3.7585e-01 -3.5933e-03  4.1870e-02  1.8544e+00         0.0637\nTaojiang      -2.5394e-01 -1.2395e-03  1.4477e-02 -2.1002e+00         0.0357\nTaoyuan        1.4729e-02 -1.2039e-04  8.5103e-04  5.0903e-01         0.6107\nTongdao        4.6482e-01 -6.9870e-03  1.9879e-01  1.0582e+00         0.2900\nWangcheng      4.4220e+00 -1.1067e-01  1.3596e+00  3.8873e+00         0.0001\nWugang         7.1003e-01 -7.8144e-03  1.0710e-01  2.1935e+00         0.0283\nXiangtan       2.4530e-01 -3.6457e-04  3.2319e-03  4.3213e+00         0.0000\nXiangxiang     2.6271e-01 -1.2703e-03  2.1290e-02  1.8092e+00         0.0704\nXiangyin       5.4525e-01 -4.7442e-03  7.9236e-02  1.9539e+00         0.0507\nXinhua         1.1810e-01 -6.2649e-03  8.6001e-02  4.2409e-01         0.6715\nXinhuang       1.5725e-01 -4.1820e-03  3.6648e-01  2.6667e-01         0.7897\nXinning        6.8928e-01 -9.6674e-03  2.0328e-01  1.5502e+00         0.1211\nXinshao        5.7578e-02 -8.5932e-03  1.1769e-01  1.9289e-01         0.8470\nXintian       -7.4050e-03 -5.1493e-03  1.0877e-01 -6.8395e-03         0.9945\nXupu           3.2406e-01 -5.7468e-03  5.7735e-02  1.3726e+00         0.1699\nYanling       -6.9021e-02 -5.9211e-04  9.9306e-03 -6.8667e-01         0.4923\nYizhang       -2.6844e-01 -2.2463e-03  4.7588e-02 -1.2202e+00         0.2224\nYongshun       6.3064e-01 -1.1350e-02  1.8830e-01  1.4795e+00         0.1390\nYongxing       4.3411e-01 -9.0735e-03  1.5088e-01  1.1409e+00         0.2539\nYou            7.8750e-02 -7.2728e-03  1.2116e-01  2.4714e-01         0.8048\nYuanjiang      2.0004e-04 -1.7760e-04  2.9798e-03  6.9181e-03         0.9945\nYuanling       8.7298e-03 -2.2981e-06  2.3221e-05  1.8121e+00         0.0700\nYueyang        4.1189e-02 -1.9768e-04  2.3113e-03  8.6085e-01         0.3893\nZhijiang       1.0476e-01 -7.8123e-04  1.3100e-02  9.2214e-01         0.3565\nZhongfang     -2.2685e-01 -2.1455e-03  3.5927e-02 -1.1855e+00         0.2358\nZhuzhou        3.2864e-01 -5.2432e-04  7.2391e-03  3.8688e+00         0.0001\nZixing        -7.6849e-01 -8.8210e-02  9.4057e-01 -7.0144e-01         0.4830\n\n\n\n\nMapping the local Moran’s I\nBefore mapping the local Moran’s I map, it is wise to append the local Moran’s I dataframe (i.e. localMI) onto hunan SpatialPolygonDataFrame. The code chunks below can be used to perform the task. The out SpatialPolygonDataFrame is called hunan.localMI.\n\nhunan.localMI <- cbind(hunan,localMI) %>%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\n\n\nMapping local Moran’s I values\nUsing choropleth mapping functions of tmap package, we can plot the local Moran’s I values by using the code chinks below.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\nMapping local Moran’s I p-values\nThe choropleth shows there is evidence for both positive and negative Ii values. However, it is useful to consider the p-values for each of these values, as consider above.\nThe code chunks below produce a choropleth map of Moran’s I p-values by using functions of tmap package.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\nMapping both local Moran’s I values and p-values\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such visualisation.\n\nlocalMI.map <- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\", \n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue.map <- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#creating-a-lisa-cluster-map",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#creating-a-lisa-cluster-map",
    "title": "Hands-on Exercise 5",
    "section": "Creating a LISA Cluster Map",
    "text": "Creating a LISA Cluster Map\nThe LISA Cluster Map shows the significant locations color coded by type of spatial autocorrelation. The first step before we can generate the LISA cluster map is to plot the Moran scatterplot.\n\nPlotting Moran scatterplot\nThe Moran scatterplot is an illustration of the relationship between the values of the chosen attribute at each location and the average value of the same attribute at neighboring locations.\nThe code chunk below plots the Moran scatterplot of GDPPC 2012 by using moran.plot() of spdep.\n\nnci <- moran.plot(hunan$GDPPC, rswm_q,\n                  labels=as.character(hunan$County), \n                  xlab=\"GDPPC 2012\", \n                  ylab=\"Spatially Lag GDPPC 2012\")\n\n\n\n\nNotice that the plot is split in 4 quadrants. The top right corner belongs to areas that have high GDPPC and are surrounded by other areas that have the average level of GDPPC. This are the high-high locations in the lesson slide.\n\n\nPlotting Moran scatterplot with standardised variable\nFirst we will use scale() to centers and scales the variable. Here centering is done by subtracting the mean (omitting NAs) the corresponding columns, and scaling is done by dividing the (centered) variable by their standard deviations.\n\nhunan$Z.GDPPC <- scale(hunan$GDPPC) %>% \n  as.vector \n\nThe as.vector() added to the end is to make sure that the data type we get out of this is a vector, that map neatly into out dataframe.\nNow, we are ready to plot the Moran scatterplot again by using the code chunk below.\n\nnci2 <- moran.plot(hunan$Z.GDPPC, rswm_q,\n                   labels=as.character(hunan$County),\n                   xlab=\"z-GDPPC 2012\", \n                   ylab=\"Spatially Lag z-GDPPC 2012\")\n\n\n\n\n\n\nPreparing LISA map classes\nThe code chunks below show the steps to prepare a LISA cluster map.\n\nquadrant <- vector(mode=\"numeric\",length=nrow(localMI))\n\nNext, derives the spatially lagged variable of interest (i.e. GDPPC) and centers the spatially lagged variable around its mean.\n\nhunan$lag_GDPPC <- lag.listw(rswm_q, hunan$GDPPC)\nDV <- hunan$lag_GDPPC - mean(hunan$lag_GDPPC) \n\nThis is follow by centering the local Moran’s around the mean.\n\nLM_I <- localMI[,1] - mean(localMI[,1])    \n\nNext, we will set a statistical significance level for the local Moran.\n\nsignif <- 0.05       \n\nThese four command lines define the low-low (1), low-high (2), high-low (3) and high-high (4) categories.\n\nquadrant[DV <0 & LM_I>0] <- 1\nquadrant[DV >0 & LM_I<0] <- 2\nquadrant[DV <0 & LM_I<0] <- 3  \nquadrant[DV >0 & LM_I>0] <- 4      \n\nLastly, places non-significant Moran in the category 0.\n\nquadrant[localMI[,5]>signif] <- 0\n\nIn fact, we can combined all the steps into one single code chunk as shown below:\n\nquadrant <- vector(mode=\"numeric\",length=nrow(localMI))\nhunan$lag_GDPPC <- lag.listw(rswm_q, hunan$GDPPC)\nDV <- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)     \nLM_I <- localMI[,1]   \nsignif <- 0.05       \nquadrant[DV <0 & LM_I>0] <- 1\nquadrant[DV >0 & LM_I<0] <- 2\nquadrant[DV <0 & LM_I<0] <- 3  \nquadrant[DV >0 & LM_I>0] <- 4    \nquadrant[localMI[,5]>signif] <- 0\n\n\n\nPlotting LISA map\nNow, we can build the LISA map by using the code chunks below.\n\nhunan.localMI$quadrant <- quadrant\ncolors <- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters <- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\n\n\n\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such visualisation.\n\ngdppc <- qtm(hunan, \"GDPPC\")\n\nhunan.localMI$quadrant <- quadrant\ncolors <- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters <- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap <- tm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\ntmap_arrange(gdppc, LISAmap, \n             asp=1, ncol=2)\n\n\n\n\nWe can also include the local Moran’s I map and p-value map as shown below for easy comparison."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#hot-spot-and-cold-spot-area-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#hot-spot-and-cold-spot-area-analysis",
    "title": "Hands-on Exercise 5",
    "section": "Hot Spot and Cold Spot Area Analysis",
    "text": "Hot Spot and Cold Spot Area Analysis\nBeside detecting cluster and outliers, localised spatial statistics can be also used to detect hot spot and/or cold spot areas.\nThe term ‘hot spot’ has been used generically across disciplines to describe a region or value that is higher relative to its surroundings (Lepers et al 2005, Aben et al 2012, Isobe et al 2015).\n\nGetis and Ord’s G-Statistics\nAn alternative spatial statistics to detect spatial anomalies is the Getis and Ord’s G-statistics (Getis and Ord, 1972; Ord and Getis, 1995). It looks at neighbours within a defined proximity to identify where either high or low values clutser spatially. Here, statistically significant hot-spots are recognised as areas of high values where other areas within a neighbourhood range also share high values too.\nThe analysis consists of three steps:\n\nDeriving spatial weight matrix\nComputing Gi statistics\nMapping Gi statistics\n\n\n\nDeriving distance-based weight matrix\nFirst, we need to define a new set of neighbours. Whist the spatial autocorrelation considered units which shared borders, for Getis-Ord we are defining neighbours based on distance.\nThere are two type of distance-based proximity matrix, they are:\n\nfixed distance weight matrix; and\nadaptive distance weight matrix.\n\n\nDeriving the centroid\nWe will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running st_centroid() on the sf object: us.bound. We need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of us.bound. Our function will be st_centroid(). We will be using map_dbl variation of map from the purrr package. For more documentation, check out map documentation\nTo get our longitude values we map the st_centroid() function over the geometry column of us.bound and access the longitude value through double bracket notation [[]] and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\nlongitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\nlatitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\ncoords <- cbind(longitude, latitude)\n\n\n\nDetermine the cut-off distance\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\n#coords <- coordinates(hunan)\nk1 <- knn2nb(knearneigh(coords))\nk1dists <- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\nComputing fixed distance weight matrix\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\nwm_d62 <- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\nwm62_lw <- nb2listw(wm_d62, style = 'B')\nsummary(wm62_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \nLink number distribution:\n\n 1  2  3  4  5  6 \n 6 15 14 26 20  7 \n6 least connected regions:\n6 15 30 32 56 65 with 1 link\n7 most connected regions:\n21 28 35 45 50 52 82 with 6 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1   S2\nB 88 7744 324 648 5440\n\n\nThe output spatial weights object is called wm62_lw.\n\n\n\nComputing adaptive distance weight matrix\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\nknn <- knn2nb(knearneigh(coords, k=8))\nknn\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\nknn_lw <- nb2listw(knn, style = 'B')\nsummary(knn_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n 8 \n88 \n88 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n88 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 704 1300 23014"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#computing-gi-statistics",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#computing-gi-statistics",
    "title": "Hands-on Exercise 5",
    "section": "Computing Gi statistics",
    "text": "Computing Gi statistics\n\nGi statistics using fixed distance\n\nfips <- order(hunan$County)\ngi.fixed <- localG(hunan$GDPPC, wm62_lw)\ngi.fixed\n\n [1]  0.436075843 -0.265505650 -0.073033665  0.413017033  0.273070579\n [6] -0.377510776  2.863898821  2.794350420  5.216125401  0.228236603\n[11]  0.951035346 -0.536334231  0.176761556  1.195564020 -0.033020610\n[16]  1.378081093 -0.585756761 -0.419680565  0.258805141  0.012056111\n[21] -0.145716531 -0.027158687 -0.318615290 -0.748946051 -0.961700582\n[26] -0.796851342 -1.033949773 -0.460979158 -0.885240161 -0.266671512\n[31] -0.886168613 -0.855476971 -0.922143185 -1.162328599  0.735582222\n[36] -0.003358489 -0.967459309 -1.259299080 -1.452256513 -1.540671121\n[41] -1.395011407 -1.681505286 -1.314110709 -0.767944457 -0.192889342\n[46]  2.720804542  1.809191360 -1.218469473 -0.511984469 -0.834546363\n[51] -0.908179070 -1.541081516 -1.192199867 -1.075080164 -1.631075961\n[56] -0.743472246  0.418842387  0.832943753 -0.710289083 -0.449718820\n[61] -0.493238743 -1.083386776  0.042979051  0.008596093  0.136337469\n[66]  2.203411744  2.690329952  4.453703219 -0.340842743 -0.129318589\n[71]  0.737806634 -1.246912658  0.666667559  1.088613505 -0.985792573\n[76]  1.233609606 -0.487196415  1.626174042 -1.060416797  0.425361422\n[81] -0.837897118 -0.314565243  0.371456331  4.424392623 -0.109566928\n[86]  1.364597995 -1.029658605 -0.718000620\nattr(,\"internals\")\n               Gi      E(Gi)        V(Gi)        Z(Gi) Pr(z != E(Gi))\n [1,] 0.064192949 0.05747126 2.375922e-04  0.436075843   6.627817e-01\n [2,] 0.042300020 0.04597701 1.917951e-04 -0.265505650   7.906200e-01\n [3,] 0.044961480 0.04597701 1.933486e-04 -0.073033665   9.417793e-01\n [4,] 0.039475779 0.03448276 1.461473e-04  0.413017033   6.795941e-01\n [5,] 0.049767939 0.04597701 1.927263e-04  0.273070579   7.847990e-01\n [6,] 0.008825335 0.01149425 4.998177e-05 -0.377510776   7.057941e-01\n [7,] 0.050807266 0.02298851 9.435398e-05  2.863898821   4.184617e-03\n [8,] 0.083966739 0.04597701 1.848292e-04  2.794350420   5.200409e-03\n [9,] 0.115751554 0.04597701 1.789361e-04  5.216125401   1.827045e-07\n[10,] 0.049115587 0.04597701 1.891013e-04  0.228236603   8.194623e-01\n[11,] 0.045819180 0.03448276 1.420884e-04  0.951035346   3.415864e-01\n[12,] 0.049183846 0.05747126 2.387633e-04 -0.536334231   5.917276e-01\n[13,] 0.048429181 0.04597701 1.924532e-04  0.176761556   8.596957e-01\n[14,] 0.034733752 0.02298851 9.651140e-05  1.195564020   2.318667e-01\n[15,] 0.011262043 0.01149425 4.945294e-05 -0.033020610   9.736582e-01\n[16,] 0.065131196 0.04597701 1.931870e-04  1.378081093   1.681783e-01\n[17,] 0.027587075 0.03448276 1.385862e-04 -0.585756761   5.580390e-01\n[18,] 0.029409313 0.03448276 1.461397e-04 -0.419680565   6.747188e-01\n[19,] 0.061466754 0.05747126 2.383385e-04  0.258805141   7.957856e-01\n[20,] 0.057656917 0.05747126 2.371303e-04  0.012056111   9.903808e-01\n[21,] 0.066518379 0.06896552 2.820326e-04 -0.145716531   8.841452e-01\n[22,] 0.045599896 0.04597701 1.928108e-04 -0.027158687   9.783332e-01\n[23,] 0.030646753 0.03448276 1.449523e-04 -0.318615290   7.500183e-01\n[24,] 0.035635552 0.04597701 1.906613e-04 -0.748946051   4.538897e-01\n[25,] 0.032606647 0.04597701 1.932888e-04 -0.961700582   3.362000e-01\n[26,] 0.035001352 0.04597701 1.897172e-04 -0.796851342   4.255374e-01\n[27,] 0.012746354 0.02298851 9.812587e-05 -1.033949773   3.011596e-01\n[28,] 0.061287917 0.06896552 2.773884e-04 -0.460979158   6.448136e-01\n[29,] 0.014277403 0.02298851 9.683314e-05 -0.885240161   3.760271e-01\n[30,] 0.009622875 0.01149425 4.924586e-05 -0.266671512   7.897221e-01\n[31,] 0.014258398 0.02298851 9.705244e-05 -0.886168613   3.755267e-01\n[32,] 0.005453443 0.01149425 4.986245e-05 -0.855476971   3.922871e-01\n[33,] 0.043283712 0.05747126 2.367109e-04 -0.922143185   3.564539e-01\n[34,] 0.020763514 0.03448276 1.393165e-04 -1.162328599   2.451020e-01\n[35,] 0.081261843 0.06896552 2.794398e-04  0.735582222   4.619850e-01\n[36,] 0.057419907 0.05747126 2.338437e-04 -0.003358489   9.973203e-01\n[37,] 0.013497133 0.02298851 9.624821e-05 -0.967459309   3.333145e-01\n[38,] 0.019289310 0.03448276 1.455643e-04 -1.259299080   2.079223e-01\n[39,] 0.025996272 0.04597701 1.892938e-04 -1.452256513   1.464303e-01\n[40,] 0.016092694 0.03448276 1.424776e-04 -1.540671121   1.233968e-01\n[41,] 0.035952614 0.05747126 2.379439e-04 -1.395011407   1.630124e-01\n[42,] 0.031690963 0.05747126 2.350604e-04 -1.681505286   9.266481e-02\n[43,] 0.018750079 0.03448276 1.433314e-04 -1.314110709   1.888090e-01\n[44,] 0.015449080 0.02298851 9.638666e-05 -0.767944457   4.425202e-01\n[45,] 0.065760689 0.06896552 2.760533e-04 -0.192889342   8.470456e-01\n[46,] 0.098966900 0.05747126 2.326002e-04  2.720804542   6.512325e-03\n[47,] 0.085415780 0.05747126 2.385746e-04  1.809191360   7.042128e-02\n[48,] 0.038816536 0.05747126 2.343951e-04 -1.218469473   2.230456e-01\n[49,] 0.038931873 0.04597701 1.893501e-04 -0.511984469   6.086619e-01\n[50,] 0.055098610 0.06896552 2.760948e-04 -0.834546363   4.039732e-01\n[51,] 0.033405005 0.04597701 1.916312e-04 -0.908179070   3.637836e-01\n[52,] 0.043040784 0.06896552 2.829941e-04 -1.541081516   1.232969e-01\n[53,] 0.011297699 0.02298851 9.615920e-05 -1.192199867   2.331829e-01\n[54,] 0.040968457 0.05747126 2.356318e-04 -1.075080164   2.823388e-01\n[55,] 0.023629663 0.04597701 1.877170e-04 -1.631075961   1.028743e-01\n[56,] 0.006281129 0.01149425 4.916619e-05 -0.743472246   4.571958e-01\n[57,] 0.063918654 0.05747126 2.369553e-04  0.418842387   6.753313e-01\n[58,] 0.070325003 0.05747126 2.381374e-04  0.832943753   4.048765e-01\n[59,] 0.025947288 0.03448276 1.444058e-04 -0.710289083   4.775249e-01\n[60,] 0.039752578 0.04597701 1.915656e-04 -0.449718820   6.529132e-01\n[61,] 0.049934283 0.05747126 2.334965e-04 -0.493238743   6.218439e-01\n[62,] 0.030964195 0.04597701 1.920248e-04 -1.083386776   2.786368e-01\n[63,] 0.058129184 0.05747126 2.343319e-04  0.042979051   9.657182e-01\n[64,] 0.046096514 0.04597701 1.932637e-04  0.008596093   9.931414e-01\n[65,] 0.012459080 0.01149425 5.008051e-05  0.136337469   8.915545e-01\n[66,] 0.091447733 0.05747126 2.377744e-04  2.203411744   2.756574e-02\n[67,] 0.049575872 0.02298851 9.766513e-05  2.690329952   7.138140e-03\n[68,] 0.107907212 0.04597701 1.933581e-04  4.453703219   8.440175e-06\n[69,] 0.019616151 0.02298851 9.789454e-05 -0.340842743   7.332220e-01\n[70,] 0.032923393 0.03448276 1.454032e-04 -0.129318589   8.971056e-01\n[71,] 0.030317663 0.02298851 9.867859e-05  0.737806634   4.606320e-01\n[72,] 0.019437582 0.03448276 1.455870e-04 -1.246912658   2.124295e-01\n[73,] 0.055245460 0.04597701 1.932838e-04  0.666667559   5.049845e-01\n[74,] 0.074278054 0.05747126 2.383538e-04  1.088613505   2.763244e-01\n[75,] 0.013269580 0.02298851 9.719982e-05 -0.985792573   3.242349e-01\n[76,] 0.049407829 0.03448276 1.463785e-04  1.233609606   2.173484e-01\n[77,] 0.028605749 0.03448276 1.455139e-04 -0.487196415   6.261191e-01\n[78,] 0.039087662 0.02298851 9.801040e-05  1.626174042   1.039126e-01\n[79,] 0.031447120 0.04597701 1.877464e-04 -1.060416797   2.889550e-01\n[80,] 0.064005294 0.05747126 2.359641e-04  0.425361422   6.705732e-01\n[81,] 0.044606529 0.05747126 2.357330e-04 -0.837897118   4.020885e-01\n[82,] 0.063700493 0.06896552 2.801427e-04 -0.314565243   7.530918e-01\n[83,] 0.051142205 0.04597701 1.933560e-04  0.371456331   7.102977e-01\n[84,] 0.102121112 0.04597701 1.610278e-04  4.424392623   9.671399e-06\n[85,] 0.021901462 0.02298851 9.843172e-05 -0.109566928   9.127528e-01\n[86,] 0.064931813 0.04597701 1.929430e-04  1.364597995   1.723794e-01\n[87,] 0.031747344 0.04597701 1.909867e-04 -1.029658605   3.031703e-01\n[88,] 0.015893319 0.02298851 9.765131e-05 -0.718000620   4.727569e-01\nattr(,\"cluster\")\n [1] Low  Low  High High High High High High High Low  Low  High Low  Low  Low \n[16] High High High High Low  High High Low  Low  High Low  Low  Low  Low  Low \n[31] Low  Low  Low  High Low  Low  Low  Low  Low  Low  High Low  Low  Low  Low \n[46] High High Low  Low  Low  Low  High Low  Low  Low  Low  Low  High Low  Low \n[61] Low  Low  Low  High High High Low  High Low  Low  High Low  High High Low \n[76] High Low  Low  Low  Low  Low  Low  High High Low  High Low  Low \nLevels: Low High\nattr(,\"gstari\")\n[1] FALSE\nattr(,\"call\")\nlocalG(x = hunan$GDPPC, listw = wm62_lw)\nattr(,\"class\")\n[1] \"localG\"\n\n\nThe output of localG() is a vector of G or Gstar values, with attributes “gstari” set to TRUE or FALSE, “call” set to the function call, and class “localG”.\nThe Gi statistics is represented as a Z-score. Greater values represent a greater intensity of clustering and the direction (positive or negative) indicates high or low clusters.\nNext, we will join the Gi values to their corresponding hunan sf data frame by using the code chunk below.\n\nhunan.gi <- cbind(hunan, as.matrix(gi.fixed)) %>%\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\nIn fact, the code chunk above performs three tasks. First, it convert the output vector (i.e. gi.fixed) into r matrix object by using as.matrix(). Next, cbind() is used to join hunan@data and gi.fixed matrix to produce a new SpatialPolygonDataFrame called hunan.gi. Lastly, the field name of the gi values is renamed to gstat_fixed by using rename().\n\n\nMapping Gi values with fixed distance weights\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\ngdppc <- qtm(hunan, \"GDPPC\")\n\nGimap <-tm_shape(hunan.gi) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"local Gi\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, Gimap, asp=1, ncol=2)\n\n\n\n\n\n\nGi statistics using adaptive distance\nThe code chunk below are used to compute the Gi values for GDPPC2012 by using an adaptive distance weight matrix (i.e knb_lw).\n\nfips <- order(hunan$County)\ngi.adaptive <- localG(hunan$GDPPC, knn_lw)\nhunan.gi <- cbind(hunan, as.matrix(gi.adaptive)) %>%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\n\n\nMapping Gi values with adaptive distance weights\nIt is time for us to visualise the locations of hot spot and cold spot areas. The choropleth mapping functions of tmap package will be used to map the Gi values.\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\ngdppc<- qtm(hunan, \"GDPPC\")\n\nGimap <- tm_shape(hunan.gi) + \n  tm_fill(col = \"gstat_adaptive\", \n          style = \"pretty\", \n          palette=\"-RdBu\", \n          title = \"local Gi\") + \n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, \n             Gimap, \n             asp=1, \n             ncol=2)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html",
    "title": "Take Home Exercise 1",
    "section": "",
    "text": "In recent years, the rise of ride-hailing applications is greatly present in our everyday lives. This can be attributed to the widespread adoption of smartphones, allowing access to the collection of large volumes of location-based data.\nGrab is Southeast Asia’s leading ride-sharing company and in this exercise, we deep dive into Grab’s dataset.The dataset is sampled from Grab drivers’ trajectories, with the drivers’ personal information encrypted and the real start/end locations removed.\nThese datasets provides us with valuable insghts to spatial-temporal characteristics of human mobility. In the context of smart city and urban planning, analysing human mobility data through GIS methods can be transformative to allow urban planners and policymakers to gain a comprehensive understanding of how people interact with their surroundings."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#packages-used",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#packages-used",
    "title": "Take Home Exercise 1",
    "section": "4.1 Packages Used",
    "text": "4.1 Packages Used\nThe R packages used in this exercise are:\n\nsf: this packages provides an efficient framework designed for importing, managing and processing vector-based geospatial data n R\nspatstat: a wide range of useful functions for point pattern analysis, which will be used to derive the Kernel Density Estimation (KDE) layer\nraster: reads, writes, manipulates, analyses and model of gridded spatial data (i.e. raster), which will be used to convert image output generated by spastat into raster format\nmaptools: provides a set of tools for manipulating geographic data, which we will mainly use to convert Spatial objects into ppp format\ntmap: provides functions for plotting cartographic quality static point patterns maps or interactive maps by using Leaflet API\narrow: this package exposes the interface to the Arrow C++, enabling access to many features in R, which we will be using to read Parquet files into R environment\nlubridate: a popular R package that simplifies the handling of date-time objects in R and is a member of the tidyverse family\ntidyverse: a collection of R packages designed to facilitate and streamline the process of data analysis and manipulation in R\n\nLoad all these packages into your R environment:\n\npacman::p_load(sf, tmap, tidyverse, lubridate, arrow ,maptools, raster, spatstat)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#importing-geospatial-data",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#importing-geospatial-data",
    "title": "Take Home Exercise 1",
    "section": "5.1 Importing Geospatial Data",
    "text": "5.1 Importing Geospatial Data\nIn this section, we will use st_read() to read our MPSZ 2019 layer as a simple features data frame. We can convert it to other objects later on if necessary!\n\nmpsz_sf <- st_read(dsn = \"data/MPSZ-2019\", \n                layer = \"MPSZ-2019\")\n\nReading layer `MPSZ-2019' from data source \n  `C:\\jaymieseet\\IS415-GAA-JS\\Take-home_Ex\\Take-home_Ex01\\data\\MPSZ-2019' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\nsg_roads <- st_read(dsn = \"data/OSM\", \n                layer = \"gis_osm_roads_free_1\")\n\nReading layer `gis_osm_roads_free_1' from data source \n  `C:\\jaymieseet\\IS415-GAA-JS\\Take-home_Ex\\Take-home_Ex01\\data\\OSM' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1759836 features and 10 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 99.66041 ymin: 0.8021131 xmax: 119.2601 ymax: 7.514393\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#geospatial-data-pre-processing",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#geospatial-data-pre-processing",
    "title": "Take Home Exercise 1",
    "section": "5.2 Geospatial Data Pre-processing",
    "text": "5.2 Geospatial Data Pre-processing\n\n5.2.1 Changing the Referencing System to Singapore national Projected coordinate system\nWhen we inspect the Projected CRS, the MPSZ 2019 layer that we just imported is in WGS84. However, since our analysis is focused on Singapore, we should change it to the Singapore Projected Coordinate System, SVY21 (EPSG Code 3414), using st_transform().\n\nmpsz3414 <- st_transform(mpsz_sf, crs = 3414)\nst_crs(mpsz3414)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\nsg_roads3414 <- st_transform(sg_roads, crs = 3414)\nst_crs(sg_roads3414)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nNow we can see that the Project CRS is changed to SVY21!\n\n\n5.2.2 Initial Visualisation\nSince the map of the MPSZ would be separated into many subzones, in order for a cleaner visualisation, we can use st_union() to combine all the polygons, to give us the outline of Singapore.\n\nsg_sf <- mpsz3414 %>%\n  st_union()\nplot(sg_sf)\n\n\n\n\nFrom this visualisation, we can see that there are the presence of the small surrounding islands surrounding the main Singapore island. For this exercise, we will exclude the surrounding islands from analysis as our focus is specifically on understanding human mobility within Singapore. It is reasonable to assume that Grab services do not extend to these islands.\n\n\n5.2.3 Filtering Out Surrounding Islands\nIn order to filter out these islands from our data set, we can inspect the attribute table of the mpsz3414 data set that has been loaded into our R environment.\nWhen we look at the SUBZONE_N or PLN_AREA_N fields, we can see that the names include the word ‘ISLAND’, which suggests that these might be the rows of data we would like to exclude.\n\n\n\nHowever, to further confirm this, I loaded this ESRI shapefile into QGIS in order to accurately identify the names of the islands. As you can see from the screenshot below, it is indeed the case.\n\nHence, now all we need to do is to exclude these rows from our mpsz3414 data set with the code chunk below. Then, we can plot out the map again to find out whether this is successful!\n\nsg_sf <- mpsz3414[!(mpsz3414$PLN_AREA_N %in% c('NORTH-EASTERN ISLANDS', 'SOUTHERN ISLANDS', 'WESTERN ISLANDS')), ]\nplot(st_geometry(sg_sf))\n\n\n\n\nNow, we can see that the islands have been excluded!\n\n\n\n\n\n\nTip\n\n\n\nWhen we use plot(sf_object) we are plotting the entire sf object, including all its attributes and geometries. Hence, if we call plot(mpsz3414_filter), we will have 6 different visualisations, one for each attribute. Since we only want the outline of the map and not focusing on the individual attributes, we can use plot(st_geometry(mpsz3414_filter)) to only plot out the geometries within the sf.\nHowever, after performing st_union(), it creates a new geometry object that represents the combined geometries of the sf object and it only operates on the geometries, leaving the attributes intact. Thus, using plot(sg_sf) after st_union() will only give us one visualisation, displaying the unioned geometries and there is no need to use st_geometry()."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#importing-grab-posisi-data-set",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#importing-grab-posisi-data-set",
    "title": "Take Home Exercise 1",
    "section": "6.1 Importing Grab-Posisi Data Set",
    "text": "6.1 Importing Grab-Posisi Data Set\nIn the GrabPosisi Dataset that we are using, there are a total of 100 parts. Let’s first import all parts using read_parquet() of arrow package that allows us to read parquet format into R. By default, the output file will be in tibble data frame.\n\ngrab00 <- read_parquet(\"data/GrabPosisi/part-00000-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\n\ngrab01 <- read_parquet(\"data/GrabPosisi/part-00001-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\n\ngrab02 <- read_parquet(\"data/GrabPosisi/part-00002-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\n\ngrab03 <- read_parquet(\"data/GrabPosisi/part-00003-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\n\ngrab04 <- read_parquet(\"data/GrabPosisi/part-00004-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\n\ngrab05 <- read_parquet(\"data/GrabPosisi/part-00005-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\n\ngrab06 <- read_parquet(\"data/GrabPosisi/part-00006-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\n\ngrab07 <- read_parquet(\"data/GrabPosisi/part-00007-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\n\ngrab08 <- read_parquet(\"data/GrabPosisi/part-00008-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\n\ngrab09 <- read_parquet(\"data/GrabPosisi/part-00009-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#data-preparation",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#data-preparation",
    "title": "Take Home Exercise 1",
    "section": "6.2 Data Preparation",
    "text": "6.2 Data Preparation\nFor us to efficiently work with this data set, we have to combine all the parts into one data frame using rbind(), which is a base R package used for combining objects by row-wise binding. Then, we can review the data type using glimpse() from dplyr package to display the structure of the tibble data frame.\n\ngrab_df <- rbind(grab00, grab01, grab02, grab03, grab04, grab05, grab06, grab07, grab08, grab09)\nglimpse(grab_df)\n\nUpon inspection of grab_df, pingtimestamp is in interger format, which is the wrong data type format. We have to change it to a date/time format.\n\ngrab_df$pingtimestamp <- as_datetime(grab_df$pingtimestamp)\nglimpse(grab_df)\n\nNow we can see that the data type is POSIXct, which is used to represent date and time values, including both the date and the time of day.\n\n6.2.1 Extracting trip origins\nThe data set includes all the details of a trip, including all the intermediate check points taken. For this analysis, our focus will be solely on the origin and destination locations, as we want to learn where are the most frequented areas for ride-hailing services.\nIn this code chunk, there are several steps to take note:\n\ngroup_by() of dplyr: groups the records according to the values of trj_id\narrange() of dplyr: to sort the rows of a data frame by pingtimestamp field, which are sorted in ascending by default\nfilter() of dplyr: to retain all rows that meet the selection criteria, i.e. row_number()==1\nWe specify this rule because since the data is already sorted by time, we want to first row, which will be the earliest time stamp of a recorded trip to give us the origin location.\nmutate() of dplyr: to derive new fields by using functions\nwday() of lubridate: to return the day of the week, which is the full character of the day (e.g. Monday) by default\nhour() of lubridate: returns the hour of the day\nmday() of lubridate: returns the day of the month\n\n\norigin_df <- grab_df %>% \n  group_by(trj_id) %>% \n  arrange(pingtimestamp) %>% \n  filter(row_number()==1) %>%\n  mutate(weekday = wday(pingtimestamp,\n                        label=TRUE,\n                        abbr=TRUE),\n         start_hr = factor(hour(pingtimestamp)),\n         day = factor(mday(pingtimestamp)))\n\n\n\n6.2.2 Extracting trip destinations\nThe steps to extract the trip destinations are the same, except that arrange(desc(pingtimestamp)) is used to sort the values in pingtimestamp field in a descending order instead. This will allow us to filter the rows that have the latest time stamp of a trip, to give us the destination locations.\n\ndestination_df <- grab_df %>%\n  group_by(trj_id) %>%\n  arrange(desc(pingtimestamp)) %>%\n  filter(row_number()==1) %>%\n  mutate(weekday = wday(pingtimestamp,\n                        label=TRUE,\n                        abbr=TRUE),\n         end_hr = factor(hour(pingtimestamp)),\n         day = factor(mday(pingtimestamp)))\n\n\nwrite_rds(origin_df, \"data/rds/origin_df.rds\")\nwrite_rds(destination_df, \"data/rds/destination_df.rds\")\n\n\norigin_df <- read_rds(\"data/rds/origin_df.rds\")\ndestination_df <- read_rds(\"data/rds/destination_df.rds\")\n\n\n\n\n\n\n\nNote\n\n\n\nIf you want to save the tidied data for future use, you can use the code chunk below:\nwrite_rds(origin_df, \"data/rds/origin_df.rds\")\nwrite_rds(destination_df, \"data/rds/destination_df.rds\")\nWhen you want to import the data, you can use the code chunk below:\norigin_df <- read_rds(\"data/rds/origin_df.rds\")\ndestination_df <- read_rds(\"data/rds/destination_df.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#converting-tibble-dataframe-into-sf-tibble-dataframe",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#converting-tibble-dataframe-into-sf-tibble-dataframe",
    "title": "Take Home Exercise 1",
    "section": "6.3 Converting tibble dataframe into sf tibble dataframe",
    "text": "6.3 Converting tibble dataframe into sf tibble dataframe\nTibble dataframes are not designed for spatial data and do not have support for spatial operations or geometries. The sf package has the capabilities of tibbles to handle spatial data as it includes spatial geometries, such as points, lines or polygons. Thus, to perform a wide range of spatial operations, we should convert our tibble data frame into a sf tibble data frame.\n\nst_as_sf() of sf: to perform conversion\n\ncoords argument: specifies the names of the columns in the input data frame that represent longitude and latitude coordinates, it is crucial that the longitude column comes first, followed by the latitude column\ncrs argument: allows you to specify the source’s coordinate reference system\n\nst_transform() of sf: transform the CRS to the appropriate one, EPSG Code 3414\n\n\norigin_sf <- st_as_sf(origin_df,\n                      coords = c(\"rawlng\", \"rawlat\"),\n                      crs = 4326) %>%\n  st_transform(crs = 3414)\n\ndestination_sf <- st_as_sf(destination_df,\n                      coords = c(\"rawlng\", \"rawlat\"),\n                      crs = 4326) %>%\n  st_transform(crs = 3414)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#visualising-the-data",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#visualising-the-data",
    "title": "Take Home Exercise 1",
    "section": "6.4 Visualising the Data",
    "text": "6.4 Visualising the Data\n\n6.4.1 Visualising Frequency Distribution\nThe code uses ggplot2 to create the visualizations and generate a bar plot of the weekday variable in the origin_df data frame.\n\nggplot(data = origin_df, aes(x = weekday)): sets up the ggplot object with ’origin_df as the data source and maps weekday variable to x-axis\ngeom_bar(): adds a bar layer to the plot, creating a bar chart where the height of each bar represents the count of origin trips for each weekday.\ngeom_text(): Adds text labels to the bars, displaying the count of observations above each bar.\n\naes(label = after_stat(count)): label each bar to display the count of observations\nvjust = -0.1: adjust the vertical position of text label\n\n\n\nOriginDestination\n\n\n\nggplot(data=origin_df, \n       aes(x=weekday)) + \n  geom_bar() +\n   geom_text(\n    aes(label = after_stat(count)),\n    stat = \"count\",\n    vjust = -0.1\n  )\n\n\n\n\n\n\n\nggplot(data=destination_df, \n       aes(x=weekday)) + \n  geom_bar() +\n   geom_text(\n    aes(label = after_stat(count)),\n    stat = \"count\",\n    vjust = -0.1\n  )\n\n\n\n\n\n\n\n\n\n6.4.2 Visualising as Point Symbol Map\nUsing tmap to create a map and plotting the dots to represent the spatial points.\n\nOriginDestination\n\n\n\ntmap_mode(\"plot\")\ntm_shape(origin_sf) +\n  tm_dots()\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\ntm_shape(destination_sf) +\n  tm_dots()"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#converting-sf-dataframe-to-sp-spatial-classes",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#converting-sf-dataframe-to-sp-spatial-classes",
    "title": "Take Home Exercise 1",
    "section": "7.1 Converting sf dataframe to sp Spatial* classes",
    "text": "7.1 Converting sf dataframe to sp Spatial* classes\nWhile simple feature data frames are becoming more popular, many geospatial analysis packages still rely on sp’s Spatial* classes for input data.In this section, you will learn how to convert simple feature data frame to sp’s Spatial* class.\nThe code chunk below uses as_Spatial() of sf package to convert the three geospatial data from simple feature data frame to sp’s Spatial* class.\n\norigins <- as_Spatial(origin_sf)\ndestination <- as_Spatial(destination_sf)\nmpsz <- as_Spatial(sg_sf)\n\n\nmpsz\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 326 \nextent      : 2667.538, 55941.94, 21448.47, 50256.33  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 6\nnames       : SUBZONE_N, SUBZONE_C, PLN_AREA_N, PLN_AREA_C,       REGION_N, REGION_C \nmin values  : ADMIRALTY,    AMSZ01, ANG MO KIO,         AM, CENTRAL REGION,       CR \nmax values  :    YUNNAN,    YSSZ09,     YISHUN,         YS,    WEST REGION,       WR \n\n\n\norigins\n\nclass       : SpatialPointsDataFrame \nfeatures    : 28000 \nextent      : 3628.243, 49845.23, 25198.14, 49689.64  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 10\nnames       : trj_id, driving_mode,  osname, pingtimestamp,            speed, bearing, accuracy, weekday, start_hr, day \nmin values  :     10,          car, android,    1554682166,               -1,       0,        1,     Fri,        0,  10 \nmax values  :   9984,          car,     ios,    1555889608, 30.9490566253662,     359,      728,     Wed,        9,   9 \n\n\n\ndestination\n\nclass       : SpatialPointsDataFrame \nfeatures    : 28000 \nextent      : 3637.207, 49870.63, 25221.3, 49507.79  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 10\nnames       : trj_id, driving_mode,  osname, pingtimestamp,            speed, bearing, accuracy, weekday, end_hr, day \nmin values  :     10,          car, android,    1554683192,               -1,       0,      1.5,     Fri,      0,  10 \nmax values  :   9984,          car,     ios,    1555891009, 36.4799995422363,     359,     1414,     Wed,      9,   9 \n\n\nWhen inspecting the data, we can now see that the geospatial data have been converted into their respective sp’s Spatial* classes now."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#converting-the-spatial-class-into-generic-sp-format",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#converting-the-spatial-class-into-generic-sp-format",
    "title": "Take Home Exercise 1",
    "section": "7.2 Converting the Spatial* class into generic sp format",
    "text": "7.2 Converting the Spatial* class into generic sp format\nSince we want to use spatstat package, it requires the analytical data in ppp object form. There is no direct way to convert a Spatial* classes into ppp object, thus we need to convert the Spatial classes* into Spatial object first.\n\norigin_sp <- as(origins, \"SpatialPoints\")\ndestination_sp <- as(destination, \"SpatialPoints\")\nsg_sp <- as(mpsz, \"SpatialPolygons\")\n\n\norigin_sp\n\nclass       : SpatialPoints \nfeatures    : 28000 \nextent      : 3628.243, 49845.23, 25198.14, 49689.64  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n\n\n\ndestination_sp\n\nclass       : SpatialPoints \nfeatures    : 28000 \nextent      : 3637.207, 49870.63, 25221.3, 49507.79  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n\n\n\nsg_sp\n\nclass       : SpatialPolygons \nfeatures    : 326 \nextent      : 2667.538, 55941.94, 21448.47, 50256.33  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#converting-the-generic-sp-format-into-spatstats-ppp-format",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#converting-the-generic-sp-format-into-spatstats-ppp-format",
    "title": "Take Home Exercise 1",
    "section": "7.3 Converting the generic sp format into spatstat’s ppp format",
    "text": "7.3 Converting the generic sp format into spatstat’s ppp format\nNow we can use as.ppp() of spatstat to convert the spatial data into a ppp object format and summary() to inspect of the details of the created ppp object.\n\nOriginDestination\n\n\n\norigin_ppp <- as(origin_sp, \"ppp\")\nplot(origin_ppp)\n\n\n\nsummary(origin_ppp)\n\nPlanar point pattern:  28000 points\nAverage intensity 2.473666e-05 points per square unit\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: rectangle = [3628.24, 49845.23] x [25198.14, 49689.64] units\n                    (46220 x 24490 units)\nWindow area = 1131920000 square units\n\n\n\n\n\ndestination_ppp <- as(destination_sp, \"ppp\")\nplot(destination_ppp)\n\n\n\nsummary(destination_ppp)\n\nPlanar point pattern:  28000 points\nAverage intensity 2.493661e-05 points per square unit\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: rectangle = [3637.21, 49870.63] x [25221.3, 49507.79] units\n                    (46230 x 24290 units)\nWindow area = 1122850000 square units\n\n\n\n\n\n\n7.3.1 Checking for Duplicates\nAlthough there was no warning message, it is still good practice to check the data sets for any duplicates in the ppp objects.\n\nany(duplicated(origin_ppp))\n\n[1] FALSE\n\n\n\nany(duplicated(destination_ppp))\n\n[1] FALSE\n\n\nSince both returned FALSE, we can confirm there are no duplicate points we have to deal with.\n\n\n\n\n\n\nTip\n\n\n\nIf there are duplicate points to deal with, one solution is to use jittering, which will add a small perturbation to the duplicate points so that they do not occupy the same space\nchildcare_ppp_jit <- rjitter(childcare_ppp, retry=TRUE, nsim=1, drop=TRUE)\nHowever, the easiest and most accurate way to handle the duplicate points would be to delete it, but might also lead to point events being lost."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#creating-owin-object",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#creating-owin-object",
    "title": "Take Home Exercise 1",
    "section": "7.4 Creating Owin Object",
    "text": "7.4 Creating Owin Object\nWhen conducting spatial point pattern analysis, it is advisable to confine the analysis within a specific geographical area, such as the boundary of Singapore. In spatstat, a specialized object called owin is used to represent polygonal regions like Singapore’s boundary. This allows us to define the spatial extent of our analysis to the desired area and ensure the results are relevant. Thus, the code chunk below shows you how to convert the SG SpatialPolygon object into owin object of spatstat.\n\nsg_owin <- as(sg_sp, \"owin\")\nplot(sg_owin)\n\n\n\n\n\nsummary(sg_owin)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#combining-ppp-objects-and-owin-object",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#combining-ppp-objects-and-owin-object",
    "title": "Take Home Exercise 1",
    "section": "7.5 Combining PPP objects and Owin object",
    "text": "7.5 Combining PPP objects and Owin object\nIn the last step of data wrangling, we will extract the origin and destination trips that are located within Singapore.\n\nOriginDestination\n\n\n\noriginSG_ppp = origin_ppp[sg_owin]\nplot(originSG_ppp)\n\n\n\n\n\n\n\ndestSG_ppp = destination_ppp[sg_owin]\nplot(destSG_ppp)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#computing-kde-using-automatic-bandwidth-selection-method",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#computing-kde-using-automatic-bandwidth-selection-method",
    "title": "Take Home Exercise 1",
    "section": "8.1 Computing KDE using Automatic Bandwidth Selection method",
    "text": "8.1 Computing KDE using Automatic Bandwidth Selection method\nUsing the Automatic bandwidth selection method means to automatically determine the optimal bandwidth parameter to perform KDE. These methods typically use statistical techniques to estimate the bandwidth based on the characteristics of the data.\nThe bandwidth parameter controls the smoothing of the KDE surface and can greatly influence how the data points contribute to the KDE.\n\nA small bandwidth can leads to undersmoothing, more localized estimation with higher variability\nA large bandwidth can lead to oversmoothing, smoother estimate with lower variability\n\nThe code chunk below computes a kernel density by using configurations of density() of spatstat:\n\nThe method bw.diggle() automatically selects the bandwidth for kernel density estimation. Alternative methods include bw.CvL(), bw.scott(), or bw.ppl()\nThe smoothing kernel is Gaussian, but other options such as “epanechnikov”, “quartic”, or “disc” are available\nEdge effect bias correction, based on Jones (1993) and Diggle (2010, equation 18.9), is optional and defaults to FALSE\n\n\nOriginDestination\n\n\n\nkde_originSG_bw <- density(originSG_ppp,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                            kernel=\"gaussian\") \nplot(kde_originSG_bw)\n\n\n\n\n\n\n\nkde_destSG_bw <- density(destSG_ppp,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                            kernel=\"gaussian\") \nplot(kde_destSG_bw)\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can retrieve the bandwidth used to compute the KDE layer using this code chunk:\nbw <- bw.diggle(originSG_ppp)\n\n\n\n8.1.1 Rescaling KDE Values\nFrom their legneds, we can see that the density values range from 0 to 0.001, which are difficult to interprt due to their small magnitude.This is because the default unit of measurement for SVY21 is meters. Consequently, the density values are calculated as “number of points per square meter”. Hence, we should do some rescaling to make the data more comprehensible.\n\nOriginDestination\n\n\n\noriginSG_ppp.km <- rescale(originSG_ppp, 1000, \"km\")\nkde_originSG.bw <- density(originSG_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nplot(kde_originSG.bw)\n\n\n\n\n\n\n\ndestSG_ppp.km <- rescale(destSG_ppp, 1000, \"km\")\nkde_destSG.bw <- density(destSG_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nplot(kde_destSG.bw)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#adaptive-bandwidth",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#adaptive-bandwidth",
    "title": "Take Home Exercise 1",
    "section": "8.2 Adaptive Bandwidth",
    "text": "8.2 Adaptive Bandwidth\nThe fixed bandwidth method in kernel density estimation is highly sensitive to the skewed distribution of spatial point patterns across geographical units, such as urban versus rural areas. One way to address this issue is by employing an adaptive bandwidth instead.Adaptive bandwidths adjust according to the local density of points, allowing for more flexibilty in capturing spatial patterns across varying densities.\nHere, we use density.adaptive() function of spatstat.\n\nOriginDestination\n\n\n\nkde_originSG_adaptive <- adaptive.density(originSG_ppp.km, method=\"kernel\")\nplot(kde_originSG_adaptive)\n\n\n\n\n\n\n\nkde_destSG_adaptive <- adaptive.density(destSG_ppp.km, method=\"kernel\")\nplot(kde_destSG_adaptive)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#fixed-bandwidth",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#fixed-bandwidth",
    "title": "Take Home Exercise 1",
    "section": "8.3 Fixed Bandwidth",
    "text": "8.3 Fixed Bandwidth\nUpon observation of the Kernel Density Estimation (KDE) layers we’ve generated, it’s apparent that areas of high densities aren’t easily discernible. Therefore, we may opt to customize our bandwidth to enhance the clarity and precision of our analysis.\nHowever, keep in mind that fixed bandwidths may not adapt well to spatial patterns that vary across different areas. Using the same bandwidth for one data set may result in oversmoothing or undersmoothing, but it can help us identify the general trend. Here I have chosen the bandwidth to be 800m and since we have already rescaled itto kilometers, we will write is as sigma = 0.8.\n\nOriginDestination\n\n\n\nkde_originSG_800 <- density(originSG_ppp.km, sigma=0.8, edge=TRUE, kernel=\"gaussian\")\nplot(kde_originSG_800)\n\n\n\n\n\n\n\nkde_destSG_800 <- density(destSG_ppp.km, sigma=0.8, edge=TRUE, kernel=\"gaussian\")\nplot(kde_destSG_800)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#converting-gridded-output-into-raster",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#converting-gridded-output-into-raster",
    "title": "Take Home Exercise 1",
    "section": "9.1 Converting Gridded Output into Raster",
    "text": "9.1 Converting Gridded Output into Raster\nWe will then convert the gridded kernal density objects into RasterLayer object by using raster() of raster package.\n\nkde_originSG_bw_raster <- raster(gridded_kde_originSG_bw)\nkde_destSG_bw_raster <- raster(gridded_kde_destSG_bw)\n\nTake a look at the properties of the raster layer\n\nkde_originSG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.4162063, 0.2250614  (x, y)\nextent     : 2.667538, 55.94194, 21.44847, 50.25633  (xmin, xmax, ymin, ymax)\ncrs        : NA \nsource     : memory\nnames      : v \nvalues     : -1.523218e-14, 310.3063  (min, max)\n\n\n\nkde_destSG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.4162063, 0.2250614  (x, y)\nextent     : 2.667538, 55.94194, 21.44847, 50.25633  (xmin, xmax, ymin, ymax)\ncrs        : NA \nsource     : memory\nnames      : v \nvalues     : -1.619782e-14, 299.8866  (min, max)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#assign-projection-systems",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#assign-projection-systems",
    "title": "Take Home Exercise 1",
    "section": "9.2 Assign Projection Systems",
    "text": "9.2 Assign Projection Systems\nFrom the properties observed, we can see that the CRS property is not available, so let’s assign to proper CRS by using this code below:\n\nprojection(kde_originSG_bw_raster) <- CRS(\"+init=EPSG:3414 +datum=WGS84 +units=km\")\nprojection(kde_destSG_bw_raster) <- CRS(\"+init=EPSG:3414 +datum=WGS84 +units=km\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#making-a-density-map-function",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#making-a-density-map-function",
    "title": "Take Home Exercise 1",
    "section": "9.3 Making a density map function",
    "text": "9.3 Making a density map function\nNow, we are going to generate the KDE raster layer over and OpenStreetMap basemap.\n\ntm_basemap(“OpenStreetMap”): specifies the basemap to be used\ntm_raster(“v”, alpha=0.65): adds the raster object to the map, argument “v” indicates that the raster object is t be visualised and the ‘alpha’ paramete set the transparency of the raster layer to 0.65.\nthe rest of the parameters are to customize the layout of the map\n\n\ndensity_map <- function(raster_object, map_title) {\n  tm_basemap(\"OpenStreetMap\") +\ntm_shape(raster_object) +\n  tm_raster(\"v\", alpha=0.65) + \n  tm_layout(legend.position = c(\"right\", \"bottom\"), \n            legend.height = 0.5, \n            legend.width = 0.4,\n            main.title = map_title,\n            main.title.position = 'center',\n            main.title.size = 1,\n            frame = FALSE)\n} \n\n\n9.3.1 Plotting KDE on Open Streetmap\nWe call the density_map() function that we created earlier and input our raster layer.\n\norigin_osm <- density_map(kde_originSG_bw_raster, map_title = 'Grab-hailing Origin Density Map')\ndestination_osm <- density_map(kde_destSG_bw_raster, map_title = 'Grab-hailing Destination Density Map')\n\nLet’s see the visualisation!\n\ntmap_arrange(origin_osm, destination_osm)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#comparing-spatial-point-patterns-using-kde",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#comparing-spatial-point-patterns-using-kde",
    "title": "Take Home Exercise 1",
    "section": "10.1 Comparing Spatial Point patterns using KDE",
    "text": "10.1 Comparing Spatial Point patterns using KDE\nMost of the steps performed here have already been explained previously, but now we are extracting the specific planning areas that we would like to analyse!\n\n10.1.1 Extracting Study Area\n\ntm = mpsz[mpsz@data$PLN_AREA_N == \"TAMPINES\",]\njw = mpsz[mpsz@data$PLN_AREA_N == \"JURONG WEST\",]\ndc = mpsz[mpsz@data$PLN_AREA_N == \"DOWNTOWN CORE\",]\nwl = mpsz[mpsz@data$PLN_AREA_N == \"WOODLANDS\",]\n\nPlotting the target areas:\n\npar(mfrow=c(2,2))\nplot(tm, main = \"Tampines\")\nplot(jw, main = \"Jurong West\")\nplot(dc, main = \"Downtown Core\")\nplot(wl, main = \"Woodlands\")\n\n\n\n\n\n\n10.1.2 Converting the spatial point dataframe into generic sp format\n\nwl_sp = as(wl, \"SpatialPolygons\")\ntm_sp = as(tm, \"SpatialPolygons\")\ndc_sp = as(dc, \"SpatialPolygons\")\njw_sp = as(jw, \"SpatialPolygons\")\n\n\n\n10.1.3 Creating Owin object\n\nwl_owin = as(wl_sp, \"owin\")\ntm_owin = as(tm_sp, \"owin\")\ndc_owin = as(dc_sp, \"owin\")\njw_owin = as(jw_sp, \"owin\")\n\n\n\n10.1.4 Combining Origin Points and Study Area\n\nor_wl_ppp = originSG_ppp[wl_owin]\nor_tm_ppp = originSG_ppp[tm_owin]\nor_dc_ppp = originSG_ppp[dc_owin]\nor_jw_ppp = originSG_ppp[jw_owin]\n\n\n\n10.1.5 Rescale\n\nor_wl_ppp.km = rescale(or_wl_ppp, 1000, \"km\")\nor_tm_ppp.km = rescale(or_tm_ppp, 1000, \"km\")\nor_dc_ppp.km = rescale(or_dc_ppp, 1000, \"km\")\nor_jw_ppp.km = rescale(or_jw_ppp, 1000, \"km\")\n\nPlotting the rescaled planning areas:\n\npar(mfrow=c(2,2))\nplot(or_tm_ppp.km, main = \"Tampines\")\nplot(or_jw_ppp.km, main = \"Jurong West\")\nplot(or_dc_ppp.km, main = \"Downtown Core\")\nplot(or_wl_ppp.km, main = \"Woodlands\")\n\n\n\n\n\n\n10.1.6 Computing Fixed Bandwidth KDE\n\npar(mfrow=c(2,2))\nplot(density(or_tm_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")\nplot(density(or_jw_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Jurong West\")\nplot(density(or_dc_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Downtown Core\")\nplot(density(or_wl_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Woodlands\")\n\n\n\n\nFrom the visualisations above, we can observe that the 2 planning areas with the higher KDE values are Downtown Core and Jurong West"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#nearest-neighbour-analysis",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#nearest-neighbour-analysis",
    "title": "Take Home Exercise 1",
    "section": "10.2 Nearest Neighbour Analysis",
    "text": "10.2 Nearest Neighbour Analysis\nIn this section, we’ll conduct the Clark-Evans test of aggregation to assess the spatial point pattern of childcare services using the clarkevans.test() function from the statspat package.\nThe test hypotheses are as follows:\nHo: The distribution of origin locations is random. H1: The distribution of origin locations is not random.\nWe will use a 95% confidence interval to evaluate the results.\nThis test helps us determine whether origin locations are clustered or dispersed across the study area. If the observed distribution significantly deviates from randomness, it suggests spatial aggregation or dispersion of origin locations.\n\n10.2.1 Testing Spatial Point Patterns using Clark and Evans Test\n\n# Whole Singapore\nclarkevans.test(originSG_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  originSG_ppp\nR = 0.27981, p-value < 2.2e-16\nalternative hypothesis: clustered (R < 1)\n\n\nThe p-value is less than 0.05, indicating that there is sufficient evidence to reject our null hypothesis that the distribution is random.\nBased on the Clark-Evans test results, there is strong evidence to suggest that the origin points of Grab rides are clustered rather than randomly distributed.\n\n# Downtown Core\nclarkevans.test(or_dc_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  or_dc_ppp\nR = 0.47245, p-value < 2.2e-16\nalternative hypothesis: clustered (R < 1)\n\n\n\n# Jurong West\nclarkevans.test(or_jw_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  or_jw_ppp\nR = 0.33423, p-value < 2.2e-16\nalternative hypothesis: clustered (R < 1)\n\n\nThe same conclusion can be drawn from the Clark and Evans test on Downtown Core and Jurong West Planning Areas."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#installing-and-loading-packages-into-r",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#installing-and-loading-packages-into-r",
    "title": "Take Home Exercise 1",
    "section": "11.1 Installing and Loading Packages into R",
    "text": "11.1 Installing and Loading Packages into R\nThere are a few other packages that we need to load into R:\n\nspNetwork: to derive the netowrk constrained kernel density estimation (NetKDE) and also used to build spatial matrices to conduct traditional spatial analysis with spatial weights based on reticular distances\nsp: provides classes and methods for dealing with spatial data in R, used to manage SpatialLinesDataframe and SpatialPointsDataFrame and for performing projection transformation\n\n\npacman::p_load(sp, sf, spNetwork, tmap)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#data-preparation-1",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#data-preparation-1",
    "title": "Take Home Exercise 1",
    "section": "11.2 Data Preparation",
    "text": "11.2 Data Preparation\nSince the road network data is very large and detailed, compuing the NetKDE on the whole network would take a lot of computational power. Hence, we are picking out only a subset of the whole data set to conduct this analysis. Based on our earlier analysis, it’s evident that the Central Business District (CBD) exhibits the highest kernel density estimation (KDE) values. Hence, we’ll narrow our focus to the Downtown Core planning area for this analysis, leveraging the wealth of insights available within this densely populated and highly active region.\n\n11.2.1 Getting the road intersection for Downtown Core\nFirst, we will use st_as_sf() to convert the Downtown Core Planning area into a sf object in order to perform the intersection.\n\ndc_sf<- st_as_sf(dc)\n\nNow, we can use st_intersection() of sf to perform a spatial intersection operation between the roads and Downtown Core planning area.\nroads_dc will return us only the road segments that fall within the boundaries of the Downtown Core area.\n\nroads_dc <- st_intersection(sg_roads3414, dc_sf)\nplot(st_geometry(roads_dc))\n\n\n\n\n\norigin_dc <- st_intersection(origin_sf,dc_sf)\nplot(st_geometry(origin_dc))\n\n\n\n\nTo visualise the geospatial data with high cartographic quality and interactive manner, the mapping function of tmap package can be used as shown in the code chunk below. Change the colour to red to be able to visualise the dots better.\n\ntmap_mode('view')\ntm_shape(origin_dc) + \n  tm_dots(col='red') + \n  tm_shape(roads_dc) +\n  tm_lines()\n\n\n\n\n\n\n\n\n11.2.3 Preparing the lixels objects\nThe generation of lixel lines would typically require the input geometries to be LINESTRING. Thus, in this code chunk below, we extract the geometry types of each feature in the roads_dc data set with st_geometry_type(). We then check the presence of any non-linestring geometry type in the data set.\n\ngeometry_types <- st_geometry_type(roads_dc)\nall_linestrings <- all(geometry_types == \"LINESTRING\")\nall_linestrings\n\n[1] FALSE\n\n\nSince there are non-linestring geometry types, this code chunk will help us remove these features.\n\nnon_linestring_indices <- which(geometry_types != \"LINESTRING\")\n# Remove features with non-linestring geometry types\nroads_dc <- roads_dc[-non_linestring_indices, ]\n\nWe can now prepare the lixel objects. Before computing NetKDE, the SpatialLines object has to be cut into lixels to split the road network into smaller, manageable units. We will use lixelize_lines(), with a minimum distance of 350km to ensure that the lixels are not too short.\n\nlixels <- lixelize_lines(roads_dc, \n                         700, \n                         mindist = 350)\n\nWe will use lines_center() of spNetwork to calculate the center points for each line segment.\n\nsamples <- lines_center(lixels)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#performing-netkde",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#performing-netkde",
    "title": "Take Home Exercise 1",
    "section": "11.4 Performing NetKDE",
    "text": "11.4 Performing NetKDE\nNow, we can perform the NetKDE!\nIn this code, there are quite a few steps to explain:\n\nnkde() of spNetwork\n\nevents parameter: the point events for which the KDE will be performed, which is origin_dc\nw parameter: specifies the weights associated with each event point\nquartric kernel function used, there are other possible kernel methods supprted by spNetwork (triangle, gaussian, scaled gaussian, tricube, cosine, triweight, epanechnikov or uniform)\nmethod argument: simple method is used to calculate the NKDE, the distances between events and sampling points are replaced by network distances, and the formula of the kernel is adapted to calculate the density over a linear unit instead of an areal unit.\nOther methods:\n“discontinuous”, which equally divides the mass density of an event at intersections of lixel\n“continuous”, which divides the mss of the density at the intersection but adjusts the density before the intersection to make the function continuous\n\n\n\ndensities <- nkde(roads_dc, \n                  events = origin_dc,\n                  w = rep(1,nrow(origin_dc)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5, #we aggregate events within a 5m radius (faster calculation)\n                  sparse = TRUE,\n                  verbose = FALSE)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#visualising-netkde",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01_test.html#visualising-netkde",
    "title": "Take Home Exercise 1",
    "section": "11.5 Visualising NetKDE",
    "text": "11.5 Visualising NetKDE\nThe code chunk below will insert the computed density values into samples and lixels objects as density field.\nSince SVY21 is in metres, the computed density values are very small, thus we have to rescale to number of events per kilometre once again.\n\nsamples$density <- densities\nlixels$density <- densities\n# rescaling to help the mapping\nsamples$density <- samples$density*1000\nlixels$density <- lixels$density*1000\n\nThe following code utilizes suitable functions from the tmap package to create interactive maps with high-quality cartographic visualization.\n\ntmap_mode('view')\ntm_basemap(server = \"OpenStreetMap\", alpha=0.5)+\ntm_shape(origin_dc)+\n  tm_dots()+\ntm_shape(lixels)+\n  tm_lines(col=\"density\")\n\n\n\n\n\n\nThe NetKDE layer visualizes the density of pick-up locations within the road network, highlighting areas with a higher frequency of pick-ups. By observing this map, Grab users and drivers can identify where pick-ups are more likely to occur, helping them plan routes or anticipate demand in certain areas.\nAccording to the OpenStreetMap, these areas are:\n\nNear Raffles Place\nNear Bugis/Guoco Midtown\nOphir Road\nNear Bayfront\n\nOverlaying the NetKDE layer on the OpenStreetMap provides an efficient way to pinpoint high-demand areas within the Downtown Core. Users can easily visualize and interpret density hotspots, and can facilitate their choice of pick-up locations uring peak-hour, to avoid high traffic areas. Depending on the Grab pricing algorithm, choosing a less crowded pick up location might also lead to lower fares as there is less demand in that area."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/data/MPSZ-2019/MPSZ-2019.html",
    "href": "Take-home_Ex/Take-home_Ex01/data/MPSZ-2019/MPSZ-2019.html",
    "title": "IS415-GAA-JS",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/data/geospatial/TAINAN_VILLAGE.html",
    "href": "Take-home_Ex/Take-home_Ex02/data/geospatial/TAINAN_VILLAGE.html",
    "title": "IS415-GAA-JS",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "",
    "text": "Dengue Hemorrhagic Fever (in short dengue fever) is one of the most widespread mosquito-borne diseases in the most tropical and subtropical regions. It is an acute disease caused by dengue virus infection which is transmitted by female Aedes aegypti and Aedes albopictus mosquitoes. In 2015, Taiwan had recorded the most severe dengue fever outbreak with more than 43,000 dengue cases and 228 deaths. Since then, the annual reported dengue fever cases were maintained at the level of not more than 200 cases. However, in 2023, Taiwan recorded 26703 dengue fever cases.\n\n\nIn this exercise, we are curious to find out:\n\nif the distribution of dengue fever outbreak at Tainan City, Taiwan are independent from space and space and time.\nIf the outbreak is indeed spatial and spatio-temporal dependent, then, you would like to detect where are the clusters and outliers, and the emerging hot spot/cold spot areas.\n\n\n\n\nThe specific tasks of this take-home exercise are as follows:\n\nUsing appropriate function of sf and tidyverse, preparing the following geospatial data layer:\n\na study area layer in sf polygon features. It must be at village level and confined to the D01, D02, D04, D06, D07, D08, D32 and D39 counties of Tainan City, Taiwan.\na dengue fever layer within the study area in sf point features. The dengue fever cases should be confined to epidemiology week 31-50, 2023.\na derived dengue fever layer in spacetime s3 class of sfdep. It should contain, among many other useful information, a data field showing number of dengue fever cases by village and by epidemiology week.\n\nUsing the extracted data, perform global spatial autocorrelation analysis by using sfdep methods.\nUsing the extracted data, perform local spatial autocorrelation analysis by using sfdep methods.\nUsing the extracted data, perform emerging hotspot analysis by using sfdep methods.\nDescribe the spatial patterns revealed by the analysis above."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#geospatial-data-pre-processing",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#geospatial-data-pre-processing",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "Geospatial Data Pre-processing",
    "text": "Geospatial Data Pre-processing\n\nInspecting the Projected Coordinate System\nAs this data is situated in Taiwan, the appropriate Geographic Coordinate System is EPSG::3824 (TWD97), which is the national Projected Coordinate System for Taiwan.\n\nst_crs(tainan)\n\nCoordinate Reference System:\n  User input: TWD97 \n  wkt:\nGEOGCRS[\"TWD97\",\n    DATUM[\"Taiwan Datum 1997\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"Taiwan, Republic of China - onshore and offshore - Taiwan Island, Penghu (Pescadores) Islands.\"],\n        BBOX[17.36,114.32,26.96,123.61]],\n    ID[\"EPSG\",3824]]\n\n\nLet’s view the dataset columns with the code below.\n\nhead(tainan,1)\n\nSimple feature collection with 1 feature and 10 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 120.2695 ymin: 22.93251 xmax: 120.2905 ymax: 22.96072\nGeodetic CRS:  TWD97\n     VILLCODE COUNTYNAME TOWNNAME VILLNAME     VILLENG COUNTYID COUNTYCODE\n1 67000280002     臺南市   歸仁區   六甲里 Liujia Vil.        D      67000\n  TOWNID TOWNCODE NOTE                       geometry\n1    D33 67000280 <NA> POLYGON ((120.2725 22.95868...\n\n\nThis dataset contains information on the towns and village names in Taiwan, which includes the village and town names, ID and codes.Each row is also tagged to a polygon geometry data.\nOur analysis at the village level is confined to the D01, D02, D04, D06, D07, D08, D32 and D39 counties of Tainan City, Taiwan. Hence, let’s filter the data to only retain information of the counties we want with the code chunk below:\n\nCreate a vector town_ids containing the town IDs of interest, namely “D01”, “D02”, “D04”, “D06”, “D07”, “D08”, “D32”, and “D39”\nCreate a subset of the dataframe tainan. It selects rows where the TOWNID column matches any of the town IDs specified in the town_ids vector. The resulting subset is stored in the variable tn_counties.\n\n\ntown_ids <- c(\"D01\", \"D02\", \"D04\", \"D06\", \"D07\", \"D08\", \"D32\", \"D39\")\ntn_counties <- subset(tainan, TOWNID %in% town_ids)\ntn_counties\n\nSimple feature collection with 258 features and 10 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 120.0627 ymin: 22.89401 xmax: 120.2925 ymax: 23.09144\nGeodetic CRS:  TWD97\nFirst 10 features:\n      VILLCODE COUNTYNAME TOWNNAME VILLNAME       VILLENG COUNTYID COUNTYCODE\n2  67000350032     臺南市   安南區   青草里  Qingcao Vil.        D      67000\n18 67000270011     臺南市   仁德區   保安里   Bao'an Vil.        D      67000\n66 67000370005     臺南市   中西區   赤嵌里  Chihkan Vil.        D      67000\n67 67000330004     臺南市     南區   大成里  Dacheng Vil.        D      67000\n68 67000350028     臺南市   安南區   城北里 Chengbei Vil.        D      67000\n69 67000350030     臺南市   安南區   城南里 Chengnan Vil.        D      67000\n73 67000370009     臺南市   中西區   法華里    Fahua Vil.        D      67000\n74 67000350017     臺南市   安南區   海南里   Hainan Vil.        D      67000\n75 67000350049     臺南市   安南區   國安里   Guo'an Vil.        D      67000\n76 67000350018     臺南市   安南區   溪心里    Xixin Vil.        D      67000\n   TOWNID TOWNCODE NOTE                       geometry\n2     D06 67000350 <NA> POLYGON ((120.1176 23.08387...\n18    D32 67000270 <NA> POLYGON ((120.2304 22.93544...\n66    D08 67000370 <NA> POLYGON ((120.2012 22.99966...\n67    D02 67000330 <NA> POLYGON ((120.1985 22.98147...\n68    D06 67000350 <NA> POLYGON ((120.1292 23.06512...\n69    D06 67000350 <NA> POLYGON ((120.1246 23.06904...\n73    D08 67000370 <NA> POLYGON ((120.2094 22.98452...\n74    D06 67000350 <NA> POLYGON ((120.175 23.02218,...\n75    D06 67000350 <NA> POLYGON ((120.1866 23.02766...\n76    D06 67000350 <NA> POLYGON ((120.1834 23.06086...\n\n\nThe main columns that we will use in the analysis would be the VILLNAME and TOWNNAME, so let’s check whether there are any missing values that we have to deal with before we proceed.\n\nsum(tn_counties$VILLNAME == \"None\")\n\n[1] 0\n\n\n\nsum(tn_counties$TOWNNAME == \"None\")\n\n[1] 0\n\n\nThere are no missing values so we are good to move on!\nLet us plot the tn_counties data!\n\ntmap_mode(\"plot\")\ntm_shape(tn_counties) +\n  tm_borders()"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#aspatial-data-wrangling",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#aspatial-data-wrangling",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "Aspatial Data Wrangling",
    "text": "Aspatial Data Wrangling\nSince the column names are in chinese, we should rename the columns that we will be using for easy reference.\n\ndengue_cases <- dengue_cases %>% rename(\"onset_date\" = \"發病日\",\n                      \"x_coord\" = \"最小統計區中心點X\",\n                      \"y_coord\" = \"最小統計區中心點Y\",\n                      \"county\" = \"居住縣市\",\n                      \"VILLNAME\" = \"居住村里\",\n                      \"TOWNNAME\" = \"居住鄉鎮\")\ncolnames(dengue_cases)\n\n [1] \"onset_date\"         \"個案研判日\"         \"通報日\"            \n [4] \"性別\"               \"年齡層\"             \"county\"            \n [7] \"TOWNNAME\"           \"VILLNAME\"           \"最小統計區\"        \n[10] \"x_coord\"            \"y_coord\"            \"一級統計區\"        \n[13] \"二級統計區\"         \"感染縣市\"           \"感染鄉鎮\"          \n[16] \"感染村里\"           \"是否境外移入\"       \"感染國家\"          \n[19] \"確定病例數\"         \"居住村里代碼\"       \"感染村里代碼\"      \n[22] \"血清型\"             \"內政部居住縣市代碼\" \"內政部居住鄉鎮代碼\"\n[25] \"內政部感染縣市代碼\" \"內政部感染鄉鎮代碼\"\n\n\nIn this analysis, we want to study the distribution of dengue fever outbreak at Tainan city that is within the epidemiology week 31-50, 2023.\nThe epidemiology week is a standardized way of counting weeks in epidemiological surveillance and reporting. It provides a common reference point for reporting and analysing disease outbreaks, trends and surveillance data. This helps public health authorities to identify seasonal patterns, detect outbreaks and monitor the effectiveness of interventions.\nTherefore, we will extract the cases that falls within the period of epidemiology week 31-50 in 2023. From a search online, I found that the weeks in Taiwan starts from Sunday, thus the dates that fall within these 20 weeks are 30th July 2023 to 16th December 2023.\nThe code below does the following things:\n\nIt adds two new columns to the dengue_cases dataframe:\n\nThe year column is created by extracting the year from the onset_date column and converting it into a factor variable.\nThe week column is created by extracting the week number from the onset_date.\n\nIt filters the rows in the dengue_cases dataframe based on the onset_date. Only rows where the onset_date falls between “2023-07-30” and “2023-12-16” (inclusive) are retained.\n\n\ndengue_cases <- dengue_cases %>%\n  mutate(year = factor(year(onset_date)),\n         week = week(onset_date)) %>%\n  filter(onset_date >= \"2023-07-30\" & onset_date <= \"2023-12-16\")\n\nWe should inspect the data to make sure there are no missing data in the table\n\nsum(dengue_cases$x_coord == \"None\")\n\n[1] 14\n\n\n\nsum(dengue_cases$y_coord == \"None\")\n\n[1] 14\n\n\n\nsum(dengue_cases$VILLNAME == \"None\")\n\n[1] 2600\n\n\n\nsum(dengue_cases$TOWNNAME == \"None\")\n\n[1] 0\n\n\nUpon inspecting the data, there are missing values in the X Y coordinate fields, as well as the VILLNAME field. There are a few ways we can deal with this. One common approach to handling missing values is to impute them with the mean or median values derived from the existing data. However, when dealing with coordinates, this approach may not be ideal. The mean or median coordinates may not accurately represent the missing locations, particularly in areas with diverse geography or uneven distribution of cases. Imputing with mean or median coordinates could potentially lead to misleading spatial analysis results, as it might place missing cases in locations where they do not actually exist.\nThus, a more feasible way to deal with the missing values would be to exclude these rows with missing coordinates from the analysis.\n\ndengue_cases <- dengue_cases %>%\n                 filter(x_coord != \"None\", y_coord != \"None\", \n                        VILLNAME != \"None\")\n\nNow, let’s check the fields to ensure all the rows with null values are removed.\n\nsum(dengue_cases$x_coord == \"None\")\n\n[1] 0\n\n\n\nsum(dengue_cases$y_coord == \"None\")\n\n[1] 0\n\n\n\nsum(dengue_cases$VILLNAME == \"None\")\n\n[1] 0\n\n\nWhen reviewing the dengue_cases dataset, we can see that the X and Y coordinate fields are currently represented in character format. To integrate the data frame into an sf data frame for spatial analysis, it’s necessary to convert these fields into numeric data types.\n\n\n# Convert character fields to numeric\ndengue_cases$x_coord <- as.numeric(dengue_cases$x_coord)\ndengue_cases$y_coord <- as.numeric(dengue_cases$y_coord)\n\n\nNow we can convert it into a sf object!\n\ndengue_sf <- st_as_sf(dengue_cases,\n                      coords = c(\"x_coord\",\"y_coord\"),\n                      crs = st_crs(tn_counties))\n\nWhen we look at the data in tn_counties, we will see that the VILLNAME column are not unique values and there are a few villages of the same name but located in a different town. This might cause problems when we are joining the two tables together later on since there will not be a one-to-one join, causing inaccurate results.\nTo circumvent this challenge, we can concatenate the TOWNNAME values and VILLNAME values to make a TOWNVILL column, to act as the unique identifier of a specific location.\n\ntn_counties <- tn_counties %>%\n  mutate(TOWNVILL = paste(TOWNNAME, VILLNAME, sep = \"_\"))\n\n\ndengue_cases <- dengue_cases %>%\n  mutate(TOWNVILL = paste(TOWNNAME, VILLNAME, sep = \"_\"))\n\nNow, lets quickly plot both datas to visualise the cases on the map.\n\ntm_shape(tn_counties) +\n  tm_polygons(\"TOWNID\") +\ntm_shape(dengue_sf) +\n  tm_dots(col = \"red\")\n\n\n\n\nNext, we can perform a left join to join both data frames into 1, which is tainan_dengue using the code below. This will result in a lot of columns being made, so we can use the select function to select the specific columns that we would like to keep for our subsequent analysis.\n\nLeft join, from the dplyr package, between the tn_counties and dengue_cases data frames using the TOWNVILL column as the key for joining\nAfter the join operation, it selects specific columns from the resulting data frame, including column 11 and columns 38 to 40 using select() from dplyr package\n\n\ntainan_dengue <- left_join(tn_counties, dengue_cases, by = c(\"TOWNVILL\")) %>%\n  select(11, 30, 38:40)\n\nNext with this joined data, we want to generate the summarized total number of dengue observations within each town and village boundary.\nWe can do so by using several functions from the dplyr package:\n\nData frame tainan_dengue is grouped by the TOWNVILL column\nWithin each group, calculate the total number of cases using the sum of 確定病例數 and select the first geometry associated with each group using first(geometry)\nRemove the grouping structure, ensuring that the resulting data frame is not grouped anymore\n\n\ntainan_dengue_df <- tainan_dengue %>%\n  group_by(TOWNVILL) %>%\n  summarize(total_cases = sum(確定病例數), geometry = first(geometry)) %>%\n  ungroup()\n\nSince we did a left join, there is a possibility that not all villages that are present in tn_counties have dengue cases reported, which will cause null values in the total_cases column.\nWe can check for the null values using is.na().\n\nsum(is.na(tainan_dengue_df$total_cases))\n\n[1] 33\n\n\nWe can see that there are 33 null values. Removing the rows with the null values might cause problems in the further analysis. In order to avoid that, we can replace the null values with 0 as a place holder using replace().\n\ntainan_dengue_df$total_cases <- replace(tainan_dengue_df$total_cases, is.na(tainan_dengue_df$total_cases), 0)\n\nWe can see that there are no more null values!\n\nsum(is.na(tainan_dengue_df$total_cases))\n\n[1] 0\n\n\nSince the data preparation is done, let’s save it"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#computing-contiguitiy-spatial-weights",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#computing-contiguitiy-spatial-weights",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "Computing Contiguitiy Spatial Weights",
    "text": "Computing Contiguitiy Spatial Weights\nBefore calculating the global spatial autocorrelation statistics, we must create spatial weights for the study area. Spatial weights establish the connections between neighboring geographic units (i.e. villages) within the study area, defining their neighborhood relationships.\nWe can pass a “queen” argument that takes True or False as options. If not specified, the default is set to TRUE, which will return a list of first order neighbours using the Queen criteria. This criteria means that if even a single point on the boundary of one unit coincides with a point on the boundary of another unit, they are considered neighbours.\n\nnb = st_contiguity(geometry): Computes spatial contiguity weights based on the geometry column, determining which observations (geographic units) share a border\nst_weight():\n\nstyle: Caluculates spatial weights based on the contiguity weights (“nb”) using specified style “W”\n.before = 1: Specifies that the new columns (‘nb’ and ‘wt’) should be inserted at the first position in the dataset\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere are different styles to define the spatial weight, which determine the strength of relationships between neighbouring geographic units. The default that we have used here is “W”, which is “row standardized weights”.\nThere are other values: “B”, “C”, “U”, “minmax”, and “S”. Each option has its own advantages and is suitable for different types of analyses and datasets, depending on the nature of spatial relationships and the objectives of the analysis.\n\n\n\nwm_q <- tainan_dengue_df %>%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#computing-global-morani",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#computing-global-morani",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "Computing Global Moran’I",
    "text": "Computing Global Moran’I\nThis code computes the Global Moran’s I statistic, which is a measure of spatial autocorrelation indicating whether the spatial pattern is clustered, dispersed or randomly distributed.\nglobal_moran(): This function calculates the Global Moran’s I statistic. It takes three main arguments:\n\nx: The variable for which spatial autocorrelation is being measured (in this case, wm_q$total_cases).\nlistw: The spatial weights matrix, which defines the spatial relationships between observations (in this case, wm_q$nb).\nweights: The type of spatial weights to be used (in this case, wm_q$wt)\n\nWe can use the glimpse() function to display a concise summary of the moranI object.\n\nmoranI <- global_moran(wm_q$total_cases,\n                       wm_q$nb,\n                       wm_q$wt)\nglimpse(moranI)\n\nList of 2\n $ I: num 0.42\n $ K: num 5.58"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#performing-global-morans-i-test",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#performing-global-morans-i-test",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "Performing Global Moran’s I test",
    "text": "Performing Global Moran’s I test\nThe global_moran_test() conducts a hypothesis test for global spatial autocorrelation to assess the statistical significance of the Global Moran’s I statistic.\nThe hypotheses that we will be testing is:\n\nNull hypothesis (H0) states that there is no spatial autocorrelation present in the dataset (dengue cases is randomly distributed across space)\nAlternative hypothesis (H1) states that there is spatial autocorrelation present in the dataset (dengue cases exhibits clustering or dispersion tendencies)\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe default for alternative argument is “two.sided”. Other supported arguments are “greater” or “less”. randomization, and\nBy default the randomization argument is TRUE. If FALSE, under the assumption of normality.\n\n\n\n\nglobal_moran_test(wm_q$total_cases,\n                       wm_q$nb,\n                       wm_q$wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 11.538, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.419567083      -0.003891051       0.001347083 \n\n\nFrom the results of this test, the p-value calculated is less than 2.2e-16, which is significantly smaller than the significance level of 0.05. This indicates strong evidence against the null hypothesis of spatial randomness. Therefore, we reject the null hypothesis and conclude that the observed spatial pattern is highly unlikely to occur by random chance alone, with 95% confidence. Since the alternative hypothesis is “greater”, suggesting that there is positive spatial autocorrelation (clustering) in the data.\nThe value of Moran’s I statistic is 0.4196. This value ranges between -1 and 1. Hence, a positive value indicates positive value suggests that similar values of the variable being analyzed tend to be clustered together."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#performing-global-morani-permutation-test",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#performing-global-morani-permutation-test",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "Performing Global Moran’I Permutation Test",
    "text": "Performing Global Moran’I Permutation Test\nset.seed(1234) sets the random number generator’s seed to a specific value, in this case, 1234. This ensures that when you generate random numbers using functions like runif(), rnorm(), or other random number generators in R, you get the same sequence of random numbers every time you run your code.\nThis is useful for reproducibility purposes, especially when conducting simulations or experiments that involve randomness.\n\nset.seed(1234)\n\nNext, global_moran_perm() is used to perform Monte Carlo simulation.\n\nglobal_moran_perm(wm_q$total_cases,\n                       wm_q$nb,\n                       wm_q$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.41957, observed rank = 100, p-value < 2.2e-16\nalternative hypothesis: two.sided\n\n\nThe statistical report above shows that the p-value is smaller than alpha value of 0.05. Thus, we have enough statistical evidence to reject the null hypothesis that the dengue cases resemble random distribution. Also, as explained previously, the Moran’s I statistics is greater than 0, which we can infer that the spatial distribution shows signs of clustering."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualising-lisa-map",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualising-lisa-map",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "Visualising LISA Map",
    "text": "Visualising LISA Map\nThe LISA map categorizes spatial data to identify outliers and clusters, including High-Low and Low-High outliers, as well as High-High and Low-Low clusters. It’s generated by integrating local Moran’s I values and their associated p-values for different geographic areas.\nWithin the LISA spatial dataframe, there are three fields representing LISA categories: mean, median, and pysal. Generally, the mean classification is utilized for interpretation and analysis, as illustrated in the code below.\nThis code filters the LISA objects to include only those with significant p-values. It then creates a map using tmap, plotting all LISA objects but only filling the LISA objects that have statistically significant results based on their p-values. These significant objects are highlighted on the map, indicating where spatial clustering or outliers are present with a 95% confidence level.\nThe mean value is then used to help identify whether the clustering is characterized by high-high (similar high values clustered together) or low-low (similar low values clustered together) patterns, or if outliers are present (high-low or low-high patterns).\n\nlisa_sig <- lisa  %>%\n  filter(p_ii < 0.05)\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)\n\n\n\n\nFrom this map, we can now clearly see where are the clusters of high number of cases and clusters of low number of cases.\nThe central area of Tainan stands out as the most significant location with multiple high-high clusters, spanning across approximately 20 villages. Several factors may contribute to this concentrated pattern, including population density, environmental conditions, and human activities.\nFor instance, a comprehensive study on the Space-Time Analysis of the Dengue outbreak in Tainan revealed a major hotspot in the West-Central district during September. This district, characterized by its ancient buildings and high population density, emerges as a focal point for Dengue clusters. The age of the community might contribute to a lower awareness of the outbreak and preventive measures among the older population. Moreover, the high population density amplifies the impact of individual behaviors, potentially facilitating the spread of the virus from one person to another.\nIn essence, the presence of multiple high-high clusters in the central area suggests a complex interplay of demographic, environmental, and cultural factors that contribute to the spatial dynamics of the Dengue outbreak in Tainan."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#computing-local-gi-statistics",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#computing-local-gi-statistics",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "Computing Local Gi* statistics",
    "text": "Computing Local Gi* statistics\nAs usual, we will need to derive a spatial weight matrix before we can compute local Gi* statistics. Code chunk below will be used to derive a spatial weight matrix by using sfdep functions and tidyverse approach.\n\nCalculate spatial weights for an inverse distance weighted interpolation (IDW) analysis\n\n\nwm_idw <- tainan_dengue_df %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\nNow, we will perform HCSA using the calculated spatial weights\n\nHCSA <- wm_idw %>% \n  mutate(local_Gi = local_gstar_perm(\n    total_cases, nb, wt, nsim = 99),\n         .before = 1) %>%\n  unnest(local_Gi)\nHCSA\n\nSimple feature collection with 258 features and 14 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 120.0627 ymin: 22.89401 xmax: 120.2925 ymax: 23.09144\nGeodetic CRS:  TWD97\n# A tibble: 258 × 15\n   gi_star cluster    e_gi    var_gi std_dev p_value p_sim p_folded_sim skewness\n     <dbl> <fct>     <dbl>     <dbl>   <dbl>   <dbl> <dbl>        <dbl>    <dbl>\n 1 -1.59   Low     0.00349   1.29e-6 -1.46    0.143   0.18         0.09   0.471 \n 2  0.0390 High    0.00408   1.87e-6 -0.0961  0.923   0.92         0.46   0.349 \n 3 -1.63   Low     0.00333   1.69e-6 -1.31    0.190   0.14         0.07   1.30  \n 4 -1.23   Low     0.00332   1.75e-6 -0.871   0.383   0.36         0.18   0.202 \n 5 -1.90   Low     0.00342   1.40e-6 -1.68    0.0920  0.06         0.03   0.512 \n 6 -2.00   Low     0.00344   1.40e-6 -1.82    0.0692  0.06         0.03  -0.0488\n 7 -0.569  Low     0.00344   1.81e-6 -0.264   0.792   0.88         0.44   0.666 \n 8 -1.11   Low     0.00384   1.54e-6 -1.06    0.288   0.28         0.14   0.553 \n 9 -1.92   Low     0.00332   1.63e-6 -1.50    0.134   0.1          0.05   0.473 \n10 -2.37   Low     0.00320   1.82e-6 -2.13    0.0335  0.04         0.02   0.229 \n# ℹ 248 more rows\n# ℹ 6 more variables: kurtosis <dbl>, nb <nb>, wts <list>, TOWNVILL <chr>,\n#   total_cases <dbl>, geometry <POLYGON [°]>"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualising-local-hcsa",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualising-local-hcsa",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "Visualising Local HCSA*",
    "text": "Visualising Local HCSA*\nThe code below generates 2 thematic map based on the HCSA.The first map shows the values of the GI* statistic calculated for dengue fever cases in Tainan. The second map visualizes the p-value associated with the Gi* statistic calculated.\n\ntmap_mode(\"plot\")\nmap1 <- tm_shape(HCSA) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"Gi* of Dengue Cases\",\n            main.title.size = 0.8)\n\nmap2 <- tm_shape(HCSA) +\n  tm_fill(\"p_value\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of Gi*\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualising-hot-spot-and-cold-spot-areas",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualising-hot-spot-and-cold-spot-areas",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "Visualising Hot Spot and Cold Spot Areas",
    "text": "Visualising Hot Spot and Cold Spot Areas\nSimilarly, this map plots the the significant (i.e. p-values less than 0.05) hot spot and cold spot areas by using appropriate tmap functions.\n\nHCSA_sig <- HCSA  %>%\n  filter(p_sim < 0.05)\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(HCSA_sig) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.4)\n\n\n\n\nPositive Gi* values indicate that a feature has high values and is surrounded by other features with similarly high values, suggesting a hot spot (cluster of high values). Conversely, negative Gi* values indicate that a feature has low values and is surrounded by other features with similarly low values, suggesting a cold spot (cluster of low values).\nWhen comparing to the previous Local Moran’s I analysis, we can see that the most significant cold spot coincides with the most significant low-low cluster previously. This could suggest that the results from the two analyses are consistent since they both reinforce the spatial pattern of low values in the north-western areas."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#creating-a-time-series-cube",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#creating-a-time-series-cube",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "Creating a Time Series Cube",
    "text": "Creating a Time Series Cube\nA space-time cube is a conceptual representation used in spatio-temporal analysis to visualize and analyze spatio-temporal data.\nIt is a three-dimensional construct that integrates spatial and temporal dimensions:\n\nSpatial dimension of the space-time cube represents the geographic extent or area under study\nTemporal dimension of the space-time cube represents time, typically divided into discrete time intervals or periods\nData observations are organized and stored in a structured format that reflects both the spatial and temporal dimensions\n\n\nData Preparation\nIn order to create the space-time cube, two input objects are required:\n\nSf tibble data table that includes a geometry column (polygon feature class)\nAspatial tibble data frame that includes at least 3 columns, one for temporal, one for the unique ID and one for observation values\n\nWe aim to construct a spatio-temporal framework spanning from week 31 to week 50, hence we want to group our previously joined and cleaned data, tainan_dengue, by TOWNVILL and week. Get the sum of dengue cases in each village per week by using sum()is used to sum up the values of the variable 確定病例數\nSince this is the aspatial data input, we can use st_drop_geometry() to drop the geometry column from the dataset.\n\ndengue_VILLWEEK <- tainan_dengue %>%\n  group_by(week, TOWNVILL) %>%\n  summarise(total_cases = sum(確定病例數)) %>%\n  st_drop_geometry()\n\nGiven a number of spatial features n, and time periods m, a spatio-temporal full grid contains n x m rows. Each location has a recorded observation for each of the time periods in m. For example, if there are 10 locations and 20 time periods, there are 20 observations per location meaning there are 10 x 20 = 200 observations. This is efficient only when are there are complete time-series for each location.\n\nThe tn_counties data that we prepared earlier shows that there are 258 villages in our analysis.\nThis means that our full grid should contain 258 x 20 (weeks) = 5160 rows of data. Each row represents a unique combination of village and week, with observations or measurements recorded for each village across all 20 weeks. However, currently the dengue_VILLWEEK data only consists of 3082 rows, which suggests that the time series is incomplete.\n\nWhen we inspect the dengue_VILLWEEK data, we can see that there are some TOWNVILL rows that have missing week and total_cases values. This is because in the dataset for dengue cases, there are no records for dengue cases in these TOWNVILL. We can also see that the existing TOWNVILL data could also have missing week rows. Thus, we have to use the code below to insert the rows for missing weeks of each TOWNVILL and insert the total_cases as 0. This also inserts the week and total_cases data for the rows that have missing values.\n\n# Get unique TOWNVILL values\nunique_TOWNVILL <- unique(dengue_VILLWEEK$TOWNVILL)\n\n# Loop through each unique TOWNVILL value\nfor (town_vill in unique_TOWNVILL) {\n  # Subset dengue_VILLWEEK for the current TOWNVILL\n  subset_joinMe <- dengue_VILLWEEK[dengue_VILLWEEK$TOWNVILL == town_vill, ]\n  \n  # Get unique weeks within the subset\n  unique_weeks <- unique(subset_joinMe$week)\n  \n  # Create a vector of all weeks from 31 to 50\n  all_weeks <- 31:50\n  \n  # Find missing weeks\n  missing_weeks <- setdiff(all_weeks, unique_weeks)\n  \n  # Add rows for missing weeks with total_cases as 0\n  for (week in missing_weeks) {\n    new_row <- data.frame(TOWNVILL = town_vill,\n                          week = week,\n                          total_cases = 0)\n    dengue_VILLWEEK <- rbind(dengue_VILLWEEK, new_row)\n  }\n}\n\n  #Drop extra rows that had null values\ndengue_VILLWEEK <- na.omit(dengue_VILLWEEK)\n\n\nNow we have 5160 observations! We can now convert this table into a tibble dataframe.\n\ndengue_VILLWEEK <- as_tibble(dengue_VILLWEEK)\n\n\n\nCreate Space Time Cube\nIn the code chunk below, spacetime() of sfdep is used to create an spatio-temporal cube. is_spacetime_cube() of sfdep package will be used to varify if tainan_st is indeed a space-time cube object.\n\ntainan_st <- spacetime(.data = dengue_VILLWEEK, .geo = tn_counties, \"TOWNVILL\", \"week\")\n\nis_spacetime_cube(tainan_st)\n\n[1] TRUE\n\n\nThe TRUE return confirms that tainan_st object is indeed a space-time cube."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#computing-gi",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#computing-gi",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "Computing Gi*",
    "text": "Computing Gi*\nWe should compute the Gi* to identify locations that exhibit significant spatial patterns.\n\nDeriving the Spatial Weights\nThe code chunk below will be used to identify neighbors and to derive an inverse distance weights.\n\nactivate() of dplyr package is used to activate the geometry context\nmutate() of dplyr package is used to create two new columns nb and wt.\nThen we will activate the data context again and copy over the nb and wt columns to each time-slice using set_nbs() and set_wts()\n\nrow order is very important so do not rearrange the observations after using set_nbs() or set_wts().\n\n\n\ndengue_nb <- tainan_st %>%\n  activate(\"geometry\") %>%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         wt = st_inverse_distance(nb, geometry,\n                                  scale = 1,\n                                  alpha = 1),\n         .before = 1) %>%\n  set_nbs(\"nb\") %>%\n  set_wts(\"wt\")\n\nNote that this dataset now has neighbors and weights for each time-slice.\n\nhead(dengue_nb)\n\n# A tibble: 6 × 5\n   week TOWNVILL      total_cases nb        wt       \n  <dbl> <chr>               <dbl> <list>    <list>   \n1    31 安南區_青草里           0 <int [4]> <dbl [4]>\n2    31 仁德區_保安里           1 <int [6]> <dbl [6]>\n3    31 中西區_赤嵌里           0 <int [9]> <dbl [9]>\n4    31 南區_大成里             0 <int [7]> <dbl [7]>\n5    31 安南區_城北里           0 <int [5]> <dbl [5]>\n6    31 安南區_城南里           0 <int [8]> <dbl [8]>\n\n\n\n\nGi*\nWe can use these new columns to manually calculate the local Gi* for each location. We can do this by grouping by week and using local_gstar_perm() of sfdep package. After which, we use unnest() to unnest gi_star column of the newly created gi_starts data.frame.\n\ngi_stars <- dengue_nb %>% \n  group_by(week) %>% \n  mutate(gi_star = local_gstar_perm(\n    total_cases, nb, wt)) %>% \n  tidyr::unnest(gi_star)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#mann-kendall-test",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#mann-kendall-test",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "Mann-Kendall Test",
    "text": "Mann-Kendall Test\nThe Mann-Kendall test is a non-parametric statistical test used to detect trends in time series data. It evaluates the statistical significance of trends in time series data by comparing the observed values with expected values under the null hypothesis of no trend.\nThe hypotheses we are testing are:\n\nNull hypothesis (H0): There is no trend present in the time series data.\nAlternative hypothesis (H1): The data points are not randomly distributed but exhibit a consistent increase or decrease over time.\n\nIf the p-value associated with the test statistic is less than the chosen significance level (e.g., 0.05 or 0.01), we reject the null hypothesis, concluding that there is a statistically significant trend present in the data. If the p-value is greater than the significance level, we fail to reject the null hypothesis, suggesting that there is no significant trend in the data. In the output, sl is the p-value.\nWith these Gi* measures we can evaluate each location for a trend using the Mann-Kendall test. The locations I chose to evaluate are the hot and cold spots that are statistically significant, which we derived from our previous HCSA. I loaded the Tainan shapefile into QGIS to identify their town and village names.\n\n\n\n\n\n安南區_顯宮里 (COLD)安南區_砂崙里 (COLD)安南區_大安里 (HOT)永康區_東橋里 (HOT)\n\n\n\ncbg1 <- gi_stars %>% \n  ungroup() %>% \n  filter(TOWNVILL == \"安南區_顯宮里\") |> \n  select(TOWNVILL, week, gi_star)\n\np <- ggplot(data = cbg1, \n       aes(x = week, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(p)\n\n\n\n\n\n\ncbg1 %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 × 5\n      tau    sl     S     D  varS\n    <dbl> <dbl> <dbl> <dbl> <dbl>\n1 -0.0947 0.581   -18  190.   950\n\n\nSince sl is much greater than the significance level of 0.05, we fail to reject the null hypothesis, suggesting that there is no significant trend in the data.\n\n\n\ncbg2 <- gi_stars %>% \n  ungroup() %>% \n  filter(TOWNVILL == \"安南區_砂崙里\") |> \n  select(TOWNVILL, week, gi_star)\n\np <- ggplot(data = cbg2, \n       aes(x = week, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(p)\n\n\n\n\n\n\ncbg2 %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 × 5\n      tau    sl     S     D  varS\n    <dbl> <dbl> <dbl> <dbl> <dbl>\n1 -0.0842 0.626   -16  190.   950\n\n\nSince sl is much greater than the significance level of 0.05, we fail to reject the null hypothesis, suggesting that there is no significant trend in the data.\n\n\n\ncbg3 <- gi_stars %>% \n  ungroup() %>% \n  filter(TOWNVILL == \"安南區_大安里\") |> \n  select(TOWNVILL, week, gi_star)\n\np <- ggplot(data = cbg3, \n       aes(x = week, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(p)\n\n\n\n\n\n\ncbg3 %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 × 5\n     tau      sl     S     D  varS\n   <dbl>   <dbl> <dbl> <dbl> <dbl>\n1 -0.453 0.00582   -86  190.   950\n\n\nThe sl value here is much smaller but it is still greater than the significance level of 0.05. This tells us there is a downward but insignificant trend.\n\n\n\ncbg4 <- gi_stars %>% \n  ungroup() %>% \n  filter(TOWNVILL == \"永康區_東橋里\") |> \n  select(TOWNVILL, week, gi_star)\n\np <- ggplot(data = cbg4, \n       aes(x = week, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(p)\n\n\n\n\n\n\ncbg4 %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 × 5\n     tau      sl     S     D  varS\n   <dbl>   <dbl> <dbl> <dbl> <dbl>\n1 -0.442 0.00708   -84  190.   950\n\n\nThe sl value here is greater than the significance level of 0.05. This tells us there is a downward but insignificant trend.\n\n\n\nWe can replicate this for each location by using group_by() of dplyr package. The code below performs the Mann-Kendall test for each location in the dataset, allowing for the assessment of trends in the spatial data at a local level.\n\nehsa <- gi_stars %>%\n  group_by(TOWNVILL) %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>%\n  tidyr::unnest_wider(mk)\n\n\nArrange to show significant emerging hot/cold spots\nThe code below arranges the ehsa dataframe base of the ‘sl’ and ‘tau’ values. This is to prioritize locations with significant trends and stronger associations. Then use the slice() functions to select the top 5 rows, which are the top 5 locations with the most significant emerging hot/cold spots.\n\nemerging <- ehsa %>% \n  arrange(sl, abs(tau)) %>% \n  slice(1:5)\n\n\n\nPerforming Emerging Hotspot Analysis\nLastly, we will perform EHSA analysis by using emerging_hotspot_analysis() of sfdep package.\n\nIt takes a spacetime object x (i.e. tainan_st), and the quoted name of the variable of interest (i.e. total_cases) for .var argument.\nThe k argument is used to specify the number of time lags which is set to 1 by default.\nLastly, nsim map numbers of simulation to be performed.\n\n\nehsa <- emerging_hotspot_analysis(\n  x = tainan_st, \n  .var = \"total_cases\", \n  k = 1, \n  nsim = 99\n)\n\n\n\nVisualising the distribution of EHSA classes\nIn the code chunk below, ggplot2 functions ised used to reveal the distribution of EHSA classes as a bar chart.\n\nggplot(data = ehsa, aes(x = classification)) +\n  geom_bar() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\nThe figure above shows that oscillating hotspot has the highest numbers of villages.\nThis means that the spatial pattern of the occurrence of dengue cases varies periodically or cyclically over time. In the case of dengue fever, it can result in an oscillating hotspot due to various factors that affect the dynamics of dengue transmission. One highly plausible reason could be climatic factors, causing dengue fever transmission to occur more during hot, humid weather with heavy rainfall.\n\n\nVisualising EHSA\nIn order to visualise the geographic distribution of EHSA classes, we need to join tn_counties and ehsa together.\n\ntainan_ehsa <- tn_counties %>%\n  left_join(ehsa,\n            by = join_by(TOWNVILL == location))\n\n\nehsa_sig <- tainan_ehsa  %>%\n  filter(p_value < 0.05)\ntmap_mode(\"plot\")\ntm_shape(tainan_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(ehsa_sig) +\n  tm_fill(\"classification\") + \n  tm_borders(alpha = 0.4)\n\n\n\n\nFrom the map we can see that there are few areas that are oscillating coldspots as well, where the cases also fluctuates periodically but results in localized areas of lower dengue occurrences compared to surrounding areas.\nHowever, from the map we can also identify many areas that are sporadic hotspots, which have unusually high number of cases compared to neighbour but there are no clear trends. This can be due to localized characteristics or human behaviour, making it unpredictable."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html",
    "title": "Hands On Exercise 8",
    "section": "",
    "text": "Geographically weighted regression (GWR) is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable). In this hands-on exercise, you will learn how to build hedonic pricing models by using GWR methods. The dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#importing-geospatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#importing-geospatial-data",
    "title": "Hands On Exercise 8",
    "section": "Importing geospatial data",
    "text": "Importing geospatial data\nThe geospatial data used in this hands-on exercise is called MP14_SUBZONE_WEB_PL. It is in ESRI shapefile format. The shapefile consists of URA Master Plan 2014’s planning subzone boundaries. Polygon features are used to represent these geographic boundaries. The GIS data is in svy21 projected coordinates systems.\nThe code chunk below is used to import MP_SUBZONE_WEB_PL shapefile by using st_read() of sf packages.\n\nmpsz = st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\jaymieseet\\IS415-GAA-JS\\Hands-on_Ex\\Hands-on_Ex08\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nThe report above shows that the R object used to contain the imported MP14_SUBZONE_WEB_PL shapefile is called mpsz and it is a simple feature object. The geometry type is multipolygon. it is also important to note that mpsz simple feature object does not have EPSG information."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#updating-crs-information",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#updating-crs-information",
    "title": "Hands On Exercise 8",
    "section": "Updating CRS information",
    "text": "Updating CRS information\nThe code chunk below updates the newly imported mpsz with the correct ESPG code (i.e. 3414)\n\nmpsz_svy21 <- st_transform(mpsz, 3414)\n\nAfter transforming the projection metadata, you can varify the projection of the newly transformed mpsz_svy21 by using st_crs() of sf package.\nThe code chunk below will be used to varify the newly transformed mpsz_svy21.\n\nst_crs(mpsz_svy21)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nNotice that the EPSG: is indicated as 3414 now.\nNext, you will reveal the extent of mpsz_svy21 by using st_bbox() of sf package.\n\nst_bbox(mpsz_svy21) #view extent\n\n     xmin      ymin      xmax      ymax \n 2667.538 15748.721 56396.440 50256.334"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#importing-the-aspatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#importing-the-aspatial-data",
    "title": "Hands On Exercise 8",
    "section": "Importing the aspatial data",
    "text": "Importing the aspatial data\nThe condo_resale_2015 is in csv file format. The codes chunk below uses read_csv() function of readr package to import condo_resale_2015 into R as a tibble data frame called condo_resale.\n\ncondo_resale = read_csv(\"data/aspatial/Condo_resale_2015.csv\")\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly.\nThe codes chunks below uses glimpse() to display the data structure of will do the job.\n\nglimpse(condo_resale)\n\nRows: 1,436\nColumns: 23\n$ LATITUDE             <dbl> 1.287145, 1.328698, 1.313727, 1.308563, 1.321437,…\n$ LONGITUDE            <dbl> 103.7802, 103.8123, 103.7971, 103.8247, 103.9505,…\n$ POSTCODE             <dbl> 118635, 288420, 267833, 258380, 467169, 466472, 3…\n$ SELLING_PRICE        <dbl> 3000000, 3880000, 3325000, 4250000, 1400000, 1320…\n$ AREA_SQM             <dbl> 309, 290, 248, 127, 145, 139, 218, 141, 165, 168,…\n$ AGE                  <dbl> 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22, 6,…\n$ PROX_CBD             <dbl> 7.941259, 6.609797, 6.898000, 4.038861, 11.783402…\n$ PROX_CHILDCARE       <dbl> 0.16597932, 0.28027246, 0.42922669, 0.39473543, 0…\n$ PROX_ELDERLYCARE     <dbl> 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.121…\n$ PROX_URA_GROWTH_AREA <dbl> 6.618741, 7.505109, 6.463887, 4.906512, 6.410632,…\n$ PROX_HAWKER_MARKET   <dbl> 1.76542207, 0.54507614, 0.37789301, 1.68259969, 0…\n$ PROX_KINDERGARTEN    <dbl> 0.05835552, 0.61592412, 0.14120309, 0.38200076, 0…\n$ PROX_MRT             <dbl> 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.528…\n$ PROX_PARK            <dbl> 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.116…\n$ PROX_PRIMARY_SCH     <dbl> 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.709…\n$ PROX_TOP_PRIMARY_SCH <dbl> 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.709…\n$ PROX_SHOPPING_MALL   <dbl> 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.307…\n$ PROX_SUPERMARKET     <dbl> 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.581…\n$ PROX_BUS_STOP        <dbl> 0.10336166, 0.28673408, 0.28504777, 0.29872340, 0…\n$ NO_Of_UNITS          <dbl> 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34, 3…\n$ FAMILY_FRIENDLY      <dbl> 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD             <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\n\nhead(condo_resale$LONGITUDE) #see the data in XCOORD column\n\n[1] 103.7802 103.8123 103.7971 103.8247 103.9505 103.9386\n\n\n\nhead(condo_resale$LATITUDE) #see the data in YCOORD column\n\n[1] 1.287145 1.328698 1.313727 1.308563 1.321437 1.314198\n\n\nNext, summary() of base R is used to display the summary statistics of cond_resale tibble data frame.\n\nsummary(condo_resale)\n\n    LATITUDE       LONGITUDE        POSTCODE      SELLING_PRICE     \n Min.   :1.240   Min.   :103.7   Min.   : 18965   Min.   :  540000  \n 1st Qu.:1.309   1st Qu.:103.8   1st Qu.:259849   1st Qu.: 1100000  \n Median :1.328   Median :103.8   Median :469298   Median : 1383222  \n Mean   :1.334   Mean   :103.8   Mean   :440439   Mean   : 1751211  \n 3rd Qu.:1.357   3rd Qu.:103.9   3rd Qu.:589486   3rd Qu.: 1950000  \n Max.   :1.454   Max.   :104.0   Max.   :828833   Max.   :18000000  \n    AREA_SQM          AGE           PROX_CBD       PROX_CHILDCARE    \n Min.   : 34.0   Min.   : 0.00   Min.   : 0.3869   Min.   :0.004927  \n 1st Qu.:103.0   1st Qu.: 5.00   1st Qu.: 5.5574   1st Qu.:0.174481  \n Median :121.0   Median :11.00   Median : 9.3567   Median :0.258135  \n Mean   :136.5   Mean   :12.14   Mean   : 9.3254   Mean   :0.326313  \n 3rd Qu.:156.0   3rd Qu.:18.00   3rd Qu.:12.6661   3rd Qu.:0.368293  \n Max.   :619.0   Max.   :37.00   Max.   :19.1804   Max.   :3.465726  \n PROX_ELDERLYCARE  PROX_URA_GROWTH_AREA PROX_HAWKER_MARKET PROX_KINDERGARTEN \n Min.   :0.05451   Min.   :0.2145       Min.   :0.05182    Min.   :0.004927  \n 1st Qu.:0.61254   1st Qu.:3.1643       1st Qu.:0.55245    1st Qu.:0.276345  \n Median :0.94179   Median :4.6186       Median :0.90842    Median :0.413385  \n Mean   :1.05351   Mean   :4.5981       Mean   :1.27987    Mean   :0.458903  \n 3rd Qu.:1.35122   3rd Qu.:5.7550       3rd Qu.:1.68578    3rd Qu.:0.578474  \n Max.   :3.94916   Max.   :9.1554       Max.   :5.37435    Max.   :2.229045  \n    PROX_MRT         PROX_PARK       PROX_PRIMARY_SCH  PROX_TOP_PRIMARY_SCH\n Min.   :0.05278   Min.   :0.02906   Min.   :0.07711   Min.   :0.07711     \n 1st Qu.:0.34646   1st Qu.:0.26211   1st Qu.:0.44024   1st Qu.:1.34451     \n Median :0.57430   Median :0.39926   Median :0.63505   Median :1.88213     \n Mean   :0.67316   Mean   :0.49802   Mean   :0.75471   Mean   :2.27347     \n 3rd Qu.:0.84844   3rd Qu.:0.65592   3rd Qu.:0.95104   3rd Qu.:2.90954     \n Max.   :3.48037   Max.   :2.16105   Max.   :3.92899   Max.   :6.74819     \n PROX_SHOPPING_MALL PROX_SUPERMARKET PROX_BUS_STOP       NO_Of_UNITS    \n Min.   :0.0000     Min.   :0.0000   Min.   :0.001595   Min.   :  18.0  \n 1st Qu.:0.5258     1st Qu.:0.3695   1st Qu.:0.098356   1st Qu.: 188.8  \n Median :0.9357     Median :0.5687   Median :0.151710   Median : 360.0  \n Mean   :1.0455     Mean   :0.6141   Mean   :0.193974   Mean   : 409.2  \n 3rd Qu.:1.3994     3rd Qu.:0.7862   3rd Qu.:0.220466   3rd Qu.: 590.0  \n Max.   :3.4774     Max.   :2.2441   Max.   :2.476639   Max.   :1703.0  \n FAMILY_FRIENDLY     FREEHOLD      LEASEHOLD_99YR  \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.4868   Mean   :0.4227   Mean   :0.4882  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#converting-aspatial-data-frame-into-a-sf-object",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#converting-aspatial-data-frame-into-a-sf-object",
    "title": "Hands On Exercise 8",
    "section": "Converting aspatial data frame into a sf object",
    "text": "Converting aspatial data frame into a sf object\nCurrently, the condo_resale tibble data frame is aspatial. We will convert it to a sf object. The code chunk below converts condo_resale data frame into a simple feature data frame by using st_as_sf() of sf packages.\n\ncondo_resale.sf <- st_as_sf(condo_resale,                             coords = c(\"LONGITUDE\", \"LATITUDE\"),                             crs=4326) %>%   st_transform(crs=3414)\n\nNotice that st_transform() of sf package is used to convert the coordinates from wgs84 (i.e. crs:4326) to svy21 (i.e. crs=3414).\nNext, head() is used to list the content of condo_resale.sf object.\n\nhead(condo_resale.sf)\n\nSimple feature collection with 6 features and 21 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 22085.12 ymin: 29951.54 xmax: 41042.56 ymax: 34546.2\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 6 × 22\n  POSTCODE SELLING_PRICE AREA_SQM   AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE\n     <dbl>         <dbl>    <dbl> <dbl>    <dbl>          <dbl>            <dbl>\n1   118635       3000000      309    30     7.94          0.166            2.52 \n2   288420       3880000      290    32     6.61          0.280            1.93 \n3   267833       3325000      248    33     6.90          0.429            0.502\n4   258380       4250000      127     7     4.04          0.395            1.99 \n5   467169       1400000      145    28    11.8           0.119            1.12 \n6   466472       1320000      139    22    10.3           0.125            0.789\n# ℹ 15 more variables: PROX_URA_GROWTH_AREA <dbl>, PROX_HAWKER_MARKET <dbl>,\n#   PROX_KINDERGARTEN <dbl>, PROX_MRT <dbl>, PROX_PARK <dbl>,\n#   PROX_PRIMARY_SCH <dbl>, PROX_TOP_PRIMARY_SCH <dbl>,\n#   PROX_SHOPPING_MALL <dbl>, PROX_SUPERMARKET <dbl>, PROX_BUS_STOP <dbl>,\n#   NO_Of_UNITS <dbl>, FAMILY_FRIENDLY <dbl>, FREEHOLD <dbl>,\n#   LEASEHOLD_99YR <dbl>, geometry <POINT [m]>\n\n\nNotice that the output is in point feature data frame."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#eda-using-statistical-graphics",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#eda-using-statistical-graphics",
    "title": "Hands On Exercise 8",
    "section": "EDA using statistical graphics",
    "text": "EDA using statistical graphics\nWe can plot the distribution of SELLING_PRICE by using appropriate Exploratory Data Analysis (EDA) as shown in the code chunk below.\n\nggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +   geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\n\n\n\nThe figure above reveals a right skewed distribution. This means that more condominium units were transacted at relative lower prices.\nStatistically, the skewed dsitribution can be normalised by using log transformation. The code chunk below is used to derive a new variable called LOG_SELLING_PRICE by using a log transformation on the variable SELLING_PRICE. It is performed using mutate() of dplyr package.\n\ncondo_resale.sf <- condo_resale.sf %>%   \n  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))\n\nNow, you can plot the LOG_SELLING_PRICE using the code chunk below.\n\nggplot(data=condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\n\n\n\nNotice that the distribution is relatively less skewed after the transformation."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#multiple-histogram-plots-distribution-of-variables",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#multiple-histogram-plots-distribution-of-variables",
    "title": "Hands On Exercise 8",
    "section": "Multiple Histogram Plots distribution of variables",
    "text": "Multiple Histogram Plots distribution of variables\nIn this section, you will learn how to draw a small multiple histograms (also known as trellis plot) by using ggarrange() of ggpubr package.\nThe code chunk below is used to create 12 histograms. Then, ggarrange() is used to organised these histogram into a 3 columns by 4 rows small multiple plot.\n\nAREA_SQM <- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nAGE <- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CBD <- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CHILDCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_ELDERLYCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_URA_GROWTH_AREA <- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_URA_GROWTH_AREA`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_HAWKER_MARKET <- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_KINDERGARTEN <- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_MRT <- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PARK <- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PRIMARY_SCH <- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_TOP_PRIMARY_SCH <- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, \n          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,\n          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  \n          ncol = 3, nrow = 4)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#drawing-statistical-point-map",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#drawing-statistical-point-map",
    "title": "Hands On Exercise 8",
    "section": "Drawing Statistical Point Map",
    "text": "Drawing Statistical Point Map\nLastly, we want to reveal the geospatial distribution condominium resale prices in Singapore. The map will be prepared by using tmap package.\nFirst, we will turn on the interactive mode of tmap by using the code chunk below.\n\ntmap_mode(\"view\")\ntmap_options(check.and.fix = TRUE)\n\nNext, the code chunks below is used to create an interactive point symbol map.\n\ntm_shape(mpsz_svy21)+\n  tm_polygons() +\ntm_shape(condo_resale.sf) +  \n  tm_dots(col = \"SELLING_PRICE\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\n\nNotice that tm_dots() is used instead of tm_bubbles().\nset.zoom.limits argument of tm_view() sets the minimum and maximum zoom level to 11 and 14 respectively.\nBefore moving on to the next section, the code below will be used to turn R display into plot mode.\n\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#simple-linear-regression-method",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#simple-linear-regression-method",
    "title": "Hands On Exercise 8",
    "section": "Simple Linear Regression Method**",
    "text": "Simple Linear Regression Method**\nFirst, we will build a simple linear regression model by using SELLING_PRICE as the dependent variable and AREA_SQM as the independent variable.\n\ncondo.slr <- lm(formula=SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\nlm() returns an object of class “lm” or for multiple responses of class c(“mlm”, “lm”).\nThe functions summary() and anova() can be used to obtain and print a summary and analysis of variance table of the results. The generic accessor functions coefficients, effects, fitted.values and residuals extract various useful features of the value returned by lm.\n\nsummary(condo.slr)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3695815  -391764   -87517   258900 13503875 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -258121.1    63517.2  -4.064 5.09e-05 ***\nAREA_SQM      14719.0      428.1  34.381  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 942700 on 1434 degrees of freedom\nMultiple R-squared:  0.4518,    Adjusted R-squared:  0.4515 \nF-statistic:  1182 on 1 and 1434 DF,  p-value: < 2.2e-16\n\n\nThe output report reveals that the SELLING_PRICE can be explained by using the formula:\n      *y = -258121.1 + 14719x1*\nThe R-squared of 0.4518 reveals that the simple regression model built is able to explain about 45% of the resale prices.\nSince p-value is much smaller than 0.0001, we will reject the null hypothesis that mean is a good estimator of SELLING_PRICE. This will allow us to infer that simple linear regression model above is a good estimator of SELLING_PRICE.\nThe Coefficients: section of the report reveals that the p-values of both the estimates of the Intercept and ARA_SQM are smaller than 0.001. In view of this, the null hypothesis of the B0 and B1 are equal to 0 will be rejected. As a results, we will be able to infer that the B0 and B1 are good parameter estimates.\nTo visualise the best fit curve on a scatterplot, we can incorporate lm() as a method function in ggplot’s geometry as shown in the code chunk below.\n\nggplot(data=condo_resale.sf,          aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +   geom_point() +   geom_smooth(method = lm)\n\n\n\n\nFigure above reveals that there are a few statistical outliers with relatively high selling prices."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#multiple-linear-regression-method",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#multiple-linear-regression-method",
    "title": "Hands On Exercise 8",
    "section": "Multiple Linear Regression Method**",
    "text": "Multiple Linear Regression Method**\n\nVisualising the relationships of the independent variables\nBefore building a multiple regression model, it is important to ensure that the indepdent variables used are not highly correlated to each other. If these highly correlated independent variables are used in building a regression model by mistake, the quality of the model will be compromised. This phenomenon is known as multicollinearity in statistics.\nCorrelation matrix is commonly used to visualise the relationships between the independent variables. Beside the pairs() of R, there are many packages support the display of a correlation matrix. In this section, the corrplot package will be used.\nThe code chunk below is used to plot a scatterplot matrix of the relationship between the independent variables in condo_resale data.frame.\n\ncorrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = \"AOE\",          tl.pos = \"td\", tl.cex = 0.5, method = \"number\", type = \"upper\")\n\n\n\n\nMatrix reorder is very important for mining the hiden structure and patter in the matrix. There are four methods in corrplot (parameter order), named “AOE”, “FPC”, “hclust”, “alphabet”. In the code chunk above, AOE order is used. It orders the variables by using the angular order of the eigenvectors method suggested by Michael Friendly.\nFrom the scatterplot matrix, it is clear that Freehold is highly correlated to LEASE_99YEAR. In view of this, it is wiser to only include either one of them in the subsequent model building. As a result, LEASE_99YEAR is excluded in the subsequent model building.\n\n\n13.8.3 Building a hedonic pricing model using multiple linear regression method\nThe code chunk below using lm() to calibrate the multiple linear regression model.\n\ncondo.mlr <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + \n                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + \n                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + \n                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                data=condo_resale.sf)\nsummary(condo.mlr)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + \n    PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + \n    PROX_KINDERGARTEN + PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + \n    PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3475964  -293923   -23069   241043 12260381 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)           481728.40  121441.01   3.967 7.65e-05 ***\nAREA_SQM               12708.32     369.59  34.385  < 2e-16 ***\nAGE                   -24440.82    2763.16  -8.845  < 2e-16 ***\nPROX_CBD              -78669.78    6768.97 -11.622  < 2e-16 ***\nPROX_CHILDCARE       -351617.91  109467.25  -3.212  0.00135 ** \nPROX_ELDERLYCARE      171029.42   42110.51   4.061 5.14e-05 ***\nPROX_URA_GROWTH_AREA   38474.53   12523.57   3.072  0.00217 ** \nPROX_HAWKER_MARKET     23746.10   29299.76   0.810  0.41782    \nPROX_KINDERGARTEN     147468.99   82668.87   1.784  0.07466 .  \nPROX_MRT             -314599.68   57947.44  -5.429 6.66e-08 ***\nPROX_PARK             563280.50   66551.68   8.464  < 2e-16 ***\nPROX_PRIMARY_SCH      180186.08   65237.95   2.762  0.00582 ** \nPROX_TOP_PRIMARY_SCH    2280.04   20410.43   0.112  0.91107    \nPROX_SHOPPING_MALL   -206604.06   42840.60  -4.823 1.57e-06 ***\nPROX_SUPERMARKET      -44991.80   77082.64  -0.584  0.55953    \nPROX_BUS_STOP         683121.35  138353.28   4.938 8.85e-07 ***\nNO_Of_UNITS             -231.18      89.03  -2.597  0.00951 ** \nFAMILY_FRIENDLY       140340.77   47020.55   2.985  0.00289 ** \nFREEHOLD              359913.01   49220.22   7.312 4.38e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 755800 on 1417 degrees of freedom\nMultiple R-squared:  0.6518,    Adjusted R-squared:  0.6474 \nF-statistic: 147.4 on 18 and 1417 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#preparing-publication-quality-table-olsrr-method",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#preparing-publication-quality-table-olsrr-method",
    "title": "Hands On Exercise 8",
    "section": "Preparing Publication Quality Table: olsrr method",
    "text": "Preparing Publication Quality Table: olsrr method\nWith reference to the report above, it is clear that not all the independent variables are statistically significant. We will revised the model by removing those variables which are not statistically significant.\nNow, we are ready to calibrate the revised model by using the code chunk below.\n\ncondo.mlr1 <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                   PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK + \n                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL    + PROX_BUS_STOP + \n                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,\n                 data=condo_resale.sf)\nols_regress(condo.mlr1)\n\n                                Model Summary                                 \n-----------------------------------------------------------------------------\nR                            0.807       RMSE                     751998.679 \nR-Squared                    0.651       MSE                571471422208.591 \nAdj. R-Squared               0.647       Coef. Var                    43.168 \nPred R-Squared               0.638       AIC                       42966.758 \nMAE                     414819.628       SBC                       43051.072 \n-----------------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                     ANOVA                                       \n--------------------------------------------------------------------------------\n                    Sum of                                                      \n                   Squares          DF         Mean Square       F         Sig. \n--------------------------------------------------------------------------------\nRegression    1.512586e+15          14        1.080418e+14    189.059    0.0000 \nResidual      8.120609e+14        1421    571471422208.591                      \nTotal         2.324647e+15        1435                                          \n--------------------------------------------------------------------------------\n\n                                               Parameter Estimates                                                \n-----------------------------------------------------------------------------------------------------------------\n               model           Beta    Std. Error    Std. Beta       t        Sig           lower          upper \n-----------------------------------------------------------------------------------------------------------------\n         (Intercept)     527633.222    108183.223                   4.877    0.000     315417.244     739849.200 \n            AREA_SQM      12777.523       367.479        0.584     34.771    0.000      12056.663      13498.382 \n                 AGE     -24687.739      2754.845       -0.167     -8.962    0.000     -30091.739     -19283.740 \n            PROX_CBD     -77131.323      5763.125       -0.263    -13.384    0.000     -88436.469     -65826.176 \n      PROX_CHILDCARE    -318472.751    107959.512       -0.084     -2.950    0.003    -530249.889    -106695.613 \n    PROX_ELDERLYCARE     185575.623     39901.864        0.090      4.651    0.000     107302.737     263848.510 \nPROX_URA_GROWTH_AREA      39163.254     11754.829        0.060      3.332    0.001      16104.571      62221.936 \n            PROX_MRT    -294745.107     56916.367       -0.112     -5.179    0.000    -406394.234    -183095.980 \n           PROX_PARK     570504.807     65507.029        0.150      8.709    0.000     442003.938     699005.677 \n    PROX_PRIMARY_SCH     159856.136     60234.599        0.062      2.654    0.008      41697.849     278014.424 \n  PROX_SHOPPING_MALL    -220947.251     36561.832       -0.115     -6.043    0.000    -292668.213    -149226.288 \n       PROX_BUS_STOP     682482.221    134513.243        0.134      5.074    0.000     418616.359     946348.082 \n         NO_Of_UNITS       -245.480        87.947       -0.053     -2.791    0.005       -418.000        -72.961 \n     FAMILY_FRIENDLY     146307.576     46893.021        0.057      3.120    0.002      54320.593     238294.560 \n            FREEHOLD     350599.812     48506.485        0.136      7.228    0.000     255447.802     445751.821 \n-----------------------------------------------------------------------------------------------------------------"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#preparing-publication-quality-table-gtsummary-method",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#preparing-publication-quality-table-gtsummary-method",
    "title": "Hands On Exercise 8",
    "section": "Preparing Publication Quality Table: gtsummary method",
    "text": "Preparing Publication Quality Table: gtsummary method\nThe gtsummary package provides an elegant and flexible way to create publication-ready summary tables in R.\nIn the code chunk below, tbl_regression() is used to create a well formatted regression report.\n\ntbl_regression(condo.mlr1, intercept = TRUE)\n\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n527,633\n315,417, 739,849\n<0.001\n    AREA_SQM\n12,778\n12,057, 13,498\n<0.001\n    AGE\n-24,688\n-30,092, -19,284\n<0.001\n    PROX_CBD\n-77,131\n-88,436, -65,826\n<0.001\n    PROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n    PROX_ELDERLYCARE\n185,576\n107,303, 263,849\n<0.001\n    PROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n<0.001\n    PROX_MRT\n-294,745\n-406,394, -183,096\n<0.001\n    PROX_PARK\n570,505\n442,004, 699,006\n<0.001\n    PROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n    PROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n<0.001\n    PROX_BUS_STOP\n682,482\n418,616, 946,348\n<0.001\n    NO_Of_UNITS\n-245\n-418, -73\n0.005\n    FAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n    FREEHOLD\n350,600\n255,448, 445,752\n<0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nWith gtsummary package, model statistics can be included in the report by either appending them to the report table by using add_glance_table() or adding as a table source note by using add_glance_source_note() as shown in the code chunk below.\n\ntbl_regression(condo.mlr1, \n               intercept = TRUE) %>% \n  add_glance_source_note(\n    label = list(sigma ~ \"\\U03C3\"),\n    include = c(r.squared, adj.r.squared, \n                AIC, statistic,\n                p.value, sigma))\n\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n527,633\n315,417, 739,849\n<0.001\n    AREA_SQM\n12,778\n12,057, 13,498\n<0.001\n    AGE\n-24,688\n-30,092, -19,284\n<0.001\n    PROX_CBD\n-77,131\n-88,436, -65,826\n<0.001\n    PROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n    PROX_ELDERLYCARE\n185,576\n107,303, 263,849\n<0.001\n    PROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n<0.001\n    PROX_MRT\n-294,745\n-406,394, -183,096\n<0.001\n    PROX_PARK\n570,505\n442,004, 699,006\n<0.001\n    PROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n    PROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n<0.001\n    PROX_BUS_STOP\n682,482\n418,616, 946,348\n<0.001\n    NO_Of_UNITS\n-245\n-418, -73\n0.005\n    FAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n    FREEHOLD\n350,600\n255,448, 445,752\n<0.001\n  \n  \n    \n      R² = 0.651; Adjusted R² = 0.647; AIC = 42,967; Statistic = 189; p-value = <0.001; σ = 755,957\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nFor more customisation options, refer to Tutorial: tbl_regression\n\nChecking for multicolinearity\nIn this section, we would like to introduce you a fantastic R package specially programmed for performing OLS regression. It is called olsrr. It provides a collection of very useful methods for building better multiple linear regression models:\n\ncomprehensive regression output\nresidual diagnostics\nmeasures of influence\nheteroskedasticity tests\ncollinearity diagnostics\nmodel fit assessment\nvariable contribution assessment\nvariable selection procedures\n\nIn the code chunk below, the ols_vif_tol() of olsrr package is used to test if there are sign of multicollinearity.\n\nols_vif_tol(condo.mlr1)\n\n              Variables Tolerance      VIF\n1              AREA_SQM 0.8728554 1.145665\n2                   AGE 0.7071275 1.414172\n3              PROX_CBD 0.6356147 1.573280\n4        PROX_CHILDCARE 0.3066019 3.261559\n5      PROX_ELDERLYCARE 0.6598479 1.515501\n6  PROX_URA_GROWTH_AREA 0.7510311 1.331503\n7              PROX_MRT 0.5236090 1.909822\n8             PROX_PARK 0.8279261 1.207837\n9      PROX_PRIMARY_SCH 0.4524628 2.210126\n10   PROX_SHOPPING_MALL 0.6738795 1.483945\n11        PROX_BUS_STOP 0.3514118 2.845664\n12          NO_Of_UNITS 0.6901036 1.449058\n13      FAMILY_FRIENDLY 0.7244157 1.380423\n14             FREEHOLD 0.6931163 1.442759\n\n\nSince the VIF of the independent variables are less than 10. We can safely conclude that there are no sign of multicollinearity among the independent variables.\n\n\nTest for Non-Linearity\nIn multiple linear regression, it is important for us to test the assumption that linearity and additivity of the relationship between dependent and independent variables.\nIn the code chunk below, the ols_plot_resid_fit() of olsrr package is used to perform linearity assumption test.\n\nols_plot_resid_fit(condo.mlr1)\n\n\n\n\nThe figure above reveals that most of the data poitns are scattered around the 0 line, hence we can safely conclude that the relationships between the dependent variable and independent variables are linear.\n\n\nTest for Normality Assumption\nLastly, the code chunk below uses ols_plot_resid_hist() of olsrr package to perform normality assumption test.\n\nols_plot_resid_hist(condo.mlr1)\n\n\n\n\nThe figure reveals that the residual of the multiple linear regression model (i.e. condo.mlr1) is resemble normal distribution.\nIf you prefer formal statistical test methods, the ols_test_normality() of olsrr package can be used as shown in the code chun below.\n\nols_test_normality(condo.mlr1)\n\n-----------------------------------------------\n       Test             Statistic       pvalue  \n-----------------------------------------------\nShapiro-Wilk              0.6856         0.0000 \nKolmogorov-Smirnov        0.1366         0.0000 \nCramer-von Mises         121.0768        0.0000 \nAnderson-Darling         67.9551         0.0000 \n-----------------------------------------------\n\n\nThe summary table above reveals that the p-values of the four tests are way smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is statistical evidence that the residual are not normally distributed.\n\n\nTesting for Spatial Autocorrelation\nThe hedonic model we try to build are using geographically referenced attributes, hence it is also important for us to visual the residual of the hedonic pricing model.\nIn order to perform spatial autocorrelation test, we need to convert condo_resale.sf from sf data frame into a SpatialPointsDataFrame.\nFirst, we will export the residual of the hedonic pricing model and save it as a data frame.\n\nmlr.output <- as.data.frame(condo.mlr1$residuals)\n\nNext, we will join the newly created data frame with condo_resale.sf object.\n\ncondo_resale.res.sf <- cbind(condo_resale.sf,                          condo.mlr1$residuals) %>% rename(`MLR_RES` = `condo.mlr1.residuals`)\n\nNext, we will convert condo_resale.res.sf from simple feature object into a SpatialPointsDataFrame because spdep package can only process sp conformed spatial data objects.\nThe code chunk below will be used to perform the data conversion process.\n\ncondo_resale.sp <- as_Spatial(condo_resale.res.sf) \ncondo_resale.sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1436 \nextent      : 14940.85, 43352.45, 24765.67, 48382.81  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 23\nnames       : POSTCODE, SELLING_PRICE, AREA_SQM, AGE,    PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN,    PROX_MRT,   PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH, PROX_SHOPPING_MALL, ... \nmin values  :    18965,        540000,       34,   0, 0.386916393,    0.004927023,      0.054508623,          0.214539508,        0.051817113,       0.004927023, 0.052779424, 0.029064164,      0.077106132,          0.077106132,                  0, ... \nmax values  :   828833,       1.8e+07,      619,  37, 19.18042832,     3.46572633,      3.949157205,           9.15540001,        5.374348075,       2.229045366,  3.48037319,  2.16104919,      3.928989144,          6.748192062,        3.477433767, ... \n\n\nThe summary table above reveals that the p-values of the four tests are way smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is statistical evidence that the residual are not normally distributed.\n\n13.8.5.4 Testing for Spatial Autocorrelation\nThe hedonic model we try to build are using geographically referenced attributes, hence it is also important for us to visual the residual of the hedonic pricing model.\nIn order to perform spatial autocorrelation test, we need to convert condo_resale.sf from sf data frame into a SpatialPointsDataFrame.\nFirst, we will export the residual of the hedonic pricing model and save it as a data frame.\n\nmlr.output <- as.data.frame(condo.mlr1$residuals)\n\nNext, we will join the newly created data frame with condo_resale.sf object.\n\ncondo_resale.res.sf <- cbind(condo_resale.sf,                          condo.mlr1$residuals) %>% rename(`MLR_RES` = `condo.mlr1.residuals`)\n\nNext, we will convert condo_resale.res.sf from simple feature object into a SpatialPointsDataFrame because spdep package can only process sp conformed spatial data objects.\nThe code chunk below will be used to perform the data conversion process.\n\ncondo_resale.sp <- as_Spatial(condo_resale.res.sf) \ncondo_resale.sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1436 \nextent      : 14940.85, 43352.45, 24765.67, 48382.81  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 23\nnames       : POSTCODE, SELLING_PRICE, AREA_SQM, AGE,    PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN,    PROX_MRT,   PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH, PROX_SHOPPING_MALL, ... \nmin values  :    18965,        540000,       34,   0, 0.386916393,    0.004927023,      0.054508623,          0.214539508,        0.051817113,       0.004927023, 0.052779424, 0.029064164,      0.077106132,          0.077106132,                  0, ... \nmax values  :   828833,       1.8e+07,      619,  37, 19.18042832,     3.46572633,      3.949157205,           9.15540001,        5.374348075,       2.229045366,  3.48037319,  2.16104919,      3.928989144,          6.748192062,        3.477433767, ... \n\n\nNext, we will use tmap package to display the distribution of the residuals on an interactive map.\nThe code churn below will turn on the interactive mode of tmap.\n\ntmap_mode(\"view\")\n\nThe code chunks below is used to create an interactive point symbol map.\n\ntm_shape(mpsz_svy21)+\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale.res.sf) +  \n  tm_dots(col = \"MLR_RES\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\n\nRemember to switch back to “plot” mode before continue.\n\ntmap_mode(\"plot\")\n\nThe figure above reveal that there is sign of spatial autocorrelation.\nTo proof that our observation is indeed true, the Moran’s I test will be performed\nFirst, we will compute the distance-based weight matrix by using dnearneigh() function of spdep.\n\nnb <- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE)\nsummary(nb)\n\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \n10 disjoint connected subgraphs\nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\n\nNext, nb2listw() of spdep packge will be used to convert the output neighbours lists (i.e. nb) into a spatial weights.\n\nnb_lw <- nb2listw(nb, style = 'W') \nsummary(nb_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \n10 disjoint connected subgraphs\nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\nWeights style: W \nWeights constants summary:\n     n      nn   S0       S1       S2\nW 1436 2062096 1436 94.81916 5798.341\n\n\nNext, lm.morantest() of spdep package will be used to perform Moran’s I test for residual spatial autocorrelation\n\nlm.morantest(condo.mlr1, nb_lw)\n\n\n    Global Moran I for regression residuals\n\ndata:  \nmodel: lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD +\nPROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_MRT +\nPROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP +\nNO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data = condo_resale.sf)\nweights: nb_lw\n\nMoran I statistic standard deviate = 24.366, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nObserved Moran I      Expectation         Variance \n    1.438876e-01    -5.487594e-03     3.758259e-05 \n\n\nThe Global Moran’s I test for residual spatial autocorrelation shows that it’s p-value is less than 0.00000000000000022 which is less than the alpha value of 0.05. Hence, we will reject the null hypothesis that the residuals are randomly distributed.\nSince the Observed Global Moran I = 0.1424418 which is greater than 0, we can infer than the residuals resemble cluster distribution."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#building-fixed-bandwidth-gwr-model",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#building-fixed-bandwidth-gwr-model",
    "title": "Hands On Exercise 8",
    "section": "Building Fixed Bandwidth GWR Model",
    "text": "Building Fixed Bandwidth GWR Model\n\nComputing fixed bandwith\nIn the code chunk below bw.gwr() of GWModel package is used to determine the optimal fixed bandwidth to use in the model. Notice that the argument adaptive is set to FALSE indicates that we are interested to compute the fixed bandwidth.\nThere are two possible approaches can be uused to determine the stopping rule, they are: CV cross-validation approach and AIC corrected (AICc) approach. We define the stopping rule using approach argeement.{r\n\nbw.fixed <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                     FAMILY_FRIENDLY + FREEHOLD, \n                   data=condo_resale.sp, \n                   approach=\"CV\", \n                   kernel=\"gaussian\", \n                   adaptive=FALSE, \n                   longlat=FALSE)\n\nFixed bandwidth: 17660.96 CV score: 8.259118e+14 \nFixed bandwidth: 10917.26 CV score: 7.970454e+14 \nFixed bandwidth: 6749.419 CV score: 7.273273e+14 \nFixed bandwidth: 4173.553 CV score: 6.300006e+14 \nFixed bandwidth: 2581.58 CV score: 5.404958e+14 \nFixed bandwidth: 1597.687 CV score: 4.857515e+14 \nFixed bandwidth: 989.6077 CV score: 4.722431e+14 \nFixed bandwidth: 613.7939 CV score: 1.378294e+16 \nFixed bandwidth: 1221.873 CV score: 4.778717e+14 \nFixed bandwidth: 846.0596 CV score: 4.791629e+14 \nFixed bandwidth: 1078.325 CV score: 4.751406e+14 \nFixed bandwidth: 934.7772 CV score: 4.72518e+14 \nFixed bandwidth: 1023.495 CV score: 4.730305e+14 \nFixed bandwidth: 968.6643 CV score: 4.721317e+14 \nFixed bandwidth: 955.7206 CV score: 4.722072e+14 \nFixed bandwidth: 976.6639 CV score: 4.721387e+14 \nFixed bandwidth: 963.7202 CV score: 4.721484e+14 \nFixed bandwidth: 971.7199 CV score: 4.721293e+14 \nFixed bandwidth: 973.6083 CV score: 4.721309e+14 \nFixed bandwidth: 970.5527 CV score: 4.721295e+14 \nFixed bandwidth: 972.4412 CV score: 4.721296e+14 \nFixed bandwidth: 971.2741 CV score: 4.721292e+14 \nFixed bandwidth: 970.9985 CV score: 4.721293e+14 \nFixed bandwidth: 971.4443 CV score: 4.721292e+14 \nFixed bandwidth: 971.5496 CV score: 4.721293e+14 \nFixed bandwidth: 971.3793 CV score: 4.721292e+14 \nFixed bandwidth: 971.3391 CV score: 4.721292e+14 \nFixed bandwidth: 971.3143 CV score: 4.721292e+14 \nFixed bandwidth: 971.3545 CV score: 4.721292e+14 \nFixed bandwidth: 971.3296 CV score: 4.721292e+14 \nFixed bandwidth: 971.345 CV score: 4.721292e+14 \nFixed bandwidth: 971.3355 CV score: 4.721292e+14 \nFixed bandwidth: 971.3413 CV score: 4.721292e+14 \nFixed bandwidth: 971.3377 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3408 CV score: 4.721292e+14 \nFixed bandwidth: 971.3403 CV score: 4.721292e+14 \nFixed bandwidth: 971.3406 CV score: 4.721292e+14 \nFixed bandwidth: 971.3404 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \n\n\n\n\nGWModel method - fixed bandwith\nNow we can use the code chunk below to calibrate the gwr model using fixed bandwidth and gaussian kernel.\n\ngwr.fixed <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                         PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                         PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                         PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                         FAMILY_FRIENDLY + FREEHOLD, \n                       data=condo_resale.sp, \n                       bw=bw.fixed, \n                       kernel = 'gaussian', \n                       longlat = FALSE)\n\nThe output is saved in a list of class “gwrm”. The code below can be used to display the model output.\n\ngwr.fixed\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-03-11 08:49:45.000856 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.fixed, kernel = \"gaussian\", \n    longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  < 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  < 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  < 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  < 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: < 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 971.3405 \n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -3.5988e+07 -5.1998e+05  7.6780e+05  1.7412e+06\n   AREA_SQM              1.0003e+03  5.2758e+03  7.4740e+03  1.2301e+04\n   AGE                  -1.3475e+05 -2.0813e+04 -8.6260e+03 -3.7784e+03\n   PROX_CBD             -7.7047e+07 -2.3608e+05 -8.3600e+04  3.4646e+04\n   PROX_CHILDCARE       -6.0097e+06 -3.3667e+05 -9.7425e+04  2.9007e+05\n   PROX_ELDERLYCARE     -3.5000e+06 -1.5970e+05  3.1971e+04  1.9577e+05\n   PROX_URA_GROWTH_AREA -3.0170e+06 -8.2013e+04  7.0749e+04  2.2612e+05\n   PROX_MRT             -3.5282e+06 -6.5836e+05 -1.8833e+05  3.6922e+04\n   PROX_PARK            -1.2062e+06 -2.1732e+05  3.5383e+04  4.1335e+05\n   PROX_PRIMARY_SCH     -2.2695e+07 -1.7066e+05  4.8472e+04  5.1555e+05\n   PROX_SHOPPING_MALL   -7.2585e+06 -1.6684e+05 -1.0517e+04  1.5923e+05\n   PROX_BUS_STOP        -1.4676e+06 -4.5207e+04  3.7601e+05  1.1664e+06\n   NO_Of_UNITS          -1.3170e+03 -2.4822e+02 -3.0846e+01  2.5496e+02\n   FAMILY_FRIENDLY      -2.2749e+06 -1.1140e+05  7.6214e+03  1.6107e+05\n   FREEHOLD             -9.2067e+06  3.8073e+04  1.5169e+05  3.7528e+05\n                             Max.\n   Intercept            112793548\n   AREA_SQM                 21575\n   AGE                     434201\n   PROX_CBD               2704596\n   PROX_CHILDCARE         1654087\n   PROX_ELDERLYCARE      38867814\n   PROX_URA_GROWTH_AREA  78515730\n   PROX_MRT               3124316\n   PROX_PARK             18122425\n   PROX_PRIMARY_SCH       4637503\n   PROX_SHOPPING_MALL     1529952\n   PROX_BUS_STOP         11342182\n   NO_Of_UNITS              12907\n   FAMILY_FRIENDLY        1720744\n   FREEHOLD               6073636\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 438.3804 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 997.6196 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 42263.61 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41632.36 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 42515.71 \n   Residual sum of squares: 2.53407e+14 \n   R-square value:  0.8909912 \n   Adjusted R-square value:  0.8430417 \n\n   ***********************************************************************\n   Program stops at: 2024-03-11 08:49:45.773007 \n\n\nThe report shows that the AICc of the gwr is 42263.61 which is significantly smaller than the globel multiple linear regression model of 42967.1."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#building-adaptive-bandwidth-gwr-model",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#building-adaptive-bandwidth-gwr-model",
    "title": "Hands On Exercise 8",
    "section": "Building Adaptive Bandwidth GWR Model",
    "text": "Building Adaptive Bandwidth GWR Model\nIn this section, we will calibrate the gwr-based hedonic pricing model by using adaptive bandwidth approach.\n\n13.9.2.1 Computing the adaptive bandwidth\nSimilar to the earlier section, we will first use bw.gwr() to determine the recommended data point to use.\nThe code chunk used look very similar to the one used to compute the fixed bandwidth except the adaptive argument has changed to TRUE.\n\nbw.adaptive <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE  + \n                        PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE    + \n                        PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                        PROX_PRIMARY_SCH + PROX_SHOPPING_MALL   + PROX_BUS_STOP + \n                        NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                      data=condo_resale.sp, \n                      approach=\"CV\", \n                      kernel=\"gaussian\", \n                      adaptive=TRUE, \n                      longlat=FALSE)\n\nAdaptive bandwidth: 895 CV score: 7.952401e+14 \nAdaptive bandwidth: 561 CV score: 7.667364e+14 \nAdaptive bandwidth: 354 CV score: 6.953454e+14 \nAdaptive bandwidth: 226 CV score: 6.15223e+14 \nAdaptive bandwidth: 147 CV score: 5.674373e+14 \nAdaptive bandwidth: 98 CV score: 5.426745e+14 \nAdaptive bandwidth: 68 CV score: 5.168117e+14 \nAdaptive bandwidth: 49 CV score: 4.859631e+14 \nAdaptive bandwidth: 37 CV score: 4.646518e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \nAdaptive bandwidth: 25 CV score: 4.430816e+14 \nAdaptive bandwidth: 32 CV score: 4.505602e+14 \nAdaptive bandwidth: 27 CV score: 4.462172e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \n\n\nThe code below can be used to display the model output.\n\n\nConstructing the adaptive bandwidth gwr model\n\ngwr.adaptive <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                            PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + \n                            PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                            PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + \n                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                          data=condo_resale.sp, bw=bw.adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE, \n                          longlat = FALSE)\n\n\ngwr.adaptive\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-03-11 08:49:51.655353 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.adaptive, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  < 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  < 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  < 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  < 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: < 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 30 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -1.3487e+08 -2.4669e+05  7.7928e+05  1.6194e+06\n   AREA_SQM              3.3188e+03  5.6285e+03  7.7825e+03  1.2738e+04\n   AGE                  -9.6746e+04 -2.9288e+04 -1.4043e+04 -5.6119e+03\n   PROX_CBD             -2.5330e+06 -1.6256e+05 -7.7242e+04  2.6624e+03\n   PROX_CHILDCARE       -1.2790e+06 -2.0175e+05  8.7158e+03  3.7778e+05\n   PROX_ELDERLYCARE     -1.6212e+06 -9.2050e+04  6.1029e+04  2.8184e+05\n   PROX_URA_GROWTH_AREA -7.2686e+06 -3.0350e+04  4.5869e+04  2.4613e+05\n   PROX_MRT             -4.3781e+07 -6.7282e+05 -2.2115e+05 -7.4593e+04\n   PROX_PARK            -2.9020e+06 -1.6782e+05  1.1601e+05  4.6572e+05\n   PROX_PRIMARY_SCH     -8.6418e+05 -1.6627e+05 -7.7853e+03  4.3222e+05\n   PROX_SHOPPING_MALL   -1.8272e+06 -1.3175e+05 -1.4049e+04  1.3799e+05\n   PROX_BUS_STOP        -2.0579e+06 -7.1461e+04  4.1104e+05  1.2071e+06\n   NO_Of_UNITS          -2.1993e+03 -2.3685e+02 -3.4699e+01  1.1657e+02\n   FAMILY_FRIENDLY      -5.9879e+05 -5.0927e+04  2.6173e+04  2.2481e+05\n   FREEHOLD             -1.6340e+05  4.0765e+04  1.9023e+05  3.7960e+05\n                            Max.\n   Intercept            18758355\n   AREA_SQM                23064\n   AGE                     13303\n   PROX_CBD             11346650\n   PROX_CHILDCARE        2892127\n   PROX_ELDERLYCARE      2465671\n   PROX_URA_GROWTH_AREA  7384059\n   PROX_MRT              1186242\n   PROX_PARK             2588497\n   PROX_PRIMARY_SCH      3381462\n   PROX_SHOPPING_MALL   38038564\n   PROX_BUS_STOP        12081592\n   NO_Of_UNITS              1010\n   FAMILY_FRIENDLY       2072414\n   FREEHOLD              1813995\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 350.3088 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 1085.691 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 41982.22 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41546.74 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 41914.08 \n   Residual sum of squares: 2.528227e+14 \n   R-square value:  0.8912425 \n   Adjusted R-square value:  0.8561185 \n\n   ***********************************************************************\n   Program stops at: 2024-03-11 08:49:52.504807 \n\n\nThe report shows that the AICc the adaptive distance gwr is 41982.22 which is even smaller than the AICc of the fixed distance gwr of 42263.61."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#visualising-gwr-output",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#visualising-gwr-output",
    "title": "Hands On Exercise 8",
    "section": "Visualising GWR Output",
    "text": "Visualising GWR Output\nIn addition to regression residuals, the output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, and explanatory variable coefficients and standard errors:\n\nCondition Number: this diagnostic evaluates local collinearity. In the presence of strong local collinearity, results become unstable. Results associated with condition numbers larger than 30, may be unreliable.\nLocal R2: these values range between 0.0 and 1.0 and indicate how well the local regression model fits observed y values. Very low values indicate the local model is performing poorly. Mapping the Local R2 values to see where GWR predicts well and where it predicts poorly may provide clues about important variables that may be missing from the regression model.\nPredicted: these are the estimated (or fitted) y values 3. computed by GWR.\nResiduals: to obtain the residual values, the fitted y values are subtracted from the observed y values. Standardized residuals have a mean of zero and a standard deviation of 1. A cold-to-hot rendered map of standardized residuals can be produce by using these values.\nCoefficient Standard Error: these values measure the reliability of each coefficient estimate. Confidence in those estimates are higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.\n\nThey are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its “data” slot in an object called SDF of the output list."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#converting-sdf-into-sf-data.frame",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#converting-sdf-into-sf-data.frame",
    "title": "Hands On Exercise 8",
    "section": "Converting SDF into sf data.frame",
    "text": "Converting SDF into sf data.frame\nTo visualise the fields in SDF, we need to first covert it into sf data.frame by using the code chunk below.\n\ncondo_resale.sf.adaptive <- st_as_sf(gwr.adaptive$SDF) %>%   st_transform(crs=3414)\n\n\ncondo_resale.sf.adaptive.svy21 <- st_transform(condo_resale.sf.adaptive, 3414) \ncondo_resale.sf.adaptive.svy21  \n\nSimple feature collection with 1436 features and 51 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 14940.85 ymin: 24765.67 xmax: 43352.45 ymax: 48382.81\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n    Intercept  AREA_SQM        AGE  PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE\n1   2050011.7  9561.892  -9514.634 -120681.9      319266.92       -393417.79\n2   1633128.2 16576.853 -58185.479 -149434.2      441102.18        325188.74\n3   3433608.2 13091.861 -26707.386 -259397.8     -120116.82        535855.81\n4    234358.9 20730.601 -93308.988 2426853.7      480825.28        314783.72\n5   2285804.9  6722.836 -17608.018 -316835.5       90764.78       -137384.61\n6  -3568877.4  6039.581 -26535.592  327306.1     -152531.19       -700392.85\n7  -2874842.4 16843.575 -59166.727 -983577.2     -177810.50       -122384.02\n8   2038086.0  6905.135 -17681.897 -285076.6       70259.40        -96012.78\n9   1718478.4  9580.703 -14401.128  105803.4     -657698.02       -123276.00\n10  3457054.0 14072.011 -31579.884 -234895.4       79961.45        548581.04\n   PROX_URA_GROWTH_AREA    PROX_MRT  PROX_PARK PROX_PRIMARY_SCH\n1            -159980.20  -299742.96 -172104.47        242668.03\n2            -142290.39 -2510522.23  523379.72       1106830.66\n3            -253621.21  -936853.28  209099.85        571462.33\n4           -2679297.89 -2039479.50 -759153.26       3127477.21\n5             303714.81   -44567.05  -10284.62         30413.56\n6             -28051.25   733566.47 1511488.92        320878.23\n7            1397676.38 -2745430.34  710114.74       1786570.95\n8             269368.71   -14552.99   73533.34         53359.73\n9            -361974.72  -476785.32 -132067.59        -40128.92\n10           -150024.38 -1503835.53  574155.47        108996.67\n   PROX_SHOPPING_MALL PROX_BUS_STOP  NO_Of_UNITS FAMILY_FRIENDLY  FREEHOLD\n1          300881.390     1210615.4  104.8290640       -9075.370  303955.6\n2          -87693.378     1843587.2 -288.3441183      310074.664  396221.3\n3         -126732.712     1411924.9   -9.5532945        5949.746  168821.7\n4          -29593.342     7225577.5 -161.3551620     1556178.531 1212515.6\n5           -7490.586      677577.0   42.2659674       58986.951  328175.2\n6          258583.881     1086012.6 -214.3671271      201992.641  471873.1\n7         -384251.210     5094060.5   -0.9212521      359659.512  408871.9\n8          -39634.902      735767.1   30.1741069       55602.506  347075.0\n9          276718.757     2815772.4  675.1615559      -30453.297  503872.8\n10        -454726.822     2123557.0  -21.3044311     -100935.586  213324.6\n         y    yhat    residual CV_Score Stud_residual Intercept_SE AREA_SQM_SE\n1  3000000 2886532   113468.16        0    0.38207013     516105.5    823.2860\n2  3880000 3466801   413198.52        0    1.01433140     488083.5    825.2380\n3  3325000 3616527  -291527.20        0   -0.83780678     963711.4    988.2240\n4  4250000 5435482 -1185481.63        0   -2.84614670     444185.5    617.4007\n5  1400000 1388166    11834.26        0    0.03404453    2119620.6   1376.2778\n6  1320000 1516702  -196701.94        0   -0.72065800   28572883.7   2348.0091\n7  3410000 3266881   143118.77        0    0.41291992     679546.6    893.5893\n8  1420000 1431955   -11955.27        0   -0.03033109    2217773.1   1415.2604\n9  2025000 1832799   192200.83        0    0.52018109     814281.8    943.8434\n10 2550000 2223364   326635.53        0    1.10559735    2410252.0   1271.4073\n      AGE_SE PROX_CBD_SE PROX_CHILDCARE_SE PROX_ELDERLYCARE_SE\n1   5889.782    37411.22          319111.1           120633.34\n2   6226.916    23615.06          299705.3            84546.69\n3   6510.236    56103.77          349128.5           129687.07\n4   6010.511   469337.41          304965.2           127150.69\n5   8180.361   410644.47          698720.6           327371.55\n6  14601.909  5272846.47         1141599.8          1653002.19\n7   8970.629   346164.20          530101.1           148598.71\n8   8661.309   438035.69          742532.8           399221.05\n9  11791.208    89148.35          704630.7           329683.30\n10  9941.980   173532.77          500976.2           281876.74\n   PROX_URA_GROWTH_AREA_SE PROX_MRT_SE PROX_PARK_SE PROX_PRIMARY_SCH_SE\n1                 56207.39    185181.3     205499.6            152400.7\n2                 76956.50    281133.9     229358.7            165150.7\n3                 95774.60    275483.7     314124.3            196662.6\n4                470762.12    279877.1     227249.4            240878.9\n5                474339.56    363830.0     364580.9            249087.7\n6               5496627.21    730453.2    1741712.0            683265.5\n7                371692.97    375511.9     297400.9            344602.8\n8                517977.91    423155.4     440984.4            261251.2\n9                153436.22    285325.4     304998.4            278258.5\n10               239182.57    571355.7     599131.8            331284.8\n   PROX_SHOPPING_MALL_SE PROX_BUS_STOP_SE NO_Of_UNITS_SE FAMILY_FRIENDLY_SE\n1               109268.8         600668.6       218.1258           131474.7\n2                98906.8         410222.1       208.9410           114989.1\n3               119913.3         464156.7       210.9828           146607.2\n4               177104.1         562810.8       361.7767           108726.6\n5               301032.9         740922.4       299.5034           160663.7\n6              2931208.6        1418333.3       602.5571           331727.0\n7               249969.5         821236.4       532.1978           129241.2\n8               351634.0         775038.4       338.6777           171895.1\n9               289872.7         850095.5       439.9037           220223.4\n10              265529.7         631399.2       259.0169           189125.5\n   FREEHOLD_SE Intercept_TV AREA_SQM_TV     AGE_TV PROX_CBD_TV\n1     115954.0    3.9720784   11.614302  -1.615447 -3.22582173\n2     130110.0    3.3460017   20.087361  -9.344188 -6.32792021\n3     141031.5    3.5629010   13.247868  -4.102368 -4.62353528\n4     138239.1    0.5276150   33.577223 -15.524302  5.17080808\n5     210641.1    1.0784029    4.884795  -2.152474 -0.77155660\n6     374347.3   -0.1249043    2.572214  -1.817269  0.06207388\n7     182216.9   -4.2305303   18.849348  -6.595605 -2.84136028\n8     216649.4    0.9189786    4.879056  -2.041481 -0.65080678\n9     220473.7    2.1104224   10.150733  -1.221345  1.18682383\n10    206346.2    1.4343123   11.068059  -3.176418 -1.35360852\n   PROX_CHILDCARE_TV PROX_ELDERLYCARE_TV PROX_URA_GROWTH_AREA_TV PROX_MRT_TV\n1         1.00048819          -3.2612693            -2.846248368 -1.61864578\n2         1.47178634           3.8462625            -1.848971738 -8.92998600\n3        -0.34404755           4.1319138            -2.648105057 -3.40075727\n4         1.57665606           2.4756745            -5.691404992 -7.28705261\n5         0.12990138          -0.4196596             0.640289855 -0.12249416\n6        -0.13361179          -0.4237096            -0.005103357  1.00426206\n7        -0.33542751          -0.8235874             3.760298131 -7.31116712\n8         0.09462126          -0.2405003             0.520038994 -0.03439159\n9        -0.93339393          -0.3739225            -2.359121712 -1.67102293\n10        0.15961128           1.9461735            -0.627237944 -2.63204802\n   PROX_PARK_TV PROX_PRIMARY_SCH_TV PROX_SHOPPING_MALL_TV PROX_BUS_STOP_TV\n1   -0.83749312           1.5923022            2.75358842        2.0154464\n2    2.28192684           6.7019454           -0.88662640        4.4941192\n3    0.66565951           2.9058009           -1.05686949        3.0419145\n4   -3.34061770          12.9836105           -0.16709578       12.8383775\n5   -0.02820944           0.1220998           -0.02488294        0.9145046\n6    0.86781794           0.4696245            0.08821750        0.7656963\n7    2.38773567           5.1844351           -1.53719231        6.2029165\n8    0.16674816           0.2042469           -0.11271635        0.9493299\n9   -0.43301073          -0.1442145            0.95462153        3.3123012\n10   0.95831249           0.3290120           -1.71252687        3.3632555\n   NO_Of_UNITS_TV FAMILY_FRIENDLY_TV FREEHOLD_TV  Local_R2\n1     0.480589953        -0.06902748    2.621347 0.8846744\n2    -1.380026395         2.69655779    3.045280 0.8899773\n3    -0.045279967         0.04058290    1.197050 0.8947007\n4    -0.446007570        14.31276425    8.771149 0.9073605\n5     0.141120178         0.36714544    1.557983 0.9510057\n6    -0.355762335         0.60891234    1.260522 0.9247586\n7    -0.001731033         2.78285441    2.243875 0.8310458\n8     0.089093858         0.32346758    1.602012 0.9463936\n9     1.534793921        -0.13828365    2.285410 0.8380365\n10   -0.082251138        -0.53369623    1.033819 0.9080753\n                    geometry\n1  POINT (22085.12 29951.54)\n2   POINT (25656.84 34546.2)\n3   POINT (23963.99 32890.8)\n4  POINT (27044.28 32319.77)\n5  POINT (41042.56 33743.64)\n6   POINT (39717.04 32943.1)\n7   POINT (28419.1 33513.37)\n8  POINT (40763.57 33879.61)\n9  POINT (23595.63 28884.78)\n10 POINT (24586.56 33194.31)\n\n\n\ngwr.adaptive.output <- as.data.frame(gwr.adaptive$SDF) \ncondo_resale.sf.adaptive <- cbind(condo_resale.res.sf, \n                                  as.matrix(gwr.adaptive.output))\n\nNext, glimpse() is used to display the content of condo_resale.sf.adaptive sf data frame.\n\nglimpse(condo_resale.sf.adaptive)\n\nRows: 1,436\nColumns: 77\n$ POSTCODE                <dbl> 118635, 288420, 267833, 258380, 467169, 466472…\n$ SELLING_PRICE           <dbl> 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ AREA_SQM                <dbl> 309, 290, 248, 127, 145, 139, 218, 141, 165, 1…\n$ AGE                     <dbl> 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22,…\n$ PROX_CBD                <dbl> 7.941259, 6.609797, 6.898000, 4.038861, 11.783…\n$ PROX_CHILDCARE          <dbl> 0.16597932, 0.28027246, 0.42922669, 0.39473543…\n$ PROX_ELDERLYCARE        <dbl> 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.…\n$ PROX_URA_GROWTH_AREA    <dbl> 6.618741, 7.505109, 6.463887, 4.906512, 6.4106…\n$ PROX_HAWKER_MARKET      <dbl> 1.76542207, 0.54507614, 0.37789301, 1.68259969…\n$ PROX_KINDERGARTEN       <dbl> 0.05835552, 0.61592412, 0.14120309, 0.38200076…\n$ PROX_MRT                <dbl> 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.…\n$ PROX_PARK               <dbl> 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.…\n$ PROX_PRIMARY_SCH        <dbl> 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.…\n$ PROX_TOP_PRIMARY_SCH    <dbl> 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.…\n$ PROX_SHOPPING_MALL      <dbl> 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.…\n$ PROX_SUPERMARKET        <dbl> 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.…\n$ PROX_BUS_STOP           <dbl> 0.10336166, 0.28673408, 0.28504777, 0.29872340…\n$ NO_Of_UNITS             <dbl> 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34…\n$ FAMILY_FRIENDLY         <dbl> 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD                <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ LOG_SELLING_PRICE       <dbl> 14.91412, 15.17135, 15.01698, 15.26243, 14.151…\n$ MLR_RES                 <dbl> -1489099.55, 415494.57, 194129.69, 1088992.71,…\n$ Intercept               <dbl> 2050011.67, 1633128.24, 3433608.17, 234358.91,…\n$ AREA_SQM.1              <dbl> 9561.892, 16576.853, 13091.861, 20730.601, 672…\n$ AGE.1                   <dbl> -9514.634, -58185.479, -26707.386, -93308.988,…\n$ PROX_CBD.1              <dbl> -120681.94, -149434.22, -259397.77, 2426853.66…\n$ PROX_CHILDCARE.1        <dbl> 319266.925, 441102.177, -120116.816, 480825.28…\n$ PROX_ELDERLYCARE.1      <dbl> -393417.795, 325188.741, 535855.806, 314783.72…\n$ PROX_URA_GROWTH_AREA.1  <dbl> -159980.203, -142290.389, -253621.206, -267929…\n$ PROX_MRT.1              <dbl> -299742.96, -2510522.23, -936853.28, -2039479.…\n$ PROX_PARK.1             <dbl> -172104.47, 523379.72, 209099.85, -759153.26, …\n$ PROX_PRIMARY_SCH.1      <dbl> 242668.03, 1106830.66, 571462.33, 3127477.21, …\n$ PROX_SHOPPING_MALL.1    <dbl> 300881.390, -87693.378, -126732.712, -29593.34…\n$ PROX_BUS_STOP.1         <dbl> 1210615.44, 1843587.22, 1411924.90, 7225577.51…\n$ NO_Of_UNITS.1           <dbl> 104.8290640, -288.3441183, -9.5532945, -161.35…\n$ FAMILY_FRIENDLY.1       <dbl> -9075.370, 310074.664, 5949.746, 1556178.531, …\n$ FREEHOLD.1              <dbl> 303955.61, 396221.27, 168821.75, 1212515.58, 3…\n$ y                       <dbl> 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ yhat                    <dbl> 2886531.8, 3466801.5, 3616527.2, 5435481.6, 13…\n$ residual                <dbl> 113468.16, 413198.52, -291527.20, -1185481.63,…\n$ CV_Score                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Stud_residual           <dbl> 0.38207013, 1.01433140, -0.83780678, -2.846146…\n$ Intercept_SE            <dbl> 516105.5, 488083.5, 963711.4, 444185.5, 211962…\n$ AREA_SQM_SE             <dbl> 823.2860, 825.2380, 988.2240, 617.4007, 1376.2…\n$ AGE_SE                  <dbl> 5889.782, 6226.916, 6510.236, 6010.511, 8180.3…\n$ PROX_CBD_SE             <dbl> 37411.22, 23615.06, 56103.77, 469337.41, 41064…\n$ PROX_CHILDCARE_SE       <dbl> 319111.1, 299705.3, 349128.5, 304965.2, 698720…\n$ PROX_ELDERLYCARE_SE     <dbl> 120633.34, 84546.69, 129687.07, 127150.69, 327…\n$ PROX_URA_GROWTH_AREA_SE <dbl> 56207.39, 76956.50, 95774.60, 470762.12, 47433…\n$ PROX_MRT_SE             <dbl> 185181.3, 281133.9, 275483.7, 279877.1, 363830…\n$ PROX_PARK_SE            <dbl> 205499.6, 229358.7, 314124.3, 227249.4, 364580…\n$ PROX_PRIMARY_SCH_SE     <dbl> 152400.7, 165150.7, 196662.6, 240878.9, 249087…\n$ PROX_SHOPPING_MALL_SE   <dbl> 109268.8, 98906.8, 119913.3, 177104.1, 301032.…\n$ PROX_BUS_STOP_SE        <dbl> 600668.6, 410222.1, 464156.7, 562810.8, 740922…\n$ NO_Of_UNITS_SE          <dbl> 218.1258, 208.9410, 210.9828, 361.7767, 299.50…\n$ FAMILY_FRIENDLY_SE      <dbl> 131474.73, 114989.07, 146607.22, 108726.62, 16…\n$ FREEHOLD_SE             <dbl> 115954.0, 130110.0, 141031.5, 138239.1, 210641…\n$ Intercept_TV            <dbl> 3.9720784, 3.3460017, 3.5629010, 0.5276150, 1.…\n$ AREA_SQM_TV             <dbl> 11.614302, 20.087361, 13.247868, 33.577223, 4.…\n$ AGE_TV                  <dbl> -1.6154474, -9.3441881, -4.1023685, -15.524301…\n$ PROX_CBD_TV             <dbl> -3.22582173, -6.32792021, -4.62353528, 5.17080…\n$ PROX_CHILDCARE_TV       <dbl> 1.000488185, 1.471786337, -0.344047555, 1.5766…\n$ PROX_ELDERLYCARE_TV     <dbl> -3.26126929, 3.84626245, 4.13191383, 2.4756745…\n$ PROX_URA_GROWTH_AREA_TV <dbl> -2.846248368, -1.848971738, -2.648105057, -5.6…\n$ PROX_MRT_TV             <dbl> -1.61864578, -8.92998600, -3.40075727, -7.2870…\n$ PROX_PARK_TV            <dbl> -0.83749312, 2.28192684, 0.66565951, -3.340617…\n$ PROX_PRIMARY_SCH_TV     <dbl> 1.59230221, 6.70194543, 2.90580089, 12.9836104…\n$ PROX_SHOPPING_MALL_TV   <dbl> 2.753588422, -0.886626400, -1.056869486, -0.16…\n$ PROX_BUS_STOP_TV        <dbl> 2.0154464, 4.4941192, 3.0419145, 12.8383775, 0…\n$ NO_Of_UNITS_TV          <dbl> 0.480589953, -1.380026395, -0.045279967, -0.44…\n$ FAMILY_FRIENDLY_TV      <dbl> -0.06902748, 2.69655779, 0.04058290, 14.312764…\n$ FREEHOLD_TV             <dbl> 2.6213469, 3.0452799, 1.1970499, 8.7711485, 1.…\n$ Local_R2                <dbl> 0.8846744, 0.8899773, 0.8947007, 0.9073605, 0.…\n$ coords.x1               <dbl> 22085.12, 25656.84, 23963.99, 27044.28, 41042.…\n$ coords.x2               <dbl> 29951.54, 34546.20, 32890.80, 32319.77, 33743.…\n$ geometry                <POINT [m]> POINT (22085.12 29951.54), POINT (25656.…\n\n\n\nsummary(gwr.adaptive$SDF$yhat)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  171347  1102001  1385528  1751842  1982307 13887901"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#visualising-local-r2",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#visualising-local-r2",
    "title": "Hands On Exercise 8",
    "section": "Visualising local R2",
    "text": "Visualising local R2\nThe code chunks below is used to create an interactive point symbol map.\n\ntmap_mode(\"view\")\ntm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"Local_R2\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\n\n\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#visualising-coefficient-estimates",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#visualising-coefficient-estimates",
    "title": "Hands On Exercise 8",
    "section": "Visualising coefficient estimates",
    "text": "Visualising coefficient estimates\nThe code chunks below is used to create an interactive point symbol map.\n\ntmap_mode(\"view\")\nAREA_SQM_SE <- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_SE\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\nAREA_SQM_TV <- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_TV\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\ntmap_arrange(AREA_SQM_SE, AREA_SQM_TV, \n             asp=1, ncol=2,\n             sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\n\nBy URA Planning Region\n\ntm_shape(mpsz_svy21[mpsz_svy21$REGION_N==\"CENTRAL REGION\", ])+\n  tm_polygons()+\ntm_shape(condo_resale.sf.adaptive) + \n  tm_bubbles(col = \"Local_R2\",\n           size = 0.15,\n           border.col = \"gray60\",\n           border.lwd = 1)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html",
    "title": "Hands On Exercise 8",
    "section": "",
    "text": "pacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary, ggstatsplot)\nWithout using ggstatsplot, we will have to calculate a correlation matrix but using ggcoormat, we do not need to.\nSELLING Price is dependent variable\nUsually the output gives us all the information not formatted, so the gtsummary package allows us to format it in a html table to make thie information more comprehensible. By default, there is no intercept row, but we can add it in."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#visualising-model-parameters",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#visualising-model-parameters",
    "title": "Hands On Exercise 8",
    "section": "Visualising model parameters",
    "text": "Visualising model parameters\nVisualise the confidence interval clearer than using the table format.\n\nmlr.p <- ggcoefstats(condo.mlr, sort = \"ascending\")\nmlr.p\n\n\n\n\n\ncondo_resale.res.sf <- cbind(condo_resale.sf,                          condo.mlr$residuals) %>% rename(`MLR_RES` = `condo.mlr.residuals`)\n\nThere are may parameters that we can customise in this visualisation to present the modelling results better. If the variable falls after the line, it means that there is an inverse relation with the dependent variable (positive or negative Beta value).\n\ncondo_resale.sp <- as_Spatial(condo_resale.res.sf) \ncondo_resale.sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1436 \nextent      : 14940.85, 43352.45, 24765.67, 48382.81  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 22\nnames       : POSTCODE, SELLING_PRICE, AREA_SQM, AGE,    PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN,    PROX_MRT,   PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH, PROX_SHOPPING_MALL, ... \nmin values  :    18965,        540000,       34,   0, 0.386916393,    0.004927023,      0.054508623,          0.214539508,        0.051817113,       0.004927023, 0.052779424, 0.029064164,      0.077106132,          0.077106132,                  0, ... \nmax values  :   828833,       1.8e+07,      619,  37, 19.18042832,     3.46572633,      3.949157205,           9.15540001,        5.374348075,       2.229045366,  3.48037319,  2.16104919,      3.928989144,          6.748192062,        3.477433767, ..."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html",
    "title": "Geographically Weighted Predictive Models",
    "section": "",
    "text": "Predictive modelling uses statistical learning or machine learning techniques to predict outcomes. By and large, the event one wants to predict is in the future. However, a set of known outcome and predictors (also known as variables) will be used to calibrate the predictive models.\nGeospatial predictive modelling is conceptually rooted in the principle that the occurrences of events being modeled are limited in distribution. When geographically referenced data are used, occurrences of events are neither uniform nor random in distribution over space. There are geospatial factors (infrastructure, sociocultural, topographic, etc.) that constrain and influence where the locations of events occur. Geospatial predictive modeling attempts to describe those constraints and influences by spatially correlating occurrences of historical geospatial locations with environmental factors that represent those constraints and influences.\n\n\nIn this in-class exercise, you will learn how to build predictive model by using geographical random forest method. By the end of this hands-on exercise, you will acquire the skills of:\n\npreparing training and test data sets by using appropriate data sampling methods,\ncalibrating predictive models by using both geospatial statistical learning and machine learning methods,\ncomparing and selecting the best model for predicting the future outcome,\npredicting the future outcomes by using the best model calibrated.\n\n#The Data\n\nAspatial dataset:\n\nHDB Resale data: a list of HDB resale transacted prices in Singapore from Jan 2017 onwards. It is in csv format which can be downloaded from Data.gov.sg.\n\nGeospatial dataset:\n\nMP14_SUBZONE_WEB_PL: a polygon feature data providing information of URA 2014 Master Plan Planning Subzone boundary data. It is in ESRI shapefile format. This data set was also downloaded from Data.gov.sg\n\nLocational factors with geographic coordinates:\n\nDownloaded from Data.gov.sg.\n\nEldercare data is a list of eldercare in Singapore. It is in shapefile format.\nHawker Centre data is a list of hawker centres in Singapore. It is in geojson format.\nParks data is a list of parks in Singapore. It is in geojson format.\nSupermarket data is a list of supermarkets in Singapore. It is in geojson format.\nCHAS clinics data is a list of CHAS clinics in Singapore. It is in geojson format.\nChildcare service data is a list of childcare services in Singapore. It is in geojson format.\nKindergartens data is a list of kindergartens in Singapore. It is in geojson format.\n\nDownloaded from Datamall.lta.gov.sg.\n\nMRT data is a list of MRT/LRT stations in Singapore with the station names and codes. It is in shapefile format.\nBus stops data is a list of bus stops in Singapore. It is in shapefile format.\n\n\nLocational factors without geographic coordinates:\n\nDownloaded from Data.gov.sg.\n\nPrimary school data is extracted from the list on General information of schools from data.gov portal. It is in csv format.\n\nRetrieved/Scraped from other sources\n\nCBD coordinates obtained from Google.\nShopping malls data is a list of Shopping malls in Singapore obtained from Wikipedia.\nGood primary schools is a list of primary schools that are ordered in ranking in terms of popularity and this can be found at Local Salary Forum."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#reading-data-file-to-rds",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#reading-data-file-to-rds",
    "title": "Geographically Weighted Predictive Models",
    "section": "Reading data file to rds",
    "text": "Reading data file to rds\nReading the input data sets. It is in simple feature data frame.\n\nmdata <- read_rds(\"data/aspatial/mdata.rds\")\n\n\nData Sampling\nThe entire data are split into training and test data sets with 65% and 35% respectively by using initial_split() of rsample package. rsample is one of the package of tigymodels.\n\nset.seed(1234)\nresale_split <- initial_split(mdata, \n                              prop = 6.5/10,)\ntrain_data <- training(resale_split)\ntest_data <- testing(resale_split)\n\n\nwrite_rds(train_data, \"data/aspatial/train_data.rds\")\nwrite_rds(test_data, \"data/aspatial/test_data.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#converting-the-sf-data.frame-to-spatialpointdataframe",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#converting-the-sf-data.frame-to-spatialpointdataframe",
    "title": "Geographically Weighted Predictive Models",
    "section": "Converting the sf data.frame to SpatialPointDataFrame",
    "text": "Converting the sf data.frame to SpatialPointDataFrame\n\ntrain_data_sp <- as_Spatial(train_data)\ntrain_data_sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 10335 \nextent      : 11597.31, 42623.63, 28217.39, 48741.06  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 17\nnames       : resale_price, floor_area_sqm, storey_order, remaining_lease_mths,          PROX_CBD,     PROX_ELDERLYCARE,        PROX_HAWKER,           PROX_MRT,          PROX_PARK,   PROX_GOOD_PRISCH,        PROX_MALL,            PROX_CHAS,     PROX_SUPERMARKET, WITHIN_350M_KINDERGARTEN, WITHIN_350M_CHILDCARE, ... \nmin values  :       218000,             74,            1,                  555, 0.999393538715878, 1.98943787433087e-08, 0.0333358643817954, 0.0220407324774434, 0.0441643212802781, 0.0652540365486641,                0, 6.20621206270077e-09, 1.21715176356525e-07,                        0,                     0, ... \nmax values  :      1186888,            133,           17,                 1164,  19.6500691667807,     3.30163731686804,   2.86763031236184,   2.13060636038504,   2.41313695915468,   10.6223726149914, 2.27100643784442,    0.808332738794272,     1.57131703651196,                        7,                    20, ..."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#computing-adaptive-bandwidth",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#computing-adaptive-bandwidth",
    "title": "Geographically Weighted Predictive Models",
    "section": "Computing adaptive bandwidth",
    "text": "Computing adaptive bandwidth\nNext, bw.gwr() of GWmodel package will be used to determine the optimal bandwidth to be used.\n\n\n\n\n\n\nNote\n\n\n\nThe code chunk below is used to determine adaptive bandwidth and CV method is used to determine the optimal bandwidth.\n\n\n\nbw_adaptive <- bw.gwr(resale_price ~ floor_area_sqm +\n                  storey_order + remaining_lease_mths +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                  data=train_data_sp,\n                  approach=\"CV\",\n                  kernel=\"gaussian\",\n                  adaptive=TRUE,\n                  longlat=FALSE)\n\nThe result shows that 40 neighbour points will be the optimal bandwidth to be used if adaptive bandwidth is used for this data set.\n\nwrite_rds(bw_adaptive, \"data/aspatial/bw_adaptive.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#constructing-the-adaptive-bandwidth-gwr-model",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#constructing-the-adaptive-bandwidth-gwr-model",
    "title": "Geographically Weighted Predictive Models",
    "section": "Constructing the adaptive bandwidth gwr model",
    "text": "Constructing the adaptive bandwidth gwr model\nFirst, let us call the save bandwidth by using the code chunk below.\n\nbw_adaptive <- read_rds(\"data/aspatial/bw_adaptive.rds\")\n\nNow, we can go ahead to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and Gaussian kernel as shown in the code chunk below.\n\ngwr_adaptive <- gwr.basic(formula = resale_price ~\n                            floor_area_sqm + storey_order +\n                            remaining_lease_mths + PROX_CBD + \n                            PROX_ELDERLYCARE + PROX_HAWKER +\n                            PROX_MRT + PROX_PARK + PROX_MALL + \n                            PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                            WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                            WITHIN_1KM_PRISCH,\n                          data=train_data_sp,\n                          bw=bw_adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE,\n                          longlat = FALSE)\n\nThe code chunk below will be used to save the model in rds format for future use.\n\nwrite_rds(gwr_adaptive, \"data/aspatial/gwr_adaptive.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#retrieve-gwr-output-object",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#retrieve-gwr-output-object",
    "title": "Geographically Weighted Predictive Models",
    "section": "Retrieve gwr output object",
    "text": "Retrieve gwr output object\nThe code chunk below will be used to retrieve the save gwr model object.\n\ngwr_adaptive <- read_rds(\"data/aspatial/gwr_adaptive.rds\")\n\nThe code below can be used to display the model output.\n\ngwr_adaptive\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-03-18 12:04:51.653633 \n   Call:\n   gwr.basic(formula = resale_price ~ floor_area_sqm + storey_order + \n    remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + \n    PROX_MRT + PROX_PARK + PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + \n    WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH, \n    data = train_data_sp, bw = bw_adaptive, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  resale_price\n   Independent variables:  floor_area_sqm storey_order remaining_lease_mths PROX_CBD PROX_ELDERLYCARE PROX_HAWKER PROX_MRT PROX_PARK PROX_MALL PROX_SUPERMARKET WITHIN_350M_KINDERGARTEN WITHIN_350M_CHILDCARE WITHIN_350M_BUS WITHIN_1KM_PRISCH\n   Number of data points: 10335\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n    Min      1Q  Median      3Q     Max \n-205193  -39120   -1930   36545  472355 \n\n   Coefficients:\n                              Estimate Std. Error t value Pr(>|t|)    \n   (Intercept)              107601.073  10601.261  10.150  < 2e-16 ***\n   floor_area_sqm             2780.698     90.579  30.699  < 2e-16 ***\n   storey_order              14299.298    339.115  42.167  < 2e-16 ***\n   remaining_lease_mths        344.490      4.592  75.027  < 2e-16 ***\n   PROX_CBD                 -16930.196    201.254 -84.124  < 2e-16 ***\n   PROX_ELDERLYCARE         -14441.025    994.867 -14.516  < 2e-16 ***\n   PROX_HAWKER              -19265.648   1273.597 -15.127  < 2e-16 ***\n   PROX_MRT                 -32564.272   1744.232 -18.670  < 2e-16 ***\n   PROX_PARK                 -5712.625   1483.885  -3.850 0.000119 ***\n   PROX_MALL                -14717.388   2007.818  -7.330 2.47e-13 ***\n   PROX_SUPERMARKET         -26881.938   4189.624  -6.416 1.46e-10 ***\n   WITHIN_350M_KINDERGARTEN   8520.472    632.812  13.464  < 2e-16 ***\n   WITHIN_350M_CHILDCARE     -4510.650    354.015 -12.741  < 2e-16 ***\n   WITHIN_350M_BUS             813.493    222.574   3.655 0.000259 ***\n   WITHIN_1KM_PRISCH         -8010.834    491.512 -16.298  < 2e-16 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 61650 on 10320 degrees of freedom\n   Multiple R-squared: 0.7373\n   Adjusted R-squared: 0.737 \n   F-statistic:  2069 on 14 and 10320 DF,  p-value: < 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 3.922202e+13\n   Sigma(hat): 61610.08\n   AIC:  257320.2\n   AICc:  257320.3\n   BIC:  247249\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 40 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                                   Min.     1st Qu.      Median     3rd Qu.\n   Intercept                -3.2478e+08 -4.7727e+05 -8.3004e+03  5.5025e+05\n   floor_area_sqm           -2.8714e+04  1.4475e+03  2.3011e+03  3.3900e+03\n   storey_order              3.3186e+03  8.5899e+03  1.0826e+04  1.3397e+04\n   remaining_lease_mths     -1.4431e+03  2.6063e+02  3.9048e+02  5.2865e+02\n   PROX_CBD                 -1.0837e+07 -5.7697e+04 -1.3787e+04  2.6552e+04\n   PROX_ELDERLYCARE         -3.2195e+07 -4.0643e+04  1.0562e+04  6.1054e+04\n   PROX_HAWKER              -2.3985e+08 -5.1365e+04  3.0026e+03  6.4287e+04\n   PROX_MRT                 -1.1632e+07 -1.0488e+05 -4.9373e+04  5.1037e+03\n   PROX_PARK                -6.5961e+06 -4.8671e+04 -8.8128e+02  5.3498e+04\n   PROX_MALL                -1.8112e+07 -7.4238e+04 -1.3982e+04  4.9779e+04\n   PROX_SUPERMARKET         -4.5761e+06 -6.3461e+04 -1.7429e+04  3.5616e+04\n   WITHIN_350M_KINDERGARTEN -4.1823e+05 -6.0040e+03  9.0209e+01  4.7127e+03\n   WITHIN_350M_CHILDCARE    -1.0273e+05 -2.2375e+03  2.6668e+02  2.6388e+03\n   WITHIN_350M_BUS          -1.1757e+05 -1.4719e+03  1.1626e+02  1.7584e+03\n   WITHIN_1KM_PRISCH        -6.6465e+05 -5.5959e+03  2.6916e+02  5.7500e+03\n                                  Max.\n   Intercept                1.6493e+08\n   floor_area_sqm           5.0907e+04\n   storey_order             2.9537e+04\n   remaining_lease_mths     1.8119e+03\n   PROX_CBD                 2.2411e+07\n   PROX_ELDERLYCARE         8.2444e+07\n   PROX_HAWKER              5.9654e+06\n   PROX_MRT                 2.0189e+08\n   PROX_PARK                1.5188e+07\n   PROX_MALL                1.0443e+07\n   PROX_SUPERMARKET         3.8330e+06\n   WITHIN_350M_KINDERGARTEN 6.6799e+05\n   WITHIN_350M_CHILDCARE    1.0802e+05\n   WITHIN_350M_BUS          3.7313e+04\n   WITHIN_1KM_PRISCH        5.0231e+05\n   ************************Diagnostic information*************************\n   Number of data points: 10335 \n   Effective number of parameters (2trace(S) - trace(S'S)): 1730.101 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 8604.899 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 238871.9 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 237036.9 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 238209.1 \n   Residual sum of squares: 4.829191e+12 \n   R-square value:  0.967657 \n   Adjusted R-square value:  0.9611534 \n\n   ***********************************************************************\n   Program stops at: 2024-03-18 12:05:55.832785"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#converting-the-test-data-from-sf-data.frame-to-spatialpointdataframe",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#converting-the-test-data-from-sf-data.frame-to-spatialpointdataframe",
    "title": "Geographically Weighted Predictive Models",
    "section": "Converting the test data from sf data.frame to SpatialPointDataFrame**",
    "text": "Converting the test data from sf data.frame to SpatialPointDataFrame**\n\ntest_data_sp <- test_data %>%\n  as_Spatial()\ntest_data_sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 5566 \nextent      : 11597.31, 42623.63, 28287.8, 48669.59  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 17\nnames       : resale_price, floor_area_sqm, storey_order, remaining_lease_mths,         PROX_CBD,     PROX_ELDERLYCARE,        PROX_HAWKER,           PROX_MRT,          PROX_PARK,   PROX_GOOD_PRISCH,        PROX_MALL,            PROX_CHAS,     PROX_SUPERMARKET, WITHIN_350M_KINDERGARTEN, WITHIN_350M_CHILDCARE, ... \nmin values  :       230888,             74,            1,                  546, 1.00583660772922, 3.34897933104965e-07, 0.0474019664161957, 0.0414043955932523, 0.0502664084494264, 0.0907500295577619,                0, 4.55547870890763e-09, 1.21715176356525e-07,                        0,                     0, ... \nmax values  :      1050000,            138,           14,                 1151,  19.632402730488,     3.30163731686804,   2.83106651960209,   2.13060636038504,   2.41313695915468,   10.6169590126272, 2.26056404492346,     0.79249074802552,     1.53786629004208,                        7,                    16, ..."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#computing-adaptive-bandwidth-for-the-test-data",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#computing-adaptive-bandwidth-for-the-test-data",
    "title": "Geographically Weighted Predictive Models",
    "section": "Computing adaptive bandwidth for the test data",
    "text": "Computing adaptive bandwidth for the test data\n\ngwr_bw_test_adaptive <- bw.gwr(resale_price ~ floor_area_sqm +\n                  storey_order + remaining_lease_mths +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                  data=test_data_sp,\n                  approach=\"CV\",\n                  kernel=\"gaussian\",\n                  adaptive=TRUE,\n                  longlat=FALSE)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#computing-predicted-values-of-the-test-data",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#computing-predicted-values-of-the-test-data",
    "title": "Geographically Weighted Predictive Models",
    "section": "Computing predicted values of the test data",
    "text": "Computing predicted values of the test data\n\ngwr_pred <- gwr.predict(formula = resale_price ~\n                          floor_area_sqm + storey_order +\n                          remaining_lease_mths + PROX_CBD + \n                          PROX_ELDERLYCARE + PROX_HAWKER + \n                          PROX_MRT + PROX_PARK + PROX_MALL + \n                          PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                          WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + \n                          WITHIN_1KM_PRISCH, \n                        data=train_data_sp, \n                        predictdata = test_data_sp, \n                        bw=40, \n                        kernel = 'gaussian', \n                        adaptive=TRUE, \n                        longlat = FALSE)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#extracting-coordinates-data",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#extracting-coordinates-data",
    "title": "Geographically Weighted Predictive Models",
    "section": "Extracting coordinates data",
    "text": "Extracting coordinates data\nThe code chunk below extract the x,y coordinates of the full, training and test data sets.\n\ncoords <- st_coordinates(mdata)\ncoords_train <- st_coordinates(train_data)\ncoords_test <- st_coordinates(test_data)\n\nBefore continue, we write all the output into rds for future used.\n\ncoords_train <- write_rds(coords_train, \"data/aspatial/coords_train.rds\" )\ncoords_test <- write_rds(coords_test, \"data/aspatial/coords_test.rds\" )\n\n\ncoords_train <- read_rds(\"data/aspatial/coords_train.rds\")\ncoords_test <- read_rds(\"data/aspatial/coords_test.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#dropping-geometry-field",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#dropping-geometry-field",
    "title": "Geographically Weighted Predictive Models",
    "section": "Dropping geometry field",
    "text": "Dropping geometry field\nFirst, we will drop geometry column of the sf data.frame by using st_drop_geometry() of sf package.\n\ntrain_data <- train_data %>% \n  st_drop_geometry()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#calibrating-using-training-data",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#calibrating-using-training-data",
    "title": "Geographically Weighted Predictive Models",
    "section": "Calibrating using training data",
    "text": "Calibrating using training data\nThe code chunk below calibrate a geographic ranform forest model by using grf() of SpatialML package.\n\nset.seed(1234)\ngwRF_adaptive <- grf(formula = resale_price ~ floor_area_sqm + storey_order +\n                       remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE +\n                       PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL +\n                       PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                       WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                       WITHIN_1KM_PRISCH,\n                     dframe=train_data, \n                     bw=55,\n                     kernel=\"adaptive\",\n                     coords=coords_train)\n\nLet’s save the model output by using the code chunk below.\n\nwrite_rds(gwRF_adaptive, \"data/aspatial/gwRF_adaptive.rds\")\n\nThe code chunk below can be used to retrieve the save model in future.\n\ngwRF_adaptive <- read_rds(\"data/aspatial/gwRF_adaptive.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#predicting-by-using-test-data",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#predicting-by-using-test-data",
    "title": "Geographically Weighted Predictive Models",
    "section": "Predicting by using test data",
    "text": "Predicting by using test data\n\nPreparing the test data\nThe code chunk below will be used to combine the test data with its corresponding coordinates data.\n\ntest_data <- cbind(test_data, coords_test) %>%\n  st_drop_geometry()\n\n\n\nPredicting with test data\nNext, predict.grf() of spatialML package will be used to predict the resale value by using the test data and gwRF_adaptive model calibrated earlier.\n\ngwRF_pred <- predict.grf(gwRF_adaptive, \n                           test_data, \n                           x.var.name=\"X\",\n                           y.var.name=\"Y\", \n                           local.w=1,\n                           global.w=0)\n\nBefore moving on, let us save the output into rds file for future use.\n\nGRF_pred <- write_rds(gwRF_pred, \"data/aspatial/GRF_pred.rds\")\n\n\n\nConverting the predicting output into a data frame\nThe output of the predict.grf() is a vector of predicted values. It is wiser to convert it into a data frame for further visualisation and analysis.\n\nGRF_pred <- read_rds(\"data/aspatial/GRF_pred.rds\")\nGRF_pred_df <- as.data.frame(GRF_pred)\n\nIn the code chunk below, cbind() is used to append the predicted values onto test_datathe\n\ntest_data_p <- cbind(test_data, GRF_pred_df)\n\n\nwrite_rds(test_data_p, \"data/aspatial/test_data_p.rds\")\n\n\ntest_data_p <- read_rds(\"data/aspatial/test_data_p.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#calculating-root-mean-square-error",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#calculating-root-mean-square-error",
    "title": "Geographically Weighted Predictive Models",
    "section": "Calculating Root Mean Square Error",
    "text": "Calculating Root Mean Square Error\nThe root mean square error (RMSE) allows us to measure how far predicted values are from observed values in a regression analysis. In the code chunk below, rmse() of Metrics package is used to compute the RMSE.\n\nrmse(test_data_p$resale_price, \n     test_data_p$GRF_pred)\n\nthe condition has length > 1\n\n\n[1] NA"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#visualising-the-predicted-values",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#visualising-the-predicted-values",
    "title": "Geographically Weighted Predictive Models",
    "section": "Visualising the predicted values",
    "text": "Visualising the predicted values\nAlternatively, scatterplot can be used to visualise the actual resale price and the predicted resale price by using the code chunk below.\n\nggplot(data = test_data_p,\n       aes(x = GRF_pred,\n           y = resale_price)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nA better predictive model should have the scatter point close to the diagonal line. The scatter plot can be also used to detect if any outliers in the model."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/TH3_test.html",
    "href": "Take-home_Ex/Take-home_Ex03/TH3_test.html",
    "title": "Take-home Exercise 3: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "",
    "text": "pacman::p_load(sf, spded, sfdep, tmap, tidyverse, plotly, Kendall)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09.html",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09.html",
    "title": "Geographically Weighted Predictive Models",
    "section": "",
    "text": "pacman::p_load(sf, spdep, GWmodel, SpatialML, \n               tmap, rsample, tidymodels, Metrics, tidyverse, gtsummary, rpart, rpart.plot, ggstatsplot, performance)\nSplit 50/50 for the training data and the testing data\nresale_split will contain all the data, we do not need to save it as an object if we want to save space because after we split to train and test, it will be useless. Instead, we can use pipelining %>%.\nWe have to read the documentation to know whether we need it in a normal dataframe or tibble dataframe. We use as.data.frame() to create a dataframe, which has 17 columns instead of 18 because we have dropped the geometry column."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#computing-correlation-matirx",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#computing-correlation-matirx",
    "title": "Geographically Weighted Predictive Models",
    "section": "Computing Correlation Matirx",
    "text": "Computing Correlation Matirx\n\nrs_sf1 <- rs_sf %>% \n  st_drop_geometry()\ncorrplot::corrplot(cor(rs_sf1[, 2:17]), \n                   diag = FALSE, \n                   order = \"AOE\",\n                   tl.pos = \"td\", \n                   tl.cex = 0.5, \n                   method = \"number\", \n                   type = \"upper\")\n\n\n\n\n\nprice_mlr <- lm(formula = RESALE_PRICE ~ FLOOR_AREA_SQM +\n                  STOREY_ORDER + REMAINING_LEASE_MTHS +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + PROX_CHAS + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                data=train_df)\nsummary(price_mlr)\n\n\nCall:\nlm(formula = RESALE_PRICE ~ FLOOR_AREA_SQM + STOREY_ORDER + REMAINING_LEASE_MTHS + \n    PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK + \n    PROX_MALL + PROX_SUPERMARKET + PROX_CHAS + WITHIN_350M_KINDERGARTEN + \n    WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH, \n    data = train_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-179676  -39020   -1719   36755  327324 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(>|t|)    \n(Intercept)              109622.960  11993.611   9.140  < 2e-16 ***\nFLOOR_AREA_SQM             2733.136    103.116  26.505  < 2e-16 ***\nSTOREY_ORDER              14198.168    384.182  36.957  < 2e-16 ***\nREMAINING_LEASE_MTHS        346.624      5.208  66.557  < 2e-16 ***\nPROX_CBD                 -16943.794    227.064 -74.621  < 2e-16 ***\nPROX_ELDERLYCARE         -13891.413   1124.964 -12.348  < 2e-16 ***\nPROX_HAWKER              -17758.037   1461.269 -12.152  < 2e-16 ***\nPROX_MRT                 -32357.534   1965.095 -16.466  < 2e-16 ***\nPROX_PARK                 -6714.626   1672.160  -4.016 5.99e-05 ***\nPROX_MALL                -14080.474   2268.191  -6.208 5.64e-10 ***\nPROX_SUPERMARKET         -24077.152   5068.317  -4.751 2.06e-06 ***\nPROX_CHAS                 -5819.260   7208.182  -0.807 0.419510    \nWITHIN_350M_KINDERGARTEN   8730.822    721.593  12.099  < 2e-16 ***\nWITHIN_350M_CHILDCARE     -4629.126    399.231 -11.595  < 2e-16 ***\nWITHIN_350M_BUS             979.339    252.851   3.873 0.000108 ***\nWITHIN_1KM_PRISCH         -8434.367    553.862 -15.228  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 61050 on 7934 degrees of freedom\nMultiple R-squared:  0.7405,    Adjusted R-squared:  0.7401 \nF-statistic:  1510 on 15 and 7934 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#revising-mlr-model",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#revising-mlr-model",
    "title": "Geographically Weighted Predictive Models",
    "section": "Revising mlr model",
    "text": "Revising mlr model\n\ntrain_df <- train_df %>%\n  select(-c(PROX_CHAS))\ntrain_sf <- train_sf %>%\n  select(-c(PROX_CHAS))\ntest_df <- test_df %>%\n  select(-c(PROX_CHAS))\ntest_sf <- test_sf %>%\n  select(-c(PROX_CHAS))\n\n\nprice_mlr <- lm(formula = RESALE_PRICE ~ FLOOR_AREA_SQM +\n                  STOREY_ORDER + REMAINING_LEASE_MTHS +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                data=train_df)\nsummary(price_mlr)\n\n\nCall:\nlm(formula = RESALE_PRICE ~ FLOOR_AREA_SQM + STOREY_ORDER + REMAINING_LEASE_MTHS + \n    PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK + \n    PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + \n    WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH, \n    data = train_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-179178  -39031   -1868   36751  327631 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(>|t|)    \n(Intercept)              109413.550  11990.543   9.125  < 2e-16 ***\nFLOOR_AREA_SQM             2725.663    102.698  26.541  < 2e-16 ***\nSTOREY_ORDER              14192.913    384.118  36.949  < 2e-16 ***\nREMAINING_LEASE_MTHS        346.996      5.187  66.893  < 2e-16 ***\nPROX_CBD                 -16943.081    227.058 -74.620  < 2e-16 ***\nPROX_ELDERLYCARE         -13972.191   1120.481 -12.470  < 2e-16 ***\nPROX_HAWKER              -17968.486   1437.798 -12.497  < 2e-16 ***\nPROX_MRT                 -32448.233   1961.837 -16.540  < 2e-16 ***\nPROX_PARK                 -6753.096   1671.444  -4.040 5.39e-05 ***\nPROX_MALL                -14003.731   2266.148  -6.180 6.75e-10 ***\nPROX_SUPERMARKET         -25566.285   4720.643  -5.416 6.28e-08 ***\nWITHIN_350M_KINDERGARTEN   8740.242    721.483  12.114  < 2e-16 ***\nWITHIN_350M_CHILDCARE     -4614.476    398.810 -11.571  < 2e-16 ***\nWITHIN_350M_BUS             990.698    252.454   3.924 8.77e-05 ***\nWITHIN_1KM_PRISCH         -8438.093    553.831 -15.236  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 61040 on 7935 degrees of freedom\nMultiple R-squared:  0.7405,    Adjusted R-squared:  0.7401 \nF-statistic:  1618 on 14 and 7935 DF,  p-value: < 2.2e-16\n\n\nExtract te x y coordinates of the full, training and test data sets\n\ncoords <- st_coordinates(rs_sf)\ncoords_train <- st_coordinates(train_sf)\ncoords_test <- st_coordinates(test_sf)\n\nThe algorithm will require 2 dataset, one is the dataframe that consist of independent and dependent variables. Another separate data table needs to have all the X and Y coordinates (named X and Y columns).\n\nset.seed(1234)\nrs_rp <- rpart(formula = RESALE_PRICE ~ FLOOR_AREA_SQM +\n                  STOREY_ORDER + REMAINING_LEASE_MTHS +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                data=train_df)\nrs_rp\n\nn= 7950 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 7950 1.139546e+14 433705.6  \n   2) PROX_CBD>=7.974483 6665 4.472144e+13 403736.0  \n     4) REMAINING_LEASE_MTHS< 1020.5 4228 1.573100e+13 370187.4  \n       8) PROX_CBD>=14.48068 1820 2.748388e+12 337963.6 *\n       9) PROX_CBD< 14.48068 2408 9.664405e+12 394542.6 *\n     5) REMAINING_LEASE_MTHS>=1020.5 2437 1.597594e+13 461940.1  \n      10) PROX_CBD>=10.40657 2331 9.762718e+12 451754.4  \n        20) PROX_CBD>=14.20377 1088 3.345588e+12 426109.1 *\n        21) PROX_CBD< 14.20377 1243 5.075243e+12 474201.8 *\n      11) PROX_CBD< 10.40657 106 6.532500e+11 685929.1 *\n   3) PROX_CBD< 7.974483 1285 3.219685e+13 589151.4  \n     6) REMAINING_LEASE_MTHS< 930.5 745 6.613365e+12 486637.6  \n      12) FLOOR_AREA_SQM< 98.5 451 2.446537e+12 442460.5 *\n      13) FLOOR_AREA_SQM>=98.5 294 1.936449e+12 554405.7 *\n     7) REMAINING_LEASE_MTHS>=930.5 540 6.952722e+12 730582.5  \n      14) REMAINING_LEASE_MTHS< 1071.5 314 2.461969e+12 676641.3 *\n      15) REMAINING_LEASE_MTHS>=1071.5 226 2.307737e+12 805527.4 *\n\n\nrs_rp is a collection of pbject that gives us the split result and importance of variables so on. We can then plot the tree to see how it is split.\n\nrpart.plot(rs_rp)\n\n\n\n\nWhen we use the condition PROX_CBD >= 8, we will split to 2 major groups. The next level is the REMAINING_LEASE_MONTHS. Once we know the conditions, we can use SQL to extract the target response variable.\n\nset.seed(1234)\nrs_rf <- ranger(formula = RESALE_PRICE ~ FLOOR_AREA_SQM +\n                  STOREY_ORDER + REMAINING_LEASE_MTHS +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                data=train_df,\n                importance = \"impurity\")\nrs_rf\n\nRanger result\n\nCall:\n ranger(formula = RESALE_PRICE ~ FLOOR_AREA_SQM + STOREY_ORDER +      REMAINING_LEASE_MTHS + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +      PROX_MRT + PROX_PARK + PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +      WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH,      data = train_df, importance = \"impurity\") \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      7950 \nNumber of independent variables:  14 \nMtry:                             3 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       774008152 \nR squared (OOB):                  0.9460084 \n\n\nUsing the ranger package is relatively faster, so most of the time we will use this instead of random forest. By default, we use 500 trees. Initially, the importance will be set to none, so we cannot retrieve the importnce. So we need to specify to determine which variable are important in our calculation. Impurity is commonly used in decision trees and regression trees.\n\nvi <- as.data.frame(rs_rf$variable.importance)\nvi$variables <- rownames(vi)\nvi <- vi %>%\n  rename(vi = \"rs_rf$variable.importance\")\n\n\nggplot(data = vi,\n       aes(x=vi,\n           y=reorder(variables, vi))) #use reorder to sort the bars to view the importance, if not it will be displayed alphabetically +\n\n\n\n  geom_bar(stat=\"identity\")\n\ngeom_bar: just = 0.5, width = NULL, na.rm = FALSE, orientation = NA\nstat_identity: na.rm = FALSE\nposition_stack \n\n\nIf we have a model that looks like that it is okay, because there are different importance for each variable. But if the model shows one super long bar for one variable, that means there might be issues with the data that creates a complete separation situation, then we might need to exclude the extreme predictor to be able to analyse the other variables.\nWe need to calculate the bandwidth and fit it into the model. This will take very long and the data is very big, so we should confine our observations to a certain area, so that we can build an application and it wont hang.\n\nrs_grf <- read_rds(\"data/models/rs_grf.rds\")\n\n\ntest_df <- cbind(test_sf, coords_test) %>%\n  st_drop_geometry()\n\n\ngrf_pred <- predict.grf(rs_grf,\n                        test_df,\n                        x.var.name = \"X\",\n                        y.var.name = \"Y\",\n                        local.w = 1,\n                        global.w = 0)\n\n\ngrf_pred <- read_rds(\"data/models/grf_pred.rds\")\ngrf_pred_df <- as.data.frame(grf_pred)\n\nAppend the predicted values onto the test_df\n\ntest_pred <- test_df %>% \n  select(RESALE_PRICE) %>%\n  cbind(grf_pred_df)\n\ntest_pred will have 2 columns, first one is the resale price and second is the predicted value using the geographical random forest to compare the predicted and actual values.\n\nrf_pred <- predict(rs_rf, test_df)\n\n\nrf_pred_df <- as.data.frame(rf_pred$predictions) %>%\n  rename(rf_pred = \"rf_pred$predictions\")\n\n\ntest_pred <- cbind(test_pred,\n                   rf_pred_df)\n\n\nmlr_pred <- predict(price_mlr, test_df)\n\n\nmlr_pred_df <- as.data.frame(mlr_pred) %>%\n  rename(mlr_pred = \"mlr_pred\")\n\n\ntest_pred <- cbind(test_pred,\n                   mlr_pred_df)\n\n\nyardstick::rmse(test_pred,\n                RESALE_PRICE,\n                mlr_pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      61821.\n\n\n\nmc <- test_pred %>%\n  pivot_longer(cols = c(2:4),\n               names_to = \"models\",\n               values_to = \"predicted\")\n\nThis gives us a summary of the statistics. Choose the model that gives us the best prediction.\n\nggplot(data = test_pred,\n       aes(x = grf_pred,\n           y = RESALE_PRICE)) +\n  geom_point()"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/TH3_test.html#show-all-events",
    "href": "Take-home_Ex/Take-home_Ex03/TH3_test.html#show-all-events",
    "title": "Take-home Exercise 3: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "Show all events",
    "text": "Show all events\nThis map below shows all events of earthquake from 2009 to 2023.\n\ntmap_mode('view')  # Define basemaps \nosm_basemap <- tm_basemap(server = \"OpenStreetMap.HOT\") \nimagery_basemap <- tm_basemap(server = \"Esri.WorldImagery\")\n\neo_map <- osm_basemap +\n  tm_shape(java_earthq) +\n  tm_symbols(size = 0.8, col = \"mag\", palette = \"Reds\", alpha = 1, \n             popup.vars = c(\"subdistrict\", \"year_week\"), id = \"mag\") +\n  tm_layout(title = \"Earthquake Events Magnitude\") +\n  tm_view(dot.size.fixed = TRUE)\n\neo_map"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/TH3_test.html#filter-by-year-and-week",
    "href": "Take-home_Ex/Take-home_Ex03/TH3_test.html#filter-by-year-and-week",
    "title": "Take-home Exercise 3: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "Filter by Year and Week",
    "text": "Filter by Year and Week\nSince I’ve incorporated a new column indicating the year and week of each earthquake event, we can implement a slider feature to visualize the evolution of earthquake occurrences throughout the year. As the slider moves across different weeks, the map dynamically updates to display earthquake events for the selected week. If there are no earthquake events recorded for a particular week, the map gracefully renders as empty, providing a clear representation of seismic activity over time.\nGiven the relatively sparse occurrence of earthquake events within a week, I opted to increase the size of each plotted point to enhance visibility and highlight the significance of each seismic event. By adjusting the size parameter to a higher value of 5, the resulting map provides a clearer and more pronounced representation of earthquake occurrences, ensuring that even minor seismic events are readily discernible to viewers.\n\ntmap_mode('view')  # Define basemaps \nosm_basemap <- tm_basemap(server = \"OpenStreetMap.HOT\") \nimagery_basemap <- tm_basemap(server = \"Esri.WorldImagery\")\n\nfilter_date <- \"2010, Week 48\"\n\n# Filter the dataset based on the specific year_week value\nfiltered_data <- java_earthq %>%\n  filter(year_week == filter_date)\n\nif (nrow(filtered_data) == 0) {\n  # If no rows are filtered, create an empty map or a message\n  empty_map <- osm_basemap +\n    tm_shape(java_earthq) +\n    tm_symbols(alpha = 0, size = 0) +  # Added '+' to separate tm_symbols() from tm_layout()\n    tm_layout(title = paste(\"No earthquake events for \", filter_date))  # Corrected tm_layout() function\n  \n  empty_map\n} else {\n  # If rows are filtered, create the map with the filtered dataset\n  eo_map <- osm_basemap +\n    tm_shape(filtered_data) +\n    tm_symbols(size = 5, col = \"mag\", palette = \"Reds\", alpha = 1, \n               popup.vars = c(\"subdistrict\", \"year_week\"), id = \"mag\", breaks = seq(3, 6.5, by = 0.5)) +\n    tm_layout(title = paste(\"Earthquake Events Magnitude for \", filter_date))  # Corrected tm_layout() function\n  \n  eo_map\n}"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/TH3_test.html#filter-by-year-and-month",
    "href": "Take-home_Ex/Take-home_Ex03/TH3_test.html#filter-by-year-and-month",
    "title": "Take-home Exercise 3: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "Filter by Year and Month",
    "text": "Filter by Year and Month\nThe filter can also be set by Year and Month.\n\ntmap_mode('view')  # Define basemaps \nosm_basemap <- tm_basemap(server = \"OpenStreetMap.HOT\") \nimagery_basemap <- tm_basemap(server = \"Esri.WorldImagery\")\n\nfilter_year <- 2010\nfilter_month <- \"January\"\n\n# Filter the dataset based on the specific year_week value\nfiltered_data <- java_earthq %>%\n  filter(year == filter_year,\n         month == filter_month)\n\nif (nrow(filtered_data) == 0) {\n  # If no rows are filtered, create an empty map or a message\n  empty_map <- osm_basemap +\n    tm_shape(java_earthq) +\n    tm_symbols(alpha = 0, size = 0) +  # Added '+' to separate tm_symbols() from tm_layout()\n    tm_layout(title = paste(\"No earthquake events for \", filter_date))  # Corrected tm_layout() function\n  \n  empty_map\n} else {\n  # If rows are filtered, create the map with the filtered dataset\n  eo_map <- osm_basemap +\n    tm_shape(filtered_data) +\n    tm_symbols(size = 5, col = \"mag\", palette = \"Reds\", alpha = 1, \n               popup.vars = c(\"subdistrict\", \"year_week\"), id = \"mag\", breaks = seq(3, 6.5, by = 0.5)) +\n    tm_layout(title = paste(\"Earthquake Events Magnitude for \", filter_date))  # Corrected tm_layout() function\n  \n  eo_map\n}"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html",
    "title": "Hands on Exercise 10: Modelling Geographical Accessibility",
    "section": "",
    "text": "In this hands-on exercise, you will gain hands-on experience on how to model geographical accessibility by using R’s geospatial analysis packages.\n\n\nBy the end of this hands-on exercise, you will be able:\n\nto import GIS polygon data into R and save them as simple feature data frame by using appropriate functions of sf package of R;\nto import aspatial data into R and save them as simple feature data frame by using appropriate functions of sf package of R;\nto computer accessibility measure by using Hansen’s potential model and Spatial Accessibility Measure (SAM); and\nto visualise the accessibility measures by using tmap and ggplot2 packages.\n\n\n\n\nFour data sets will be used in this hands-on exercise, they are:\n\nMP14_SUBZONE_NO_SEA_PL: URA Master Plan 2014 subzone boundary GIS data. This data set is downloaded from data.gov.sg.\nhexagons: A 250m radius hexagons GIS data. This data set was created by using st_make_grid() of sf package. It is in ESRI shapefile format.\nELDERCARE: GIS data showing location of eldercare service. This data is downloaded from data.gov.sg. There are two versions. One in ESRI shapefile format. The other one in Google kml file format. For the purpose of this hands-on exercise, ESRI shapefile format is provided.\nOD_Matrix: a distance matrix in csv format. There are six fields in the data file. They are:\n\norigin_id: the unique id values of the origin (i.e. fid of hexagon data set.),\ndestination_id: the unique id values of the destination (i.e. fid of ELDERCARE data set.),\nentry_cost: the perpendicular distance between the origins and the nearest road),\nnetwork_cost: the actual network distance from the origin and destination,\nexit_cost: the perpendicular distance between the destination and the nearest road), and\ntotal_cost: the summation of entry_cost, network_cost and exit_cost.\n\n\nAll the values of the cost related fields are in metres (we need it to be in this unit of measurement because it will affect the accessiblity index).\nReminder: Except MP14_SUBZONE_NO_SEA_PL data set, the other three data set are specially prepared by Prof. Kam for teaching and research purpose. Students taking IS415 Geospatial Analytics and Applications are allowed to use them for hands-on exercise purpose. Please obtain formal approval from Prof. Kam if you want to use them for other courses or usage."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#visualising-hansens-accessibility",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#visualising-hansens-accessibility",
    "title": "Hands on Exercise 10: Modelling Geographical Accessibility",
    "section": "Visualising Hansen’s Accessibility",
    "text": "Visualising Hansen’s Accessibility\n\nExtracting map extend\nFirstly, we will extract the extend of hexagons simple feature data frame by by using st_bbox() of sf package.\n\nmapex <- st_bbox(hexagons)\n\nThe code chunk below uses a collection of mapping fucntions of tmap package to create a high cartographic quality accessibility to eldercare centre in Singapore.\n\ntmap_mode(\"plot\")\ntm_shape(hexagon_Hansen,\n         bbox = mapex) + \n  tm_fill(col = \"accHansen\",\n          n = 10,\n          style = \"quantile\",\n          border.col = \"black\",\n          border.lwd = 1) +\ntm_shape(eldercare) +\n  tm_symbols(size = 0.1) +\n  tm_layout(main.title = \"Accessibility to eldercare: Hansen method\",\n            main.title.position = \"center\",\n            main.title.size = 2,\n            legend.outside = FALSE,\n            legend.height = 0.45, \n            legend.width = 3.0,\n            legend.format = list(digits = 6),\n            legend.position = c(\"right\", \"top\"),\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#statistical-graphic-visualisation",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#statistical-graphic-visualisation",
    "title": "Hands on Exercise 10: Modelling Geographical Accessibility",
    "section": "Statistical Graphic Visualisation",
    "text": "Statistical Graphic Visualisation\nIn this section, we are going to compare the distribution of Hansen’s accessibility values by URA Planning Region.\nFirstly, we need to add the planning region field into haxegon_Hansen simple feature data frame by using the code chunk below.\n\nhexagon_Hansen <- st_join(hexagon_Hansen, mpsz, \n                          join = st_intersects)\n\nNext, ggplot() will be used to plot the distribution by using boxplot graphical method.\n\nggplot(data=hexagon_Hansen, \n       aes(y = log(accHansen), \n           x= REGION_N)) +\n  geom_boxplot() +\n  geom_point(stat=\"summary\", \n             fun.y=\"mean\", \n             colour =\"red\", \n             size=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#computing-kd2sfcas-accessibility",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#computing-kd2sfcas-accessibility",
    "title": "Hands on Exercise 10: Modelling Geographical Accessibility",
    "section": "Computing KD2SFCA’s accessibility",
    "text": "Computing KD2SFCA’s accessibility\nIn this section, you are going to repeat most of the steps you had learned in previous section to perform the analysis. However, some of the codes will be combined into one code chunk.\nThe code chunk below calculates Hansen’s accessibility using ac() of SpatialAcc and data.frame() is used to save the output in a data frame called acc_KD2SFCA. Notice that KD2SFCA is used for family argument.\n\nacc_KD2SFCA <- data.frame(ac(hexagons$demand,\n                            eldercare$capacity,\n                            distmat_km, \n                            d0 = 50,\n                            power = 2, \n                            family = \"KD2SFCA\"))\n\ncolnames(acc_KD2SFCA) <- \"accKD2SFCA\"\nacc_KD2SFCA <- tbl_df(acc_KD2SFCA)\nhexagon_KD2SFCA <- bind_cols(hexagons, acc_KD2SFCA)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#visualising-kd2sfcas-accessibility",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#visualising-kd2sfcas-accessibility",
    "title": "Hands on Exercise 10: Modelling Geographical Accessibility",
    "section": "Visualising KD2SFCA’s accessibility",
    "text": "Visualising KD2SFCA’s accessibility\nThe code chunk below uses a collection of mapping fucntions of tmap package to create a high cartographic quality accessibility to eldercare centre in Singapore. Notice that mapex is reused for bbox argument.\n\ntmap_mode(\"plot\")\ntm_shape(hexagon_KD2SFCA,\n         bbox = mapex) + \n  tm_fill(col = \"accKD2SFCA\",\n          n = 10,\n          style = \"quantile\",\n          border.col = \"black\",\n          border.lwd = 1) +\ntm_shape(eldercare) +\n  tm_symbols(size = 0.1) +\n  tm_layout(main.title = \"Accessibility to eldercare: KD2SFCA method\",\n            main.title.position = \"center\",\n            main.title.size = 2,\n            legend.outside = FALSE,\n            legend.height = 0.45, \n            legend.width = 3.0,\n            legend.format = list(digits = 6),\n            legend.position = c(\"right\", \"top\"),\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#statistical-graphic-visualisation-1",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#statistical-graphic-visualisation-1",
    "title": "Hands on Exercise 10: Modelling Geographical Accessibility",
    "section": "Statistical Graphic Visualisation",
    "text": "Statistical Graphic Visualisation\nNow, we are going to compare the distribution of KD2CFA accessibility values by URA Planning Region.\nFirstly, we need to add the planning region field into hexagon_KD2SFCA simple feature data frame by using the code chunk below.\n\nhexagon_KD2SFCA <- st_join(hexagon_KD2SFCA, mpsz, \n                          join = st_intersects)\n\nNext, ggplot() will be used to plot the distribution by using boxplot graphical method.\n\nggplot(data=hexagon_KD2SFCA, \n       aes(y = accKD2SFCA, \n           x= REGION_N)) +\n  geom_boxplot() +\n  geom_point(stat=\"summary\", \n             fun.y=\"mean\", \n             colour =\"red\", \n             size=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#computing-sam-accessibility",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#computing-sam-accessibility",
    "title": "Hands on Exercise 10: Modelling Geographical Accessibility",
    "section": "Computing SAM accessibility",
    "text": "Computing SAM accessibility\nIn this section, you are going to repeat most of the steps you had learned in previous section to perform the analysis. However, some of the codes will be combined into one code chunk.\nThe code chunk below calculates Hansen’s accessibility using ac() of SpatialAcc and data.frame() is used to save the output in a data frame called acc_SAM. Notice that SAM is used for family argument.\n\nacc_SAM <- data.frame(ac(hexagons$demand,\n                         eldercare$capacity,\n                         distmat_km, \n                         d0 = 50,\n                         power = 2, \n                         family = \"SAM\"))\n\ncolnames(acc_SAM) <- \"accSAM\"\nacc_SAM <- tbl_df(acc_SAM)\nhexagon_SAM <- bind_cols(hexagons, acc_SAM)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#visualising-sams-accessibility",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#visualising-sams-accessibility",
    "title": "Hands on Exercise 10: Modelling Geographical Accessibility",
    "section": "Visualising SAM’s accessibility",
    "text": "Visualising SAM’s accessibility\nThe code chunk below uses a collection of mapping fucntions of tmap package to create a high cartographic quality accessibility to eldercare centre in Singapore. Notice that mapex is reused for bbox argument.\n\ntmap_mode(\"plot\")\ntm_shape(hexagon_SAM,\n         bbox = mapex) + \n  tm_fill(col = \"accSAM\",\n          n = 10,\n          style = \"quantile\",\n          border.col = \"black\",\n          border.lwd = 1) +\ntm_shape(eldercare) +\n  tm_symbols(size = 0.1) +\n  tm_layout(main.title = \"Accessibility to eldercare: SAM method\",\n            main.title.position = \"center\",\n            main.title.size = 2,\n            legend.outside = FALSE,\n            legend.height = 0.45, \n            legend.width = 3.0,\n            legend.format = list(digits = 3),\n            legend.position = c(\"right\", \"top\"),\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#statistical-graphic-visualisation-2",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#statistical-graphic-visualisation-2",
    "title": "Hands on Exercise 10: Modelling Geographical Accessibility",
    "section": "Statistical Graphic Visualisation",
    "text": "Statistical Graphic Visualisation\nNow, we are going to compare the distribution of SAM accessibility values by URA Planning Region.\nFirstly, we need to add the planning region field into hexagon_SAM simple feature data frame by using the code chunk below.\n\nhexagon_SAM <- st_join(hexagon_SAM, mpsz, \n                       join = st_intersects)\n\nNext, ggplot() will be used to plot the distribution by using boxplot graphical method.\n\nggplot(data=hexagon_SAM, \n       aes(y = accSAM, \n           x= REGION_N)) +\n  geom_boxplot() +\n  geom_point(stat=\"summary\", \n             fun.y=\"mean\", \n             colour =\"red\", \n             size=2)\n\n\n\n\nThis is a high-level aggregation that we should analyse before we decide which planning area or region we want to focus one. If we want to analyse the skewness of the data, we can use a histogram instead."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/data/geospatial/ELDERCARE.html",
    "href": "Hands-on_Ex/Hands-on_Ex10/data/geospatial/ELDERCARE.html",
    "title": "IS415-GAA-JS",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/data/geospatial/hexagons.html",
    "href": "Hands-on_Ex/Hands-on_Ex10/data/geospatial/hexagons.html",
    "title": "IS415-GAA-JS",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#importing-geospatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#importing-geospatial-data",
    "title": "Hands on Exercise 10: Modelling Geographical Accessibility",
    "section": "Importing Geospatial Data",
    "text": "Importing Geospatial Data\nThree geospatial data will be imported from the data/geospatial sub-folder. They are MP14_SUBZONE_NO_SEA_PL, hexagons and ELDERCARE.\nThe code chunk below is used to import these three data sets shapefile by using st_read() of sf packages.\n\nmpsz <- st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_NO_SEA_PL\")\n\nReading layer `MP14_SUBZONE_NO_SEA_PL' from data source \n  `C:\\jaymieseet\\IS415-GAA-JS\\Hands-on_Ex\\Hands-on_Ex10\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\nhexagons <- st_read(dsn = \"data/geospatial\", layer = \"hexagons\") \n\nReading layer `hexagons' from data source \n  `C:\\jaymieseet\\IS415-GAA-JS\\Hands-on_Ex\\Hands-on_Ex10\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3125 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 21506.33 xmax: 50010.26 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\n\n\n\neldercare <- st_read(dsn = \"data/geospatial\", layer = \"ELDERCARE\") \n\nReading layer `ELDERCARE' from data source \n  `C:\\jaymieseet\\IS415-GAA-JS\\Hands-on_Ex\\Hands-on_Ex10\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 120 features and 19 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 14481.92 ymin: 28218.43 xmax: 41665.14 ymax: 46804.9\nProjected CRS: SVY21 / Singapore TM\n\n\nThe report above shows that the R object used to contain the imported MP14_SUBZONE_WEB_PL shapefile is called mpsz and it is a simple feature object. The geometry type is multipolygon. it is also important to note that mpsz simple feature object does not have EPSG information."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#updating-crs-information",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#updating-crs-information",
    "title": "Hands on Exercise 10: Modelling Geographical Accessibility",
    "section": "Updating CRS Information",
    "text": "Updating CRS Information\nThe code chunk below updates the newly imported mpsz with the correct ESPG code (i.e. 3414)\n\nmpsz <- st_transform(mpsz, 3414)\neldercare <- st_transform(eldercare, 3414)\nhexagons <- st_transform(hexagons, 3414)\n\nAfter transforming the projection metadata, you can verify the projection of the newly transformed mpsz_svy21 by using st_crs() of sf package.\nThe code chunk below will be used to varify the newly transformed mpsz_svy21.\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nNotice that the EPSG: is indicated as 3414 now."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#cleaning-and-updating-attribute-fields-of-the-geospatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#cleaning-and-updating-attribute-fields-of-the-geospatial-data",
    "title": "Hands on Exercise 10: Modelling Geographical Accessibility",
    "section": "Cleaning and updating attribute fields of the geospatial data",
    "text": "Cleaning and updating attribute fields of the geospatial data\nThere are many redundant fields in the data tables of both eldercare and hexagons. The code chunks below will be used to exclude those redundant fields. At the same time, a new field called demand and a new field called capacity will be added into the data table of hexagons and eldercare sf data frame respectively. Both fields are derive using mutate() of dplyr package.\n\neldercare <- eldercare %>%\n  select(fid, ADDRESSPOS) %>%\n  mutate(capacity = 100)\n\n\nhexagons <- hexagons %>%\n  select(fid) %>%\n  mutate(demand = 100)\n\nNotice that for the purpose of this hands-on exercise, a constant value of 100 is used. In practice, actual demand of the hexagon and capacity of the eldercare centre should be used."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#importing-distance-matrix",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#importing-distance-matrix",
    "title": "Hands on Exercise 10: Modelling Geographical Accessibility",
    "section": "Importing Distance Matrix",
    "text": "Importing Distance Matrix\nThe code chunk below uses read_cvs() of readr package to import OD_Matrix.csv into RStudio. The imported object is a tibble data.frame called ODMatrix.\n\nODMatrix <- read_csv(\"data/aspatial/OD_Matrix.csv\", skip = 0)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#tidying-distance-matrix",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#tidying-distance-matrix",
    "title": "Hands on Exercise 10: Modelling Geographical Accessibility",
    "section": "Tidying distance matrix",
    "text": "Tidying distance matrix\nThe imported ODMatrix organised the distance matrix columnwise.\nThe rows represent origins (i.e. also know as from field) and the columns represent destination (i.e. also known as to field.)\nThe code chunk below uses spread() of tidyr package is used to transform the O-D matrix from a thin format into a fat format.\n\ndistmat <- ODMatrix %>%\n  select(origin_id, destination_id, total_cost) %>%\n  spread(destination_id, total_cost)%>%\n  select(c(-c('origin_id')))\n\nNote: Since tidyr version 1.0 a new function called pivot_wider() is introduce. You should use pivot_wider() instead of spread()\nCurrently, the distance is measured in metre because SVY21 projected coordinate system is used. The code chunk below will be used to convert the unit measurement from metre to kilometre. We also use as.matrix() to convert it into matrix form.\n\ndistmat_km <- as.matrix(distmat/1000)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#computing-hansens-accessibility",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#computing-hansens-accessibility",
    "title": "Hands on Exercise 10: Modelling Geographical Accessibility",
    "section": "Computing Hansen’s accessibility",
    "text": "Computing Hansen’s accessibility\nNow, we ready to compute Hansen’s accessibility by using ac() of SpatialAcc package. Before getting started, you are encourage to read the arguments of the function at least once in order to ensure that the required inputs are available.\nThe code chunk below calculates Hansen’s accessibility using ac() of SpatialAcc and data.frame() is used to save the output in a data frame called acc_Handsen.\n\nacc_Hansen <- data.frame(ac(hexagons$demand,\n                            eldercare$capacity,\n                            distmat_km, \n                            #d0 = 50,\n                            power = 2, \n                            family = \"Hansen\"))\n\nThe default field name is very messy, we will rename it to accHansen by using the code chunk below.\n\ncolnames(acc_Hansen) <- \"accHansen\"\n\nNext, we will convert the data table into tibble format by using the code chunk below.\n\nacc_Hansen <- tbl_df(acc_Hansen)\n\nLastly, bind_cols() of dplyr will be used to join the acc_Hansen tibble data frame with the hexagons simple feature data frame. The output is called hexagon_Hansen.\n\nhexagon_Hansen <- bind_cols(hexagons, acc_Hansen)\n\nhexagon_Hansen is a simple feature data frame and not a typical tibble data frame.\nActually, the steps above can be perform by using a single code chunk as shown below.\n\nacc_Hansen <- data.frame(ac(hexagons$demand,\n                            eldercare$capacity,\n                            distmat_km, \n                            #d0 = 50,\n                            power = 0.5, \n                            family = \"Hansen\"))\n\ncolnames(acc_Hansen) <- \"accHansen\"\nacc_Hansen <- tbl_df(acc_Hansen)\nhexagon_Hansen <- bind_cols(hexagons, acc_Hansen)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take_home_Ex03.html",
    "href": "Take-home_Ex/Take-home_Ex03/Take_home_Ex03.html",
    "title": "Take-home Exercise 3: Spatio-temporal Analysis Methods to Discover the Distribution Eathquake Events in West Java, Indonesia",
    "section": "",
    "text": "Indonesia is located in a seismically active region known as the Pacific Ring of Fire, where several tectonic plates meet. This makes the country highly susceptible to earthquakes, volcanic eruptions, and tsunamis.Indonesia experiences frequent seismic activity, ranging from minor tremors to major earthquakes with significant magnitudes. Understanding the frequency and magnitude of these events is crucial for assessing their impact on local communities, infrastructure, and the environment."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take_home_Ex03.html#show-all-events",
    "href": "Take-home_Ex/Take-home_Ex03/Take_home_Ex03.html#show-all-events",
    "title": "Take-home Exercise 3: Spatio-temporal Analysis Methods to Discover the Distribution Eathquake Events in West Java, Indonesia",
    "section": "Show all events",
    "text": "Show all events\nThis map below shows all events of earthquake from 2009 to 2023.\n\ntmap_mode('view')  # Define basemaps \nosm_basemap <- tm_basemap(server = \"OpenStreetMap.HOT\") \nimagery_basemap <- tm_basemap(server = \"Esri.WorldImagery\")\n\neo_map <- osm_basemap +\n  tm_shape(java_earthq) +\n  tm_symbols(size = 0.8, col = \"mag\", palette = \"Reds\", alpha = 1, \n             popup.vars = c(\"subdistrict\", \"year_week\"), id = \"mag\") +\n  tm_layout(title = \"Earthquake Events Magnitude\") +\n  tm_view(dot.size.fixed = TRUE)\n\neo_map"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take_home_Ex03.html#filter-by-year-and-week",
    "href": "Take-home_Ex/Take-home_Ex03/Take_home_Ex03.html#filter-by-year-and-week",
    "title": "Take-home Exercise 3: Spatio-temporal Analysis Methods to Discover the Distribution Eathquake Events in West Java, Indonesia",
    "section": "Filter by Year and Week",
    "text": "Filter by Year and Week\nSince I’ve incorporated a new column indicating the year and week of each earthquake event, we can implement a slider feature to visualize the evolution of earthquake occurrences throughout the year. As the slider moves across different weeks, the map dynamically updates to display earthquake events for the selected week. If there are no earthquake events recorded for a particular week, the map gracefully renders as empty, providing a clear representation of seismic activity over time.\nGiven the relatively sparse occurrence of earthquake events within a week, I opted to increase the size of each plotted point to enhance visibility and highlight the significance of each seismic event. By adjusting the size parameter to a higher value of 5, the resulting map provides a clearer and more pronounced representation of earthquake occurrences, ensuring that even minor seismic events are readily discernible to viewers.\n\ntmap_mode('view')  # Define basemaps \nosm_basemap <- tm_basemap(server = \"OpenStreetMap.HOT\") \nimagery_basemap <- tm_basemap(server = \"Esri.WorldImagery\")\n\nfilter_date <- \"2010, Week 48\"\n\n# Filter the dataset based on the specific year_week value\nfiltered_data <- java_earthq %>%\n  filter(year_week == filter_date)\n\nif (nrow(filtered_data) == 0) {\n  # If no rows are filtered, create an empty map or a message\n  empty_map <- osm_basemap +\n    tm_shape(java_earthq) +\n    tm_symbols(alpha = 0, size = 0) +  # Added '+' to separate tm_symbols() from tm_layout()\n    tm_layout(title = paste(\"No earthquake events for \", filter_date))  # Corrected tm_layout() function\n  \n  empty_map\n} else {\n  # If rows are filtered, create the map with the filtered dataset\n  eo_map <- osm_basemap +\n    tm_shape(filtered_data) +\n    tm_symbols(size = 5, col = \"mag\", palette = \"Reds\", alpha = 1, \n               popup.vars = c(\"subdistrict\", \"year_week\"), id = \"mag\", breaks = seq(3, 6.5, by = 0.5)) +\n    tm_layout(title = paste(\"Earthquake Events Magnitude for \", filter_date))  # Corrected tm_layout() function\n  \n  eo_map\n}"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take_home_Ex03.html#filter-by-year-and-month",
    "href": "Take-home_Ex/Take-home_Ex03/Take_home_Ex03.html#filter-by-year-and-month",
    "title": "Take-home Exercise 3: Spatio-temporal Analysis Methods to Discover the Distribution Eathquake Events in West Java, Indonesia",
    "section": "Filter by Year and Month",
    "text": "Filter by Year and Month\nThe filter can also be set by Year and Month.\n\ntmap_mode('view')  # Define basemaps \nosm_basemap <- tm_basemap(server = \"OpenStreetMap.HOT\") \nimagery_basemap <- tm_basemap(server = \"Esri.WorldImagery\")\n\nfilter_year <- 2010\nfilter_month <- \"January\"\n\n# Filter the dataset based on the specific year_week value\nfiltered_data <- java_earthq %>%\n  filter(year == filter_year,\n         month == filter_month)\n\nif (nrow(filtered_data) == 0) {\n  # If no rows are filtered, create an empty map or a message\n  empty_map <- osm_basemap +\n    tm_shape(java_earthq) +\n    tm_symbols(alpha = 0, size = 0) +  # Added '+' to separate tm_symbols() from tm_layout()\n    tm_layout(title = paste(\"No earthquake events for \", filter_date))  # Corrected tm_layout() function\n  \n  empty_map\n} else {\n  # If rows are filtered, create the map with the filtered dataset\n  eo_map <- osm_basemap +\n    tm_shape(filtered_data) +\n    tm_symbols(size = 5, col = \"mag\", palette = \"Reds\", alpha = 1, \n               popup.vars = c(\"subdistrict\", \"year_week\"), id = \"mag\", breaks = seq(3, 6.5, by = 0.5)) +\n    tm_layout(title = paste(\"Earthquake Events Magnitude for \", filter_date))  # Corrected tm_layout() function\n  \n  eo_map\n}"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take_home_Ex03.html#creating-a-time-series-cube",
    "href": "Take-home_Ex/Take-home_Ex03/Take_home_Ex03.html#creating-a-time-series-cube",
    "title": "Take-home Exercise 3: Spatio-temporal Analysis Methods to Discover the Distribution Eathquake Events in West Java, Indonesia",
    "section": "Creating a time series cube",
    "text": "Creating a time series cube\nHere, we are checking for any years that do not have any recorded earthquake cases and assigning a count of 0 to those years.\n\n# Create a vector of all years\nyears <- as.numeric(2009:2023)\n\n# Create a template dataframe with all possible combinations of regions, years, and months\ntemplate_df <- expand.grid(subdistrict = unique(west_java$subdistrict),\n                           year = years,\n                           stringsAsFactors = FALSE)\n\n# Left join with original dataframe\njava_eq_df <- left_join(template_df, java_earthq_count, by = c(\"subdistrict\", \"year\"))\n\n# Replace NA values in total cases column with 0\njava_eq_df$total_cases[is.na(java_eq_df$total_cases)] <- 0\n\n\njava_eq_df <- as_tibble(java_eq_df)\n\n\njava_st <- spacetime(.data = java_eq_df, .geo = west_java, \"subdistrict\", \"year\")\n\nis_spacetime_cube(java_st)\n\n[1] TRUE"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take_home_Ex03.html#computing-gi",
    "href": "Take-home_Ex/Take-home_Ex03/Take_home_Ex03.html#computing-gi",
    "title": "Take-home Exercise 3: Spatio-temporal Analysis Methods to Discover the Distribution Eathquake Events in West Java, Indonesia",
    "section": "Computing Gi*",
    "text": "Computing Gi*\n\nDeriving the Spatial Weights\n\njava_nb <- java_st %>%\n  activate(\"geometry\") %>%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         wt = st_inverse_distance(nb, geometry,\n                                  scale = 1,\n                                  alpha = 1),\n         .before = 1) %>%\n  set_nbs(\"nb\") %>%\n  set_wts(\"wt\")\n\n\ngi_stars <- java_nb %>% \n  group_by(year) %>% \n  mutate(gi_star = local_gstar_perm(\n    total_cases, nb, wt)) %>% \n  tidyr::unnest(gi_star)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take_home_Ex03.html#mann-kendall-test",
    "href": "Take-home_Ex/Take-home_Ex03/Take_home_Ex03.html#mann-kendall-test",
    "title": "Take-home Exercise 3: Spatio-temporal Analysis Methods to Discover the Distribution Eathquake Events in West Java, Indonesia",
    "section": "Mann-Kendall Test",
    "text": "Mann-Kendall Test\nThis code snippet generates a time series plot showing the Green Index (GI) variation over the years for a specific subdistrict. Users can choose the subdistrict they want to analyze by changing the value in the filter function. The resulting plot is interactive, allowing users to hover over data points to view specific values and trends.\n\ncbg1 <- gi_stars %>% \n  ungroup() %>% \n  filter(subdistrict == \"Cugenang_Cianjur\") |> \n  select(subdistrict, year, gi_star)\n\np <- ggplot(data = cbg1, \n       aes(x = year, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\n\nggplotly(p)\n\n\n\n\n\n\ncbg1 %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 × 5\n    tau    sl     S     D  varS\n  <dbl> <dbl> <dbl> <dbl> <dbl>\n1 0.276 0.166    29  105.  408.\n\n\nUsrs can choose to overlay a line representing the mean GI across all subdistricts. This allows users to compare the GI trend of the selected subdistrict to the average GI trend across all subdistricts, helping identify any anomalies or subdistricts with GI values higher or lower than the average.\n\nmean_gi <- gi_stars %>%\n  group_by(year) %>%\n  summarize(mean_gi_star = mean(gi_star, na.rm = TRUE))\n\ncbg1 <- gi_stars %>% \n  ungroup() %>% \n  filter(subdistrict == \"Cugenang_Cianjur\") |> \n  select(subdistrict, year, gi_star)\n\np <- ggplot() +\n  geom_line(data = cbg1, aes(x = year, y = gi_star), color = \"blue\") +\n  geom_line(data = mean_gi, aes(x = year, y = mean_gi_star), color = \"red\", linetype = \"dashed\") +\n  theme_light()\n\nggplotly(p)\n\n\n\n\n\n\nehsa <- gi_stars %>%\n  group_by(subdistrict) %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>%\n  tidyr::unnest_wider(mk)\n\n\nemerging <- ehsa %>% \n  arrange(sl, abs(tau)) %>% \n  slice(1:5)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take_home_Ex03.html#performing-emerging-hotspot-analysis",
    "href": "Take-home_Ex/Take-home_Ex03/Take_home_Ex03.html#performing-emerging-hotspot-analysis",
    "title": "Take-home Exercise 3: Spatio-temporal Analysis Methods to Discover the Distribution Eathquake Events in West Java, Indonesia",
    "section": "Performing Emerging Hotspot Analysis",
    "text": "Performing Emerging Hotspot Analysis\n\nehsa <- emerging_hotspot_analysis(\n  x = java_st, \n  .var = \"total_cases\", \n  k = 1, \n  nsim = 99\n)\n\n\nVisualising the distribution of EHSA classes\n\nggplot(data = ehsa, aes(x = classification)) +\n  geom_bar() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\njava_ehsa <- west_java %>%\n  left_join(ehsa,\n            by = join_by(subdistrict == location))\n\n\n\nVisualising EHSA\nThis map displays various hotspots detected through a spatial analysis technique. Users have the flexibility to adjust the p-value parameter, which influences the sensitivity of hotspot detection. By modifying the p-value, users can perform different types of analyses, such as identifying significant clusters or exploring spatial patterns at varying levels of statistical significance. Adjusting this parameter allows users to customize the analysis based on their specific research objectives or hypotheses.\n\nehsa_sig <- java_ehsa  %>%\n  filter(p_value < 0.1)\ntmap_mode(\"plot\")\ntm_shape(java_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(ehsa_sig) +\n  tm_fill(\"classification\") + \n  tm_borders(alpha = 0.4)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex11/data/geospatial/MPSZ-2019.html",
    "href": "Hands-on_Ex/Hands-on_Ex11/data/geospatial/MPSZ-2019.html",
    "title": "IS415-GAA-JS",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex11/Hands-on_Ex11.html",
    "href": "Hands-on_Ex/Hands-on_Ex11/Hands-on_Ex11.html",
    "title": "Hands-On Exercise 11",
    "section": "",
    "text": "mpsz <- st_read(dsn = \"data/geospatial\", layer = \"MPSZ-2019\") %>%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `C:\\jaymieseet\\IS415-GAA-JS\\Hands-on_Ex\\Hands-on_Ex11\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\nmpsz\n\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n                 SUBZONE_N SUBZONE_C       PLN_AREA_N PLN_AREA_C       REGION_N\n1              MARINA EAST    MESZ01      MARINA EAST         ME CENTRAL REGION\n2         INSTITUTION HILL    RVSZ05     RIVER VALLEY         RV CENTRAL REGION\n3           ROBERTSON QUAY    SRSZ01  SINGAPORE RIVER         SR CENTRAL REGION\n4  JURONG ISLAND AND BUKOM    WISZ01  WESTERN ISLANDS         WI    WEST REGION\n5             FORT CANNING    MUSZ02           MUSEUM         MU CENTRAL REGION\n6         MARINA EAST (MP)    MPSZ05    MARINE PARADE         MP CENTRAL REGION\n7                   SUDONG    WISZ03  WESTERN ISLANDS         WI    WEST REGION\n8                  SEMAKAU    WISZ02  WESTERN ISLANDS         WI    WEST REGION\n9           SOUTHERN GROUP    SISZ02 SOUTHERN ISLANDS         SI CENTRAL REGION\n10                 SENTOSA    SISZ01 SOUTHERN ISLANDS         SI CENTRAL REGION\n   REGION_C                       geometry\n1        CR MULTIPOLYGON (((33222.98 29...\n2        CR MULTIPOLYGON (((28481.45 30...\n3        CR MULTIPOLYGON (((28087.34 30...\n4        WR MULTIPOLYGON (((14557.7 304...\n5        CR MULTIPOLYGON (((29542.53 31...\n6        CR MULTIPOLYGON (((35279.55 30...\n7        WR MULTIPOLYGON (((15772.59 21...\n8        WR MULTIPOLYGON (((19843.41 21...\n9        CR MULTIPOLYGON (((30870.53 22...\n10       CR MULTIPOLYGON (((26879.04 26...\n\n\n\nmpsz_sp <- as(mpsz, \"Spatial\")\nmpsz_sp\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 332 \nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 6\nnames       : SUBZONE_N, SUBZONE_C, PLN_AREA_N, PLN_AREA_C,       REGION_N, REGION_C \nmin values  : ADMIRALTY,    AMSZ01, ANG MO KIO,         AM, CENTRAL REGION,       CR \nmax values  :    YUNNAN,    YSSZ09,     YISHUN,         YS,    WEST REGION,       WR \n\n\n\ndist <- spDists(mpsz_sp, \n                longlat = FALSE)\n\n\nhead(dist, n=c(10, 10))\n\n           [,1]       [,2]      [,3]      [,4]       [,5]      [,6]      [,7]\n [1,]     0.000  3926.0025  3939.108 20252.964  2989.9839  1431.330 19211.836\n [2,]  3926.003     0.0000   305.737 16513.865   951.8314  5254.066 16242.523\n [3,]  3939.108   305.7370     0.000 16412.062  1045.9088  5299.849 16026.146\n [4,] 20252.964 16513.8648 16412.062     0.000 17450.3044 21665.795  7229.017\n [5,]  2989.984   951.8314  1045.909 17450.304     0.0000  4303.232 17020.916\n [6,]  1431.330  5254.0664  5299.849 21665.795  4303.2323     0.000 20617.082\n [7,] 19211.836 16242.5230 16026.146  7229.017 17020.9161 20617.082     0.000\n [8,] 14960.942 12749.4101 12477.871 11284.279 13336.0421 16281.453  5606.082\n [9,]  7515.256  7934.8082  7649.776 18427.503  7801.6163  8403.896 14810.930\n[10,]  6391.342  4975.0021  4669.295 15469.566  5226.8731  7707.091 13111.391\n           [,8]      [,9]     [,10]\n [1,] 14960.942  7515.256  6391.342\n [2,] 12749.410  7934.808  4975.002\n [3,] 12477.871  7649.776  4669.295\n [4,] 11284.279 18427.503 15469.566\n [5,] 13336.042  7801.616  5226.873\n [6,] 16281.453  8403.896  7707.091\n [7,]  5606.082 14810.930 13111.391\n [8,]     0.000  9472.024  8575.490\n [9,]  9472.024     0.000  3780.800\n[10,]  8575.490  3780.800     0.000\n\n\n\nsz_names <- mpsz$SUBZONE_C\n\n\ncolnames(dist) <- paste0(sz_names)\nrownames(dist) <- paste0(sz_names)\n\n\ndistPair <- melt(dist) %>%\n  rename(dist = value)\nhead(distPair, 10)\n\n\ndistPair %>%\n  filter(dist > 0) %>%\n  summary()\n\n\ndistPair$dist <- ifelse(distPair$dist == 0,\n                        50, distPair$dist)\n\n\ndistPair %>%\n  summary()\n\n\ndistPair <- distPair %>%\n  rename(orig = Var1,\n         dest = Var2)\n\n\nwrite_rds(distPair, \"data/rds/distPair.rds\") \n\n\ndistPair <- read_rds(\"data/rds/distPair.rds\")\n\n\nodbus <- read_csv(\"data/aspatial/origin_destination_bus_202210.csv\")\n\n\nbusstop <- st_read(dsn = \"data/geospatial\",\n                   layer = \"BusStop\") %>%\n  st_transform(crs = 3414)\n\n\nbusstop_mpsz <- st_intersection(busstop, mpsz) %>%\n  select(BUS_STOP_N, SUBZONE_C) %>%\n  st_drop_geometry()\n\n\nodbus <- read_csv(\"data/aspatial/origin_destination_bus_202210.csv\")\n\n\nodbus$ORIGIN_PT_CODE <- as.factor(odbus$ORIGIN_PT_CODE)\nodbus$DESTINATION_PT_CODE <- as.factor(odbus$DESTINATION_PT_CODE) \n\n\nodbus6_9 <- odbus %>%\n  filter(DAY_TYPE == \"WEEKDAY\") %>%\n  filter(TIME_PER_HOUR >= 6 &\n           TIME_PER_HOUR <= 9) %>%\n  group_by(ORIGIN_PT_CODE,\n           DESTINATION_PT_CODE) %>%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n\nod_data <- left_join(odbus6_9 , busstop_mpsz,\n            by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\")) %>%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE,\n         ORIGIN_SZ = SUBZONE_C,\n         DESTIN_BS = DESTINATION_PT_CODE)\n\n\nduplicate <- od_data %>%\n  group_by_all() %>%\n  filter(n()>1) %>%\n  ungroup()\n\n\nod_data <- unique(od_data)\n\n\nod_data <- left_join(od_data , busstop_mpsz,\n            by = c(\"DESTIN_BS\" = \"BUS_STOP_N\")) \n\n\nduplicate <- od_data %>%\n  group_by_all() %>%\n  filter(n()>1) %>%\n  ungroup()\n\n\nod_data <- unique(od_data)\n\n\nod_data <- od_data %>%\n  rename(DESTIN_SZ = SUBZONE_C) %>%\n  drop_na() %>%\n  group_by(ORIGIN_SZ, DESTIN_SZ) %>%\n  summarise(MORNING_PEAK = sum(TRIPS))\n\n\nwrite_rds(od_data, \"data/rds/od_data_fii.rds\")\n\n\nod_data_fii <- read_rds(\"data/rds/od_data_fii.rds\")"
  }
]